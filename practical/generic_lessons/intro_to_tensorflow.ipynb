{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is an introduction in how to use the Tensorflow framework.\n",
    "\n",
    "We will cover the components needed for a simple feedfoward neural network.\n",
    "\n",
    "Key concepts & Tensorflow paradigms covered:\n",
    "- Placeholders\n",
    "- Variable\n",
    "- Initializers\n",
    "- Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Graph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  we use a set of tuples to define the network structure\n",
    "input_shape =(4,)  #  the length of the input numpy array\n",
    "input_nodes= (6,)  #  the number of nodes in the input layer\n",
    "hidden_nodes = (8,)  \n",
    "output_shape = (4,)  #  num. nodes in output layer and the length of the output numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first Tensorflow paradigm is the **Placeholder**.\n",
    "\n",
    "Tensorflow uses placeholders to feed data into the network.\n",
    "\n",
    "The placeholder is fed using numpy arrays.\n",
    "\n",
    "The first dimension is the number of samples in the batch - we use None to be able to input any batch size we want.\n",
    "\n",
    "The second dimension is the shape of one sample - i.e. the length of the input numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network_input = tf.placeholder(tf.float32, shape=(None, *input_shape), name='network_input')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two Tensorflow paradigms are the **Variable** and **Initializer**.  \n",
    "\n",
    "Variable objects are used to hold tensors of variable that tensorflow can change.  Both weights and biases are tf.Variables.\n",
    "\n",
    "We also need to tell Tensorflow what initial values we want our varibles to be.  \n",
    "\n",
    "To do the initialization we pass in the tf.random_normal initializer.  The shape of the Variable tensor is specified in the intializer.\n",
    "\n",
    "The * operation is used to unpack the tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_weights = tf.Variable(tf.random_normal(shape=(*input_shape, *input_nodes), name='input_weights'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the same pattern for the biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_bias = tf.Variable(tf.zeros(shape=(*input_nodes,)), name='input_bias')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can form the input layer using matrix multiplication between the input & weights, then add the biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pre_activation = tf.add(tf.matmul(network_input, input_weights), input_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can form the layer by squeezing the output through a rectified linear unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_layer = tf.nn.relu(pre_activation, name='input_layer_output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a hidden layer using the same logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h_w = tf.Variable(tf.random_normal((*input_nodes, *hidden_nodes)), name='hidden_weights')\n",
    "h_b = tf.Variable(tf.zeros((*hidden_nodes, ), name='hidden_bias'))\n",
    "hidden_layer = tf.nn.relu(tf.add(tf.matmul(input_layer, h_w), h_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output layer has no activation function (aka a linear activation function).  This allows the network to output negative values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "o_w = tf.Variable(tf.random_normal((*hidden_nodes, *output_shape)), name='output_weights')\n",
    "o_b = tf.Variable(tf.zeros((*output_shape, ), name='output_bias'))\n",
    "output = tf.add(tf.matmul(hidden_layer, o_w), o_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the code above allows us to make predictions with our network - i.e. we can do forward passes across the network.\n",
    "\n",
    "Below we will setup the code for training.\n",
    "\n",
    "We first need another placeholder.  This will serve as the target value for the network (aka y_train) - what our network should be outputting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = tf.placeholder(tf.float32, shape=(None, *output_shape), name='target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.losses.mean_squared_error(target, output, scope='loss_function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need an optimizer to do the heavy lifting of training the network.  \n",
    "\n",
    "Here we use the Adam optimizer.  Note that the input here of learning rate is one of the most important in training any neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally a Tensorflow operation to actually do the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all of the machinery to do forward passes and to back propagate error is in place.  \n",
    "\n",
    "Add in a few Tensorflow summary operations to track what is going on in our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.summary.tensor_summary('input', network_input)\n",
    "tf.summary.histogram('input_weights', input_weights)\n",
    "tf.summary.histogram('output_bias', o_b)\n",
    "tf.summary.scalar('loss', loss)\n",
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a Tensorflow Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing our model needs is data.  The function below generates training data for a simple function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_data(num_samples):\n",
    "    \"\"\"\n",
    "    Generates training data for our network.\n",
    "\n",
    "    args\n",
    "        num_samples (int)\n",
    "\n",
    "    returns\n",
    "        net_in (np.array)  fed into the network (aka features or x)\n",
    "        net_out (np.array)  aka target or y - the value we are trying to approx.\n",
    "    \"\"\"\n",
    "    net_in = np.random.rand(num_samples, *input_shape)\n",
    "    net_out = net_in + 10\n",
    "\n",
    "    return net_in, net_out\n",
    "\n",
    "#  run the function to get data to train with\n",
    "inputs, targets = generate_data(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now introduce another key Tensorflow paradigm - the Session.\n",
    "\n",
    "You can think of a Session as one instance of the model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 loss 164.50222778320312\n",
      "step 1 loss 163.87179565429688\n",
      "step 2 loss 163.2452392578125\n",
      "step 3 loss 162.623046875\n",
      "step 4 loss 162.00572204589844\n",
      "step 5 loss 161.3926544189453\n",
      "step 6 loss 160.78384399414062\n",
      "step 7 loss 160.17982482910156\n",
      "step 8 loss 159.5801239013672\n",
      "step 9 loss 158.9837646484375\n",
      "step 10 loss 158.39132690429688\n",
      "step 11 loss 157.8040313720703\n",
      "step 12 loss 157.2203826904297\n",
      "step 13 loss 156.64027404785156\n",
      "step 14 loss 156.06370544433594\n",
      "step 15 loss 155.49107360839844\n",
      "step 16 loss 154.92286682128906\n",
      "step 17 loss 154.35939025878906\n",
      "step 18 loss 153.79937744140625\n",
      "step 19 loss 153.2428741455078\n",
      "step 20 loss 152.69163513183594\n",
      "step 21 loss 152.1458740234375\n",
      "step 22 loss 151.60366821289062\n",
      "step 23 loss 151.0650177001953\n",
      "step 24 loss 150.53126525878906\n",
      "step 25 loss 150.00213623046875\n",
      "step 26 loss 149.47796630859375\n",
      "step 27 loss 148.95750427246094\n",
      "step 28 loss 148.4425048828125\n",
      "step 29 loss 147.93228149414062\n",
      "step 30 loss 147.42604064941406\n",
      "step 31 loss 146.92401123046875\n",
      "step 32 loss 146.4265594482422\n",
      "step 33 loss 145.93296813964844\n",
      "step 34 loss 145.44435119628906\n",
      "step 35 loss 144.959716796875\n",
      "step 36 loss 144.47877502441406\n",
      "step 37 loss 144.00172424316406\n",
      "step 38 loss 143.528076171875\n",
      "step 39 loss 143.05783081054688\n",
      "step 40 loss 142.5908660888672\n",
      "step 41 loss 142.1285400390625\n",
      "step 42 loss 141.67034912109375\n",
      "step 43 loss 141.21534729003906\n",
      "step 44 loss 140.7634735107422\n",
      "step 45 loss 140.31517028808594\n",
      "step 46 loss 139.8708038330078\n",
      "step 47 loss 139.42959594726562\n",
      "step 48 loss 138.99148559570312\n",
      "step 49 loss 138.55633544921875\n",
      "step 50 loss 138.12428283691406\n",
      "step 51 loss 137.6958465576172\n",
      "step 52 loss 137.2703094482422\n",
      "step 53 loss 136.84756469726562\n",
      "step 54 loss 136.4276580810547\n",
      "step 55 loss 136.01065063476562\n",
      "step 56 loss 135.59620666503906\n",
      "step 57 loss 135.18443298339844\n",
      "step 58 loss 134.77525329589844\n",
      "step 59 loss 134.36981201171875\n",
      "step 60 loss 133.96755981445312\n",
      "step 61 loss 133.56820678710938\n",
      "step 62 loss 133.17196655273438\n",
      "step 63 loss 132.77813720703125\n",
      "step 64 loss 132.38673400878906\n",
      "step 65 loss 131.9980010986328\n",
      "step 66 loss 131.61158752441406\n",
      "step 67 loss 131.22779846191406\n",
      "step 68 loss 130.8463134765625\n",
      "step 69 loss 130.4673614501953\n",
      "step 70 loss 130.09185791015625\n",
      "step 71 loss 129.71890258789062\n",
      "step 72 loss 129.34815979003906\n",
      "step 73 loss 128.98007202148438\n",
      "step 74 loss 128.61424255371094\n",
      "step 75 loss 128.25062561035156\n",
      "step 76 loss 127.88934326171875\n",
      "step 77 loss 127.5298080444336\n",
      "step 78 loss 127.1723861694336\n",
      "step 79 loss 126.81767272949219\n",
      "step 80 loss 126.46521759033203\n",
      "step 81 loss 126.11436462402344\n",
      "step 82 loss 125.76518249511719\n",
      "step 83 loss 125.41763305664062\n",
      "step 84 loss 125.0719223022461\n",
      "step 85 loss 124.7284164428711\n",
      "step 86 loss 124.38775634765625\n",
      "step 87 loss 124.04946899414062\n",
      "step 88 loss 123.71353149414062\n",
      "step 89 loss 123.37939453125\n",
      "step 90 loss 123.04678344726562\n",
      "step 91 loss 122.71575164794922\n",
      "step 92 loss 122.38664245605469\n",
      "step 93 loss 122.05923461914062\n",
      "step 94 loss 121.73343658447266\n",
      "step 95 loss 121.40904235839844\n",
      "step 96 loss 121.0863265991211\n",
      "step 97 loss 120.76518249511719\n",
      "step 98 loss 120.4463882446289\n",
      "step 99 loss 120.1292495727539\n",
      "step 100 loss 119.81410217285156\n",
      "step 101 loss 119.50059509277344\n",
      "step 102 loss 119.18832397460938\n",
      "step 103 loss 118.87701416015625\n",
      "step 104 loss 118.56646728515625\n",
      "step 105 loss 118.25704956054688\n",
      "step 106 loss 117.94842529296875\n",
      "step 107 loss 117.6407241821289\n",
      "step 108 loss 117.33390808105469\n",
      "step 109 loss 117.02816772460938\n",
      "step 110 loss 116.72377014160156\n",
      "step 111 loss 116.42028045654297\n",
      "step 112 loss 116.11756896972656\n",
      "step 113 loss 115.81555938720703\n",
      "step 114 loss 115.51416778564453\n",
      "step 115 loss 115.21331787109375\n",
      "step 116 loss 114.913330078125\n",
      "step 117 loss 114.61400604248047\n",
      "step 118 loss 114.31553649902344\n",
      "step 119 loss 114.01763916015625\n",
      "step 120 loss 113.72026062011719\n",
      "step 121 loss 113.42366027832031\n",
      "step 122 loss 113.12810516357422\n",
      "step 123 loss 112.83289337158203\n",
      "step 124 loss 112.53781127929688\n",
      "step 125 loss 112.24347686767578\n",
      "step 126 loss 111.95034790039062\n",
      "step 127 loss 111.65855407714844\n",
      "step 128 loss 111.3670883178711\n",
      "step 129 loss 111.075927734375\n",
      "step 130 loss 110.78500366210938\n",
      "step 131 loss 110.49456024169922\n",
      "step 132 loss 110.2040786743164\n",
      "step 133 loss 109.9130630493164\n",
      "step 134 loss 109.62218475341797\n",
      "step 135 loss 109.33216094970703\n",
      "step 136 loss 109.04244232177734\n",
      "step 137 loss 108.75303649902344\n",
      "step 138 loss 108.4638671875\n",
      "step 139 loss 108.17514038085938\n",
      "step 140 loss 107.88677978515625\n",
      "step 141 loss 107.59870910644531\n",
      "step 142 loss 107.31078338623047\n",
      "step 143 loss 107.02308654785156\n",
      "step 144 loss 106.73455810546875\n",
      "step 145 loss 106.44601440429688\n",
      "step 146 loss 106.15765380859375\n",
      "step 147 loss 105.8696517944336\n",
      "step 148 loss 105.58174133300781\n",
      "step 149 loss 105.29421997070312\n",
      "step 150 loss 105.00690460205078\n",
      "step 151 loss 104.71986389160156\n",
      "step 152 loss 104.4329605102539\n",
      "step 153 loss 104.14617156982422\n",
      "step 154 loss 103.8584976196289\n",
      "step 155 loss 103.57062530517578\n",
      "step 156 loss 103.28240966796875\n",
      "step 157 loss 102.99433898925781\n",
      "step 158 loss 102.70660400390625\n",
      "step 159 loss 102.41915130615234\n",
      "step 160 loss 102.13159942626953\n",
      "step 161 loss 101.8443832397461\n",
      "step 162 loss 101.55730438232422\n",
      "step 163 loss 101.27063751220703\n",
      "step 164 loss 100.98432922363281\n",
      "step 165 loss 100.69818115234375\n",
      "step 166 loss 100.41173553466797\n",
      "step 167 loss 100.12445068359375\n",
      "step 168 loss 99.83700561523438\n",
      "step 169 loss 99.54949188232422\n",
      "step 170 loss 99.26227569580078\n",
      "step 171 loss 98.97533416748047\n",
      "step 172 loss 98.68851470947266\n",
      "step 173 loss 98.4018783569336\n",
      "step 174 loss 98.11589813232422\n",
      "step 175 loss 97.8300552368164\n",
      "step 176 loss 97.54435729980469\n",
      "step 177 loss 97.25920867919922\n",
      "step 178 loss 96.9742660522461\n",
      "step 179 loss 96.68861389160156\n",
      "step 180 loss 96.40193176269531\n",
      "step 181 loss 96.11486053466797\n",
      "step 182 loss 95.8279800415039\n",
      "step 183 loss 95.54037475585938\n",
      "step 184 loss 95.25250244140625\n",
      "step 185 loss 94.96375274658203\n",
      "step 186 loss 94.67455291748047\n",
      "step 187 loss 94.38557434082031\n",
      "step 188 loss 94.09652709960938\n",
      "step 189 loss 93.80732727050781\n",
      "step 190 loss 93.51553344726562\n",
      "step 191 loss 93.22103881835938\n",
      "step 192 loss 92.92504119873047\n",
      "step 193 loss 92.62661743164062\n",
      "step 194 loss 92.32568359375\n",
      "step 195 loss 92.02283477783203\n",
      "step 196 loss 91.71916198730469\n",
      "step 197 loss 91.41361236572266\n",
      "step 198 loss 91.10791015625\n",
      "step 199 loss 90.80197143554688\n",
      "step 200 loss 90.49092102050781\n",
      "step 201 loss 90.1781234741211\n",
      "step 202 loss 89.86412048339844\n",
      "step 203 loss 89.5491714477539\n",
      "step 204 loss 89.23252868652344\n",
      "step 205 loss 88.91488647460938\n",
      "step 206 loss 88.59300994873047\n",
      "step 207 loss 88.26831817626953\n",
      "step 208 loss 87.94092559814453\n",
      "step 209 loss 87.61107635498047\n",
      "step 210 loss 87.28002166748047\n",
      "step 211 loss 86.94715118408203\n",
      "step 212 loss 86.61150360107422\n",
      "step 213 loss 86.27299499511719\n",
      "step 214 loss 85.93045043945312\n",
      "step 215 loss 85.57781219482422\n",
      "step 216 loss 85.22017669677734\n",
      "step 217 loss 84.85771179199219\n",
      "step 218 loss 84.49016571044922\n",
      "step 219 loss 84.11819458007812\n",
      "step 220 loss 83.74417877197266\n",
      "step 221 loss 83.36669158935547\n",
      "step 222 loss 82.98892211914062\n",
      "step 223 loss 82.61175537109375\n",
      "step 224 loss 82.23242950439453\n",
      "step 225 loss 81.85159301757812\n",
      "step 226 loss 81.47039031982422\n",
      "step 227 loss 81.08456420898438\n",
      "step 228 loss 80.6956558227539\n",
      "step 229 loss 80.30138397216797\n",
      "step 230 loss 79.8986587524414\n",
      "step 231 loss 79.49235534667969\n",
      "step 232 loss 79.08706665039062\n",
      "step 233 loss 78.6797103881836\n",
      "step 234 loss 78.26995849609375\n",
      "step 235 loss 77.84721374511719\n",
      "step 236 loss 77.42063903808594\n",
      "step 237 loss 76.9921646118164\n",
      "step 238 loss 76.55973815917969\n",
      "step 239 loss 76.1259536743164\n",
      "step 240 loss 75.68678283691406\n",
      "step 241 loss 75.24768829345703\n",
      "step 242 loss 74.81066131591797\n",
      "step 243 loss 74.37467956542969\n",
      "step 244 loss 73.93660736083984\n",
      "step 245 loss 73.49687194824219\n",
      "step 246 loss 73.05793762207031\n",
      "step 247 loss 72.62171173095703\n",
      "step 248 loss 72.18700408935547\n",
      "step 249 loss 71.7497787475586\n",
      "step 250 loss 71.314697265625\n",
      "step 251 loss 70.87957000732422\n",
      "step 252 loss 70.4444808959961\n",
      "step 253 loss 70.00961303710938\n",
      "step 254 loss 69.57476806640625\n",
      "step 255 loss 69.1368179321289\n",
      "step 256 loss 68.69960021972656\n",
      "step 257 loss 68.26545715332031\n",
      "step 258 loss 67.83460998535156\n",
      "step 259 loss 67.40544128417969\n",
      "step 260 loss 66.977294921875\n",
      "step 261 loss 66.55257415771484\n",
      "step 262 loss 66.13163757324219\n",
      "step 263 loss 65.71460723876953\n",
      "step 264 loss 65.30139923095703\n",
      "step 265 loss 64.88914489746094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 266 loss 64.47679901123047\n",
      "step 267 loss 64.0664291381836\n",
      "step 268 loss 63.65703201293945\n",
      "step 269 loss 63.25095748901367\n",
      "step 270 loss 62.84831237792969\n",
      "step 271 loss 62.447635650634766\n",
      "step 272 loss 62.04997634887695\n",
      "step 273 loss 61.655460357666016\n",
      "step 274 loss 61.26431655883789\n",
      "step 275 loss 60.87666320800781\n",
      "step 276 loss 60.49245071411133\n",
      "step 277 loss 60.11139678955078\n",
      "step 278 loss 59.7333984375\n",
      "step 279 loss 59.35737228393555\n",
      "step 280 loss 58.98282241821289\n",
      "step 281 loss 58.61106872558594\n",
      "step 282 loss 58.242156982421875\n",
      "step 283 loss 57.87646484375\n",
      "step 284 loss 57.513771057128906\n",
      "step 285 loss 57.15400695800781\n",
      "step 286 loss 56.7971076965332\n",
      "step 287 loss 56.44303894042969\n",
      "step 288 loss 56.091739654541016\n",
      "step 289 loss 55.7431526184082\n",
      "step 290 loss 55.3975830078125\n",
      "step 291 loss 55.054649353027344\n",
      "step 292 loss 54.714256286621094\n",
      "step 293 loss 54.376747131347656\n",
      "step 294 loss 54.0417366027832\n",
      "step 295 loss 53.709102630615234\n",
      "step 296 loss 53.37724304199219\n",
      "step 297 loss 53.047996520996094\n",
      "step 298 loss 52.72089767456055\n",
      "step 299 loss 52.395904541015625\n",
      "step 300 loss 52.072959899902344\n",
      "step 301 loss 51.752010345458984\n",
      "step 302 loss 51.43301773071289\n",
      "step 303 loss 51.1160888671875\n",
      "step 304 loss 50.801090240478516\n",
      "step 305 loss 50.487953186035156\n",
      "step 306 loss 50.17660140991211\n",
      "step 307 loss 49.867008209228516\n",
      "step 308 loss 49.559165954589844\n",
      "step 309 loss 49.25300598144531\n",
      "step 310 loss 48.94879913330078\n",
      "step 311 loss 48.64653015136719\n",
      "step 312 loss 48.34540939331055\n",
      "step 313 loss 48.046142578125\n",
      "step 314 loss 47.748451232910156\n",
      "step 315 loss 47.4525146484375\n",
      "step 316 loss 47.157840728759766\n",
      "step 317 loss 46.864593505859375\n",
      "step 318 loss 46.57286071777344\n",
      "step 319 loss 46.2829475402832\n",
      "step 320 loss 45.994590759277344\n",
      "step 321 loss 45.70769119262695\n",
      "step 322 loss 45.4222526550293\n",
      "step 323 loss 45.13837432861328\n",
      "step 324 loss 44.855918884277344\n",
      "step 325 loss 44.574859619140625\n",
      "step 326 loss 44.29505920410156\n",
      "step 327 loss 44.0166015625\n",
      "step 328 loss 43.73948287963867\n",
      "step 329 loss 43.46371078491211\n",
      "step 330 loss 43.189266204833984\n",
      "step 331 loss 42.916500091552734\n",
      "step 332 loss 42.6451301574707\n",
      "step 333 loss 42.375099182128906\n",
      "step 334 loss 42.106239318847656\n",
      "step 335 loss 41.838706970214844\n",
      "step 336 loss 41.572471618652344\n",
      "step 337 loss 41.307655334472656\n",
      "step 338 loss 41.04423904418945\n",
      "step 339 loss 40.7822380065918\n",
      "step 340 loss 40.521610260009766\n",
      "step 341 loss 40.262290954589844\n",
      "step 342 loss 40.00423812866211\n",
      "step 343 loss 39.7474365234375\n",
      "step 344 loss 39.49201202392578\n",
      "step 345 loss 39.23796081542969\n",
      "step 346 loss 38.985206604003906\n",
      "step 347 loss 38.73371124267578\n",
      "step 348 loss 38.48359298706055\n",
      "step 349 loss 38.234893798828125\n",
      "step 350 loss 37.98773956298828\n",
      "step 351 loss 37.74189758300781\n",
      "step 352 loss 37.497535705566406\n",
      "step 353 loss 37.25463104248047\n",
      "step 354 loss 37.01298904418945\n",
      "step 355 loss 36.7725944519043\n",
      "step 356 loss 36.53342819213867\n",
      "step 357 loss 36.295475006103516\n",
      "step 358 loss 36.0588493347168\n",
      "step 359 loss 35.82364273071289\n",
      "step 360 loss 35.589698791503906\n",
      "step 361 loss 35.35694885253906\n",
      "step 362 loss 35.1254768371582\n",
      "step 363 loss 34.895145416259766\n",
      "step 364 loss 34.66590881347656\n",
      "step 365 loss 34.437828063964844\n",
      "step 366 loss 34.210899353027344\n",
      "step 367 loss 33.98525619506836\n",
      "step 368 loss 33.7604866027832\n",
      "step 369 loss 33.53715515136719\n",
      "step 370 loss 33.314971923828125\n",
      "step 371 loss 33.09406661987305\n",
      "step 372 loss 32.874488830566406\n",
      "step 373 loss 32.65605926513672\n",
      "step 374 loss 32.43870162963867\n",
      "step 375 loss 32.222450256347656\n",
      "step 376 loss 32.00733947753906\n",
      "step 377 loss 31.793493270874023\n",
      "step 378 loss 31.580862045288086\n",
      "step 379 loss 31.369556427001953\n",
      "step 380 loss 31.159687042236328\n",
      "step 381 loss 30.951274871826172\n",
      "step 382 loss 30.744029998779297\n",
      "step 383 loss 30.538114547729492\n",
      "step 384 loss 30.333593368530273\n",
      "step 385 loss 30.130477905273438\n",
      "step 386 loss 29.928565979003906\n",
      "step 387 loss 29.727800369262695\n",
      "step 388 loss 29.5282039642334\n",
      "step 389 loss 29.3299560546875\n",
      "step 390 loss 29.132837295532227\n",
      "step 391 loss 28.936948776245117\n",
      "step 392 loss 28.742403030395508\n",
      "step 393 loss 28.548995971679688\n",
      "step 394 loss 28.356679916381836\n",
      "step 395 loss 28.165616989135742\n",
      "step 396 loss 27.975805282592773\n",
      "step 397 loss 27.78741455078125\n",
      "step 398 loss 27.60015106201172\n",
      "step 399 loss 27.413990020751953\n",
      "step 400 loss 27.228919982910156\n",
      "step 401 loss 27.044933319091797\n",
      "step 402 loss 26.862010955810547\n",
      "step 403 loss 26.680147171020508\n",
      "step 404 loss 26.49942398071289\n",
      "step 405 loss 26.31989097595215\n",
      "step 406 loss 26.141742706298828\n",
      "step 407 loss 25.96468734741211\n",
      "step 408 loss 25.7886905670166\n",
      "step 409 loss 25.613964080810547\n",
      "step 410 loss 25.440576553344727\n",
      "step 411 loss 25.268362045288086\n",
      "step 412 loss 25.097211837768555\n",
      "step 413 loss 24.927217483520508\n",
      "step 414 loss 24.7586727142334\n",
      "step 415 loss 24.591516494750977\n",
      "step 416 loss 24.42560577392578\n",
      "step 417 loss 24.260793685913086\n",
      "step 418 loss 24.097030639648438\n",
      "step 419 loss 23.934297561645508\n",
      "step 420 loss 23.772581100463867\n",
      "step 421 loss 23.612022399902344\n",
      "step 422 loss 23.452524185180664\n",
      "step 423 loss 23.294160842895508\n",
      "step 424 loss 23.1368465423584\n",
      "step 425 loss 22.9807186126709\n",
      "step 426 loss 22.825864791870117\n",
      "step 427 loss 22.67200469970703\n",
      "step 428 loss 22.519136428833008\n",
      "step 429 loss 22.36724853515625\n",
      "step 430 loss 22.216449737548828\n",
      "step 431 loss 22.06674575805664\n",
      "step 432 loss 21.917997360229492\n",
      "step 433 loss 21.77020835876465\n",
      "step 434 loss 21.623641967773438\n",
      "step 435 loss 21.478593826293945\n",
      "step 436 loss 21.334716796875\n",
      "step 437 loss 21.191823959350586\n",
      "step 438 loss 21.050077438354492\n",
      "step 439 loss 20.909461975097656\n",
      "step 440 loss 20.769956588745117\n",
      "step 441 loss 20.631450653076172\n",
      "step 442 loss 20.493616104125977\n",
      "step 443 loss 20.356678009033203\n",
      "step 444 loss 20.220752716064453\n",
      "step 445 loss 20.085798263549805\n",
      "step 446 loss 19.951770782470703\n",
      "step 447 loss 19.818851470947266\n",
      "step 448 loss 19.68724822998047\n",
      "step 449 loss 19.5566463470459\n",
      "step 450 loss 19.42719078063965\n",
      "step 451 loss 19.29850959777832\n",
      "step 452 loss 19.170732498168945\n",
      "step 453 loss 19.04387855529785\n",
      "step 454 loss 18.918315887451172\n",
      "step 455 loss 18.793964385986328\n",
      "step 456 loss 18.670671463012695\n",
      "step 457 loss 18.548311233520508\n",
      "step 458 loss 18.426990509033203\n",
      "step 459 loss 18.306848526000977\n",
      "step 460 loss 18.187780380249023\n",
      "step 461 loss 18.069608688354492\n",
      "step 462 loss 17.95231819152832\n",
      "step 463 loss 17.83595085144043\n",
      "step 464 loss 17.720478057861328\n",
      "step 465 loss 17.605941772460938\n",
      "step 466 loss 17.49193000793457\n",
      "step 467 loss 17.378780364990234\n",
      "step 468 loss 17.266555786132812\n",
      "step 469 loss 17.155105590820312\n",
      "step 470 loss 17.04442596435547\n",
      "step 471 loss 16.934518814086914\n",
      "step 472 loss 16.82537078857422\n",
      "step 473 loss 16.716989517211914\n",
      "step 474 loss 16.609407424926758\n",
      "step 475 loss 16.502573013305664\n",
      "step 476 loss 16.396484375\n",
      "step 477 loss 16.29119110107422\n",
      "step 478 loss 16.1866397857666\n",
      "step 479 loss 16.082969665527344\n",
      "step 480 loss 15.980155944824219\n",
      "step 481 loss 15.878207206726074\n",
      "step 482 loss 15.777002334594727\n",
      "step 483 loss 15.676535606384277\n",
      "step 484 loss 15.576804161071777\n",
      "step 485 loss 15.477751731872559\n",
      "step 486 loss 15.379286766052246\n",
      "step 487 loss 15.281672477722168\n",
      "step 488 loss 15.184769630432129\n",
      "step 489 loss 15.088715553283691\n",
      "step 490 loss 14.993537902832031\n",
      "step 491 loss 14.899127006530762\n",
      "step 492 loss 14.805464744567871\n",
      "step 493 loss 14.712807655334473\n",
      "step 494 loss 14.620893478393555\n",
      "step 495 loss 14.529767990112305\n",
      "step 496 loss 14.439414978027344\n",
      "step 497 loss 14.349753379821777\n",
      "step 498 loss 14.260778427124023\n",
      "step 499 loss 14.172479629516602\n",
      "step 500 loss 14.084860801696777\n",
      "step 501 loss 13.9981050491333\n",
      "step 502 loss 13.911882400512695\n",
      "step 503 loss 13.826240539550781\n",
      "step 504 loss 13.74121379852295\n",
      "step 505 loss 13.656817436218262\n",
      "step 506 loss 13.573029518127441\n",
      "step 507 loss 13.489853858947754\n",
      "step 508 loss 13.407278060913086\n",
      "step 509 loss 13.325300216674805\n",
      "step 510 loss 13.243910789489746\n",
      "step 511 loss 13.163106918334961\n",
      "step 512 loss 13.082880973815918\n",
      "step 513 loss 13.003288269042969\n",
      "step 514 loss 12.9245023727417\n",
      "step 515 loss 12.846336364746094\n",
      "step 516 loss 12.768836975097656\n",
      "step 517 loss 12.691969871520996\n",
      "step 518 loss 12.615675926208496\n",
      "step 519 loss 12.539952278137207\n",
      "step 520 loss 12.464900016784668\n",
      "step 521 loss 12.390429496765137\n",
      "step 522 loss 12.316509246826172\n",
      "step 523 loss 12.243289947509766\n",
      "step 524 loss 12.170705795288086\n",
      "step 525 loss 12.09866714477539\n",
      "step 526 loss 12.02721881866455\n",
      "step 527 loss 11.956416130065918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 528 loss 11.886310577392578\n",
      "step 529 loss 11.81689739227295\n",
      "step 530 loss 11.748039245605469\n",
      "step 531 loss 11.679758071899414\n",
      "step 532 loss 11.6119966506958\n",
      "step 533 loss 11.544760704040527\n",
      "step 534 loss 11.478033065795898\n",
      "step 535 loss 11.411808967590332\n",
      "step 536 loss 11.346076965332031\n",
      "step 537 loss 11.280866622924805\n",
      "step 538 loss 11.216233253479004\n",
      "step 539 loss 11.152105331420898\n",
      "step 540 loss 11.088461875915527\n",
      "step 541 loss 11.025334358215332\n",
      "step 542 loss 10.962760925292969\n",
      "step 543 loss 10.900653839111328\n",
      "step 544 loss 10.839005470275879\n",
      "step 545 loss 10.777811050415039\n",
      "step 546 loss 10.717065811157227\n",
      "step 547 loss 10.656757354736328\n",
      "step 548 loss 10.596890449523926\n",
      "step 549 loss 10.537450790405273\n",
      "step 550 loss 10.47830581665039\n",
      "step 551 loss 10.419198989868164\n",
      "step 552 loss 10.360466003417969\n",
      "step 553 loss 10.302109718322754\n",
      "step 554 loss 10.244128227233887\n",
      "step 555 loss 10.186635971069336\n",
      "step 556 loss 10.129648208618164\n",
      "step 557 loss 10.073090553283691\n",
      "step 558 loss 10.017093658447266\n",
      "step 559 loss 9.961503982543945\n",
      "step 560 loss 9.906314849853516\n",
      "step 561 loss 9.851519584655762\n",
      "step 562 loss 9.797122955322266\n",
      "step 563 loss 9.743115425109863\n",
      "step 564 loss 9.689493179321289\n",
      "step 565 loss 9.63634204864502\n",
      "step 566 loss 9.583596229553223\n",
      "step 567 loss 9.531253814697266\n",
      "step 568 loss 9.479296684265137\n",
      "step 569 loss 9.42772102355957\n",
      "step 570 loss 9.376516342163086\n",
      "step 571 loss 9.325705528259277\n",
      "step 572 loss 9.275330543518066\n",
      "step 573 loss 9.225342750549316\n",
      "step 574 loss 9.17573070526123\n",
      "step 575 loss 9.126542091369629\n",
      "step 576 loss 9.077727317810059\n",
      "step 577 loss 9.029272079467773\n",
      "step 578 loss 8.98119831085205\n",
      "step 579 loss 8.933487892150879\n",
      "step 580 loss 8.886127471923828\n",
      "step 581 loss 8.83911418914795\n",
      "step 582 loss 8.792490005493164\n",
      "step 583 loss 8.746208190917969\n",
      "step 584 loss 8.700265884399414\n",
      "step 585 loss 8.654668807983398\n",
      "step 586 loss 8.609407424926758\n",
      "step 587 loss 8.564475059509277\n",
      "step 588 loss 8.519869804382324\n",
      "step 589 loss 8.47558879852295\n",
      "step 590 loss 8.431631088256836\n",
      "step 591 loss 8.387989044189453\n",
      "step 592 loss 8.344659805297852\n",
      "step 593 loss 8.301641464233398\n",
      "step 594 loss 8.258933067321777\n",
      "step 595 loss 8.216526985168457\n",
      "step 596 loss 8.174424171447754\n",
      "step 597 loss 8.132623672485352\n",
      "step 598 loss 8.091121673583984\n",
      "step 599 loss 8.04991340637207\n",
      "step 600 loss 8.009001731872559\n",
      "step 601 loss 7.968379497528076\n",
      "step 602 loss 7.928063869476318\n",
      "step 603 loss 7.888081073760986\n",
      "step 604 loss 7.848401069641113\n",
      "step 605 loss 7.809018611907959\n",
      "step 606 loss 7.769920825958252\n",
      "step 607 loss 7.731107711791992\n",
      "step 608 loss 7.692573070526123\n",
      "step 609 loss 7.654376029968262\n",
      "step 610 loss 7.616461277008057\n",
      "step 611 loss 7.5788254737854\n",
      "step 612 loss 7.5414652824401855\n",
      "step 613 loss 7.504376411437988\n",
      "step 614 loss 7.4675612449646\n",
      "step 615 loss 7.431011199951172\n",
      "step 616 loss 7.394726753234863\n",
      "step 617 loss 7.358706474304199\n",
      "step 618 loss 7.322947978973389\n",
      "step 619 loss 7.287451267242432\n",
      "step 620 loss 7.252230167388916\n",
      "step 621 loss 7.217267036437988\n",
      "step 622 loss 7.18255615234375\n",
      "step 623 loss 7.148101329803467\n",
      "step 624 loss 7.113893985748291\n",
      "step 625 loss 7.080067157745361\n",
      "step 626 loss 7.04653787612915\n",
      "step 627 loss 7.013263702392578\n",
      "step 628 loss 6.980240345001221\n",
      "step 629 loss 6.947469711303711\n",
      "step 630 loss 6.914942741394043\n",
      "step 631 loss 6.882659912109375\n",
      "step 632 loss 6.850618839263916\n",
      "step 633 loss 6.818815231323242\n",
      "step 634 loss 6.787248611450195\n",
      "step 635 loss 6.755918979644775\n",
      "step 636 loss 6.724816799163818\n",
      "step 637 loss 6.693948745727539\n",
      "step 638 loss 6.6633076667785645\n",
      "step 639 loss 6.632890224456787\n",
      "step 640 loss 6.602696418762207\n",
      "step 641 loss 6.572727203369141\n",
      "step 642 loss 6.542975902557373\n",
      "step 643 loss 6.513455867767334\n",
      "step 644 loss 6.484174728393555\n",
      "step 645 loss 6.4551100730896\n",
      "step 646 loss 6.426260471343994\n",
      "step 647 loss 6.397625923156738\n",
      "step 648 loss 6.369203090667725\n",
      "step 649 loss 6.340988636016846\n",
      "step 650 loss 6.312983512878418\n",
      "step 651 loss 6.28518533706665\n",
      "step 652 loss 6.257594585418701\n",
      "step 653 loss 6.230205059051514\n",
      "step 654 loss 6.2030181884765625\n",
      "step 655 loss 6.176033973693848\n",
      "step 656 loss 6.149247646331787\n",
      "step 657 loss 6.122659683227539\n",
      "step 658 loss 6.09626579284668\n",
      "step 659 loss 6.070069789886475\n",
      "step 660 loss 6.044067859649658\n",
      "step 661 loss 6.018256664276123\n",
      "step 662 loss 5.992635726928711\n",
      "step 663 loss 5.967206001281738\n",
      "step 664 loss 5.941962718963623\n",
      "step 665 loss 5.9169087409973145\n",
      "step 666 loss 5.892038345336914\n",
      "step 667 loss 5.8673553466796875\n",
      "step 668 loss 5.842853546142578\n",
      "step 669 loss 5.818535804748535\n",
      "step 670 loss 5.794449329376221\n",
      "step 671 loss 5.770599842071533\n",
      "step 672 loss 5.746937274932861\n",
      "step 673 loss 5.723459243774414\n",
      "step 674 loss 5.700160026550293\n",
      "step 675 loss 5.677041053771973\n",
      "step 676 loss 5.654098033905029\n",
      "step 677 loss 5.631332874298096\n",
      "step 678 loss 5.608738899230957\n",
      "step 679 loss 5.5863189697265625\n",
      "step 680 loss 5.5640692710876465\n",
      "step 681 loss 5.541989803314209\n",
      "step 682 loss 5.520076751708984\n",
      "step 683 loss 5.4983320236206055\n",
      "step 684 loss 5.476750373840332\n",
      "step 685 loss 5.455330848693848\n",
      "step 686 loss 5.434076309204102\n",
      "step 687 loss 5.4129815101623535\n",
      "step 688 loss 5.392067909240723\n",
      "step 689 loss 5.371303081512451\n",
      "step 690 loss 5.350772857666016\n",
      "step 691 loss 5.330408096313477\n",
      "step 692 loss 5.310204982757568\n",
      "step 693 loss 5.290158271789551\n",
      "step 694 loss 5.270267486572266\n",
      "step 695 loss 5.250531196594238\n",
      "step 696 loss 5.230947971343994\n",
      "step 697 loss 5.2115159034729\n",
      "step 698 loss 5.19223165512085\n",
      "step 699 loss 5.173128604888916\n",
      "step 700 loss 5.154179096221924\n",
      "step 701 loss 5.135402679443359\n",
      "step 702 loss 5.116783142089844\n",
      "step 703 loss 5.098362445831299\n",
      "step 704 loss 5.080087661743164\n",
      "step 705 loss 5.061956882476807\n",
      "step 706 loss 5.043993949890137\n",
      "step 707 loss 5.026182174682617\n",
      "step 708 loss 5.008511543273926\n",
      "step 709 loss 4.990981578826904\n",
      "step 710 loss 4.973588466644287\n",
      "step 711 loss 4.9563307762146\n",
      "step 712 loss 4.939206600189209\n",
      "step 713 loss 4.922215461730957\n",
      "step 714 loss 4.905356407165527\n",
      "step 715 loss 4.888688564300537\n",
      "step 716 loss 4.872162342071533\n",
      "step 717 loss 4.855767726898193\n",
      "step 718 loss 4.839500427246094\n",
      "step 719 loss 4.823392391204834\n",
      "step 720 loss 4.807408332824707\n",
      "step 721 loss 4.791550159454346\n",
      "step 722 loss 4.775815010070801\n",
      "step 723 loss 4.760202884674072\n",
      "step 724 loss 4.744709014892578\n",
      "step 725 loss 4.729335784912109\n",
      "step 726 loss 4.7140793800354\n",
      "step 727 loss 4.698939800262451\n",
      "step 728 loss 4.683916091918945\n",
      "step 729 loss 4.669004440307617\n",
      "step 730 loss 4.654207706451416\n",
      "step 731 loss 4.639523029327393\n",
      "step 732 loss 4.624948024749756\n",
      "step 733 loss 4.610483169555664\n",
      "step 734 loss 4.596126079559326\n",
      "step 735 loss 4.581879138946533\n",
      "step 736 loss 4.567739486694336\n",
      "step 737 loss 4.553704738616943\n",
      "step 738 loss 4.539775848388672\n",
      "step 739 loss 4.5259504318237305\n",
      "step 740 loss 4.512227535247803\n",
      "step 741 loss 4.498608589172363\n",
      "step 742 loss 4.485089302062988\n",
      "step 743 loss 4.471670627593994\n",
      "step 744 loss 4.458351135253906\n",
      "step 745 loss 4.445131301879883\n",
      "step 746 loss 4.432008743286133\n",
      "step 747 loss 4.4189839363098145\n",
      "step 748 loss 4.406057834625244\n",
      "step 749 loss 4.3932294845581055\n",
      "step 750 loss 4.380495071411133\n",
      "step 751 loss 4.3678693771362305\n",
      "step 752 loss 4.3553466796875\n",
      "step 753 loss 4.342919826507568\n",
      "step 754 loss 4.330585479736328\n",
      "step 755 loss 4.318343639373779\n",
      "step 756 loss 4.3061933517456055\n",
      "step 757 loss 4.294131755828857\n",
      "step 758 loss 4.282159805297852\n",
      "step 759 loss 4.27027702331543\n",
      "step 760 loss 4.258481502532959\n",
      "step 761 loss 4.2467732429504395\n",
      "step 762 loss 4.2351508140563965\n",
      "step 763 loss 4.223614692687988\n",
      "step 764 loss 4.212162494659424\n",
      "step 765 loss 4.200793266296387\n",
      "step 766 loss 4.189512729644775\n",
      "step 767 loss 4.178314685821533\n",
      "step 768 loss 4.167198657989502\n",
      "step 769 loss 4.1561689376831055\n",
      "step 770 loss 4.145221710205078\n",
      "step 771 loss 4.134355068206787\n",
      "step 772 loss 4.1235671043396\n",
      "step 773 loss 4.112856864929199\n",
      "step 774 loss 4.102225303649902\n",
      "step 775 loss 4.091670989990234\n",
      "step 776 loss 4.081192970275879\n",
      "step 777 loss 4.0707902908325195\n",
      "step 778 loss 4.0604634284973145\n",
      "step 779 loss 4.050210475921631\n",
      "step 780 loss 4.0400309562683105\n",
      "step 781 loss 4.0299248695373535\n",
      "step 782 loss 4.019891262054443\n",
      "step 783 loss 4.009929180145264\n",
      "step 784 loss 4.0000386238098145\n",
      "step 785 loss 3.9902184009552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 786 loss 3.9804675579071045\n",
      "step 787 loss 3.97078800201416\n",
      "step 788 loss 3.9611759185791016\n",
      "step 789 loss 3.951631546020508\n",
      "step 790 loss 3.9421563148498535\n",
      "step 791 loss 3.9327476024627686\n",
      "step 792 loss 3.923405170440674\n",
      "step 793 loss 3.914127826690674\n",
      "step 794 loss 3.904916286468506\n",
      "step 795 loss 3.895770311355591\n",
      "step 796 loss 3.8866894245147705\n",
      "step 797 loss 3.877671480178833\n",
      "step 798 loss 3.8687167167663574\n",
      "step 799 loss 3.859823703765869\n",
      "step 800 loss 3.8509936332702637\n",
      "step 801 loss 3.8422253131866455\n",
      "step 802 loss 3.8335182666778564\n",
      "step 803 loss 3.82487154006958\n",
      "step 804 loss 3.8162853717803955\n",
      "step 805 loss 3.8077588081359863\n",
      "step 806 loss 3.799290180206299\n",
      "step 807 loss 3.7908811569213867\n",
      "step 808 loss 3.7825310230255127\n",
      "step 809 loss 3.774237632751465\n",
      "step 810 loss 3.766000986099243\n",
      "step 811 loss 3.7578213214874268\n",
      "step 812 loss 3.7496979236602783\n",
      "step 813 loss 3.7416293621063232\n",
      "step 814 loss 3.7336173057556152\n",
      "step 815 loss 3.725659132003784\n",
      "step 816 loss 3.7177557945251465\n",
      "step 817 loss 3.7099056243896484\n",
      "step 818 loss 3.702108383178711\n",
      "step 819 loss 3.6943650245666504\n",
      "step 820 loss 3.686673641204834\n",
      "step 821 loss 3.679034948348999\n",
      "step 822 loss 3.6714470386505127\n",
      "step 823 loss 3.6639111042022705\n",
      "step 824 loss 3.6564252376556396\n",
      "step 825 loss 3.648988723754883\n",
      "step 826 loss 3.641603946685791\n",
      "step 827 loss 3.6342668533325195\n",
      "step 828 loss 3.626997947692871\n",
      "step 829 loss 3.619807481765747\n",
      "step 830 loss 3.612666606903076\n",
      "step 831 loss 3.6055755615234375\n",
      "step 832 loss 3.5985326766967773\n",
      "step 833 loss 3.5915377140045166\n",
      "step 834 loss 3.584591865539551\n",
      "step 835 loss 3.5776925086975098\n",
      "step 836 loss 3.5708417892456055\n",
      "step 837 loss 3.564035654067993\n",
      "step 838 loss 3.557279586791992\n",
      "step 839 loss 3.550572395324707\n",
      "step 840 loss 3.543910503387451\n",
      "step 841 loss 3.537295341491699\n",
      "step 842 loss 3.5307223796844482\n",
      "step 843 loss 3.524196147918701\n",
      "step 844 loss 3.517711877822876\n",
      "step 845 loss 3.5112714767456055\n",
      "step 846 loss 3.504873037338257\n",
      "step 847 loss 3.498516321182251\n",
      "step 848 loss 3.492202043533325\n",
      "step 849 loss 3.485929250717163\n",
      "step 850 loss 3.4796974658966064\n",
      "step 851 loss 3.473508358001709\n",
      "step 852 loss 3.4673566818237305\n",
      "step 853 loss 3.461245059967041\n",
      "step 854 loss 3.4551737308502197\n",
      "step 855 loss 3.449141025543213\n",
      "step 856 loss 3.443148136138916\n",
      "step 857 loss 3.4371917247772217\n",
      "step 858 loss 3.4312744140625\n",
      "step 859 loss 3.425394058227539\n",
      "step 860 loss 3.419551372528076\n",
      "step 861 loss 3.413745403289795\n",
      "step 862 loss 3.4079761505126953\n",
      "step 863 loss 3.402242660522461\n",
      "step 864 loss 3.3965439796447754\n",
      "step 865 loss 3.3908820152282715\n",
      "step 866 loss 3.3852550983428955\n",
      "step 867 loss 3.3796615600585938\n",
      "step 868 loss 3.3741042613983154\n",
      "step 869 loss 3.368579626083374\n",
      "step 870 loss 3.3630893230438232\n",
      "step 871 loss 3.3576323986053467\n",
      "step 872 loss 3.352208137512207\n",
      "step 873 loss 3.346816301345825\n",
      "step 874 loss 3.341459274291992\n",
      "step 875 loss 3.3361339569091797\n",
      "step 876 loss 3.330839157104492\n",
      "step 877 loss 3.3255774974823\n",
      "step 878 loss 3.320345401763916\n",
      "step 879 loss 3.3151450157165527\n",
      "step 880 loss 3.3099753856658936\n",
      "step 881 loss 3.304837703704834\n",
      "step 882 loss 3.2997281551361084\n",
      "step 883 loss 3.294649600982666\n",
      "step 884 loss 3.2896006107330322\n",
      "step 885 loss 3.284580707550049\n",
      "step 886 loss 3.279590129852295\n",
      "step 887 loss 3.274627685546875\n",
      "step 888 loss 3.2696950435638428\n",
      "step 889 loss 3.264788866043091\n",
      "step 890 loss 3.2599120140075684\n",
      "step 891 loss 3.2550623416900635\n",
      "step 892 loss 3.2502405643463135\n",
      "step 893 loss 3.2454452514648438\n",
      "step 894 loss 3.2406768798828125\n",
      "step 895 loss 3.235936164855957\n",
      "step 896 loss 3.2312207221984863\n",
      "step 897 loss 3.226531744003296\n",
      "step 898 loss 3.2218689918518066\n",
      "step 899 loss 3.2172319889068604\n",
      "step 900 loss 3.2126214504241943\n",
      "step 901 loss 3.2080349922180176\n",
      "step 902 loss 3.2034735679626465\n",
      "step 903 loss 3.19893741607666\n",
      "step 904 loss 3.194425344467163\n",
      "step 905 loss 3.1899378299713135\n",
      "step 906 loss 3.185474157333374\n",
      "step 907 loss 3.1810333728790283\n",
      "step 908 loss 3.176597833633423\n",
      "step 909 loss 3.1721396446228027\n",
      "step 910 loss 3.167696952819824\n",
      "step 911 loss 3.16326904296875\n",
      "step 912 loss 3.158858060836792\n",
      "step 913 loss 3.154463291168213\n",
      "step 914 loss 3.1500866413116455\n",
      "step 915 loss 3.1457276344299316\n",
      "step 916 loss 3.1413865089416504\n",
      "step 917 loss 3.137064218521118\n",
      "step 918 loss 3.132761240005493\n",
      "step 919 loss 3.1284804344177246\n",
      "step 920 loss 3.1242196559906006\n",
      "step 921 loss 3.119978904724121\n",
      "step 922 loss 3.115757942199707\n",
      "step 923 loss 3.111555814743042\n",
      "step 924 loss 3.1073737144470215\n",
      "step 925 loss 3.1032116413116455\n",
      "step 926 loss 3.099069118499756\n",
      "step 927 loss 3.094947099685669\n",
      "step 928 loss 3.0908432006835938\n",
      "step 929 loss 3.0867600440979004\n",
      "step 930 loss 3.0826947689056396\n",
      "step 931 loss 3.07865047454834\n",
      "step 932 loss 3.0746240615844727\n",
      "step 933 loss 3.07061767578125\n",
      "step 934 loss 3.066629648208618\n",
      "step 935 loss 3.062661647796631\n",
      "step 936 loss 3.058711528778076\n",
      "step 937 loss 3.054779052734375\n",
      "step 938 loss 3.0508666038513184\n",
      "step 939 loss 3.046971321105957\n",
      "step 940 loss 3.043095588684082\n",
      "step 941 loss 3.039236545562744\n",
      "step 942 loss 3.0353949069976807\n",
      "step 943 loss 3.0315725803375244\n",
      "step 944 loss 3.0277669429779053\n",
      "step 945 loss 3.0239789485931396\n",
      "step 946 loss 3.020206928253174\n",
      "step 947 loss 3.016453266143799\n",
      "step 948 loss 3.012716770172119\n",
      "step 949 loss 3.00899600982666\n",
      "step 950 loss 3.0052928924560547\n",
      "step 951 loss 3.0016050338745117\n",
      "step 952 loss 2.9979355335235596\n",
      "step 953 loss 2.9942800998687744\n",
      "step 954 loss 2.9906420707702637\n",
      "step 955 loss 2.9870190620422363\n",
      "step 956 loss 2.9834115505218506\n",
      "step 957 loss 2.979820966720581\n",
      "step 958 loss 2.9762444496154785\n",
      "step 959 loss 2.9726483821868896\n",
      "step 960 loss 2.969057083129883\n",
      "step 961 loss 2.9654767513275146\n",
      "step 962 loss 2.961907386779785\n",
      "step 963 loss 2.958347797393799\n",
      "step 964 loss 2.9547998905181885\n",
      "step 965 loss 2.9512641429901123\n",
      "step 966 loss 2.947740077972412\n",
      "step 967 loss 2.944228410720825\n",
      "step 968 loss 2.940728187561035\n",
      "step 969 loss 2.937241792678833\n",
      "step 970 loss 2.933767318725586\n",
      "step 971 loss 2.9303064346313477\n",
      "step 972 loss 2.926858425140381\n",
      "step 973 loss 2.923422336578369\n",
      "step 974 loss 2.919999599456787\n",
      "step 975 loss 2.9165902137756348\n",
      "step 976 loss 2.913195848464966\n",
      "step 977 loss 2.909818410873413\n",
      "step 978 loss 2.906453847885132\n",
      "step 979 loss 2.9031035900115967\n",
      "step 980 loss 2.8997642993927\n",
      "step 981 loss 2.8964405059814453\n",
      "step 982 loss 2.8931281566619873\n",
      "step 983 loss 2.889829158782959\n",
      "step 984 loss 2.8865411281585693\n",
      "step 985 loss 2.883268356323242\n",
      "step 986 loss 2.880007266998291\n",
      "step 987 loss 2.876757860183716\n",
      "step 988 loss 2.873521089553833\n",
      "step 989 loss 2.8702969551086426\n",
      "step 990 loss 2.867084264755249\n",
      "step 991 loss 2.8638837337493896\n",
      "step 992 loss 2.860696792602539\n",
      "step 993 loss 2.857523202896118\n",
      "step 994 loss 2.8543596267700195\n",
      "step 995 loss 2.8512086868286133\n",
      "step 996 loss 2.848068952560425\n",
      "step 997 loss 2.8449409008026123\n",
      "step 998 loss 2.8418238162994385\n",
      "step 999 loss 2.8387176990509033\n",
      "step 1000 loss 2.8356237411499023\n",
      "step 1001 loss 2.832540273666382\n",
      "step 1002 loss 2.829467535018921\n",
      "step 1003 loss 2.8264048099517822\n",
      "step 1004 loss 2.8233535289764404\n",
      "step 1005 loss 2.8203125\n",
      "step 1006 loss 2.8172824382781982\n",
      "step 1007 loss 2.814262628555298\n",
      "step 1008 loss 2.8112528324127197\n",
      "step 1009 loss 2.8082525730133057\n",
      "step 1010 loss 2.805263042449951\n",
      "step 1011 loss 2.802283525466919\n",
      "step 1012 loss 2.799314022064209\n",
      "step 1013 loss 2.7963531017303467\n",
      "step 1014 loss 2.7934038639068604\n",
      "step 1015 loss 2.7904627323150635\n",
      "step 1016 loss 2.7875006198883057\n",
      "step 1017 loss 2.7845122814178467\n",
      "step 1018 loss 2.781527042388916\n",
      "step 1019 loss 2.7785444259643555\n",
      "step 1020 loss 2.775566339492798\n",
      "step 1021 loss 2.7725918292999268\n",
      "step 1022 loss 2.769622564315796\n",
      "step 1023 loss 2.766658067703247\n",
      "step 1024 loss 2.763700485229492\n",
      "step 1025 loss 2.760746955871582\n",
      "step 1026 loss 2.7578020095825195\n",
      "step 1027 loss 2.7548630237579346\n",
      "step 1028 loss 2.7519314289093018\n",
      "step 1029 loss 2.7490079402923584\n",
      "step 1030 loss 2.746090888977051\n",
      "step 1031 loss 2.7431817054748535\n",
      "step 1032 loss 2.7402803897857666\n",
      "step 1033 loss 2.737387180328369\n",
      "step 1034 loss 2.734501361846924\n",
      "step 1035 loss 2.7316243648529053\n",
      "step 1036 loss 2.7287542819976807\n",
      "step 1037 loss 2.7258942127227783\n",
      "step 1038 loss 2.7230401039123535\n",
      "step 1039 loss 2.720196485519409\n",
      "step 1040 loss 2.7173590660095215\n",
      "step 1041 loss 2.7145299911499023\n",
      "step 1042 loss 2.7117109298706055\n",
      "step 1043 loss 2.7088983058929443\n",
      "step 1044 loss 2.7060937881469727\n",
      "step 1045 loss 2.7032971382141113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1046 loss 2.7005081176757812\n",
      "step 1047 loss 2.6977288722991943\n",
      "step 1048 loss 2.694956064224243\n",
      "step 1049 loss 2.6921911239624023\n",
      "step 1050 loss 2.6894335746765137\n",
      "step 1051 loss 2.686685085296631\n",
      "step 1052 loss 2.6839439868927\n",
      "step 1053 loss 2.6812100410461426\n",
      "step 1054 loss 2.6784842014312744\n",
      "step 1055 loss 2.6757659912109375\n",
      "step 1056 loss 2.6730542182922363\n",
      "step 1057 loss 2.6703503131866455\n",
      "step 1058 loss 2.667654037475586\n",
      "step 1059 loss 2.6649651527404785\n",
      "step 1060 loss 2.6622838973999023\n",
      "step 1061 loss 2.6596102714538574\n",
      "step 1062 loss 2.6569430828094482\n",
      "step 1063 loss 2.654282331466675\n",
      "step 1064 loss 2.651630163192749\n",
      "step 1065 loss 2.648983955383301\n",
      "step 1066 loss 2.6463451385498047\n",
      "step 1067 loss 2.6437127590179443\n",
      "step 1068 loss 2.641087532043457\n",
      "step 1069 loss 2.6384692192077637\n",
      "step 1070 loss 2.6358578205108643\n",
      "step 1071 loss 2.633251905441284\n",
      "step 1072 loss 2.6306540966033936\n",
      "step 1073 loss 2.6280622482299805\n",
      "step 1074 loss 2.625476598739624\n",
      "step 1075 loss 2.6228976249694824\n",
      "step 1076 loss 2.620326042175293\n",
      "step 1077 loss 2.6177585124969482\n",
      "step 1078 loss 2.615199327468872\n",
      "step 1079 loss 2.6126458644866943\n",
      "step 1080 loss 2.610097885131836\n",
      "step 1081 loss 2.6075563430786133\n",
      "step 1082 loss 2.6050217151641846\n",
      "step 1083 loss 2.602492332458496\n",
      "step 1084 loss 2.599968910217285\n",
      "step 1085 loss 2.597452402114868\n",
      "step 1086 loss 2.5949411392211914\n",
      "step 1087 loss 2.592435836791992\n",
      "step 1088 loss 2.5899360179901123\n",
      "step 1089 loss 2.5874416828155518\n",
      "step 1090 loss 2.584954023361206\n",
      "step 1091 loss 2.5824713706970215\n",
      "step 1092 loss 2.579993963241577\n",
      "step 1093 loss 2.5775232315063477\n",
      "step 1094 loss 2.5750572681427\n",
      "step 1095 loss 2.5725972652435303\n",
      "step 1096 loss 2.570143222808838\n",
      "step 1097 loss 2.5676932334899902\n",
      "step 1098 loss 2.565249443054199\n",
      "step 1099 loss 2.5628106594085693\n",
      "step 1100 loss 2.5603768825531006\n",
      "step 1101 loss 2.557948589324951\n",
      "step 1102 loss 2.555525064468384\n",
      "step 1103 loss 2.5531067848205566\n",
      "step 1104 loss 2.5506932735443115\n",
      "step 1105 loss 2.5482847690582275\n",
      "step 1106 loss 2.5458824634552\n",
      "step 1107 loss 2.5434844493865967\n",
      "step 1108 loss 2.5410916805267334\n",
      "step 1109 loss 2.538703441619873\n",
      "step 1110 loss 2.5363194942474365\n",
      "step 1111 loss 2.5339417457580566\n",
      "step 1112 loss 2.5315680503845215\n",
      "step 1113 loss 2.5292000770568848\n",
      "step 1114 loss 2.526836395263672\n",
      "step 1115 loss 2.5244762897491455\n",
      "step 1116 loss 2.5221219062805176\n",
      "step 1117 loss 2.5197722911834717\n",
      "step 1118 loss 2.5174267292022705\n",
      "step 1119 loss 2.5150866508483887\n",
      "step 1120 loss 2.5127511024475098\n",
      "step 1121 loss 2.5104193687438965\n",
      "step 1122 loss 2.5080928802490234\n",
      "step 1123 loss 2.505770206451416\n",
      "step 1124 loss 2.5034523010253906\n",
      "step 1125 loss 2.5011391639709473\n",
      "step 1126 loss 2.4988293647766113\n",
      "step 1127 loss 2.4965250492095947\n",
      "step 1128 loss 2.4942235946655273\n",
      "step 1129 loss 2.491875410079956\n",
      "step 1130 loss 2.4895267486572266\n",
      "step 1131 loss 2.4871773719787598\n",
      "step 1132 loss 2.484825849533081\n",
      "step 1133 loss 2.482475519180298\n",
      "step 1134 loss 2.4801251888275146\n",
      "step 1135 loss 2.4777748584747314\n",
      "step 1136 loss 2.475426435470581\n",
      "step 1137 loss 2.4730794429779053\n",
      "step 1138 loss 2.470733880996704\n",
      "step 1139 loss 2.4683902263641357\n",
      "step 1140 loss 2.4660511016845703\n",
      "step 1141 loss 2.4637134075164795\n",
      "step 1142 loss 2.461379289627075\n",
      "step 1143 loss 2.459047794342041\n",
      "step 1144 loss 2.4567205905914307\n",
      "step 1145 loss 2.4543962478637695\n",
      "step 1146 loss 2.452075719833374\n",
      "step 1147 loss 2.449759006500244\n",
      "step 1148 loss 2.447446346282959\n",
      "step 1149 loss 2.445136308670044\n",
      "step 1150 loss 2.4428319931030273\n",
      "step 1151 loss 2.440530776977539\n",
      "step 1152 loss 2.438234806060791\n",
      "step 1153 loss 2.435941696166992\n",
      "step 1154 loss 2.433652877807617\n",
      "step 1155 loss 2.431368589401245\n",
      "step 1156 loss 2.4290878772735596\n",
      "step 1157 loss 2.426811456680298\n",
      "step 1158 loss 2.42453932762146\n",
      "step 1159 loss 2.4222710132598877\n",
      "step 1160 loss 2.4200079441070557\n",
      "step 1161 loss 2.41774845123291\n",
      "step 1162 loss 2.4154927730560303\n",
      "step 1163 loss 2.413241386413574\n",
      "step 1164 loss 2.4109935760498047\n",
      "step 1165 loss 2.408750534057617\n",
      "step 1166 loss 2.4065122604370117\n",
      "step 1167 loss 2.4042770862579346\n",
      "step 1168 loss 2.402045249938965\n",
      "step 1169 loss 2.3998193740844727\n",
      "step 1170 loss 2.397596836090088\n",
      "step 1171 loss 2.395376443862915\n",
      "step 1172 loss 2.393162727355957\n",
      "step 1173 loss 2.390951156616211\n",
      "step 1174 loss 2.388744354248047\n",
      "step 1175 loss 2.386540412902832\n",
      "step 1176 loss 2.3843417167663574\n",
      "step 1177 loss 2.3821465969085693\n",
      "step 1178 loss 2.379955530166626\n",
      "step 1179 loss 2.377767562866211\n",
      "step 1180 loss 2.3755831718444824\n",
      "step 1181 loss 2.373403549194336\n",
      "step 1182 loss 2.371227741241455\n",
      "step 1183 loss 2.3690555095672607\n",
      "step 1184 loss 2.366887092590332\n",
      "step 1185 loss 2.364722967147827\n",
      "step 1186 loss 2.3625619411468506\n",
      "step 1187 loss 2.3604047298431396\n",
      "step 1188 loss 2.3582515716552734\n",
      "step 1189 loss 2.3561012744903564\n",
      "step 1190 loss 2.353954792022705\n",
      "step 1191 loss 2.3518118858337402\n",
      "step 1192 loss 2.349673271179199\n",
      "step 1193 loss 2.347537040710449\n",
      "step 1194 loss 2.3454055786132812\n",
      "step 1195 loss 2.3432767391204834\n",
      "step 1196 loss 2.341151475906372\n",
      "step 1197 loss 2.3390283584594727\n",
      "step 1198 loss 2.3369104862213135\n",
      "step 1199 loss 2.3347952365875244\n",
      "step 1200 loss 2.3326833248138428\n",
      "step 1201 loss 2.3305740356445312\n",
      "step 1202 loss 2.3284685611724854\n",
      "step 1203 loss 2.3263509273529053\n",
      "step 1204 loss 2.324183702468872\n",
      "step 1205 loss 2.3220126628875732\n",
      "step 1206 loss 2.319838047027588\n",
      "step 1207 loss 2.3176581859588623\n",
      "step 1208 loss 2.3154735565185547\n",
      "step 1209 loss 2.313286542892456\n",
      "step 1210 loss 2.311098337173462\n",
      "step 1211 loss 2.3089094161987305\n",
      "step 1212 loss 2.30672025680542\n",
      "step 1213 loss 2.304530620574951\n",
      "step 1214 loss 2.302342653274536\n",
      "step 1215 loss 2.3001556396484375\n",
      "step 1216 loss 2.2979700565338135\n",
      "step 1217 loss 2.2957863807678223\n",
      "step 1218 loss 2.2936038970947266\n",
      "step 1219 loss 2.2914254665374756\n",
      "step 1220 loss 2.289247989654541\n",
      "step 1221 loss 2.287074565887451\n",
      "step 1222 loss 2.284904718399048\n",
      "step 1223 loss 2.282738447189331\n",
      "step 1224 loss 2.280574321746826\n",
      "step 1225 loss 2.278414249420166\n",
      "step 1226 loss 2.2762575149536133\n",
      "step 1227 loss 2.2741053104400635\n",
      "step 1228 loss 2.271955966949463\n",
      "step 1229 loss 2.2698094844818115\n",
      "step 1230 loss 2.267667293548584\n",
      "step 1231 loss 2.265528917312622\n",
      "step 1232 loss 2.2633936405181885\n",
      "step 1233 loss 2.2612624168395996\n",
      "step 1234 loss 2.2591352462768555\n",
      "step 1235 loss 2.2570106983184814\n",
      "step 1236 loss 2.2548916339874268\n",
      "step 1237 loss 2.252775192260742\n",
      "step 1238 loss 2.2506628036499023\n",
      "step 1239 loss 2.248553991317749\n",
      "step 1240 loss 2.2464492321014404\n",
      "step 1241 loss 2.2443478107452393\n",
      "step 1242 loss 2.242250442504883\n",
      "step 1243 loss 2.240156412124634\n",
      "step 1244 loss 2.2380669116973877\n",
      "step 1245 loss 2.235980987548828\n",
      "step 1246 loss 2.233898401260376\n",
      "step 1247 loss 2.231818914413452\n",
      "step 1248 loss 2.229743480682373\n",
      "step 1249 loss 2.2276787757873535\n",
      "step 1250 loss 2.2256221771240234\n",
      "step 1251 loss 2.2235708236694336\n",
      "step 1252 loss 2.221522569656372\n",
      "step 1253 loss 2.2194788455963135\n",
      "step 1254 loss 2.2174391746520996\n",
      "step 1255 loss 2.215402841567993\n",
      "step 1256 loss 2.2133710384368896\n",
      "step 1257 loss 2.2113425731658936\n",
      "step 1258 loss 2.209317922592163\n",
      "step 1259 loss 2.207296848297119\n",
      "step 1260 loss 2.20527982711792\n",
      "step 1261 loss 2.20326566696167\n",
      "step 1262 loss 2.2012553215026855\n",
      "step 1263 loss 2.199249029159546\n",
      "step 1264 loss 2.1972458362579346\n",
      "step 1265 loss 2.1952450275421143\n",
      "step 1266 loss 2.1932485103607178\n",
      "step 1267 loss 2.1912548542022705\n",
      "step 1268 loss 2.1892638206481934\n",
      "step 1269 loss 2.1872775554656982\n",
      "step 1270 loss 2.185293674468994\n",
      "step 1271 loss 2.1833131313323975\n",
      "step 1272 loss 2.181335687637329\n",
      "step 1273 loss 2.179360866546631\n",
      "step 1274 loss 2.1773898601531982\n",
      "step 1275 loss 2.175421714782715\n",
      "step 1276 loss 2.173457145690918\n",
      "step 1277 loss 2.171494960784912\n",
      "step 1278 loss 2.1695358753204346\n",
      "step 1279 loss 2.167579412460327\n",
      "step 1280 loss 2.165626287460327\n",
      "step 1281 loss 2.1636765003204346\n",
      "step 1282 loss 2.161729335784912\n",
      "step 1283 loss 2.159785747528076\n",
      "step 1284 loss 2.157843589782715\n",
      "step 1285 loss 2.155905246734619\n",
      "step 1286 loss 2.1539697647094727\n",
      "step 1287 loss 2.1520373821258545\n",
      "step 1288 loss 2.1501071453094482\n",
      "step 1289 loss 2.1481804847717285\n",
      "step 1290 loss 2.1462562084198\n",
      "step 1291 loss 2.1443352699279785\n",
      "step 1292 loss 2.142416000366211\n",
      "step 1293 loss 2.140500068664551\n",
      "step 1294 loss 2.13858699798584\n",
      "step 1295 loss 2.136676549911499\n",
      "step 1296 loss 2.1347687244415283\n",
      "step 1297 loss 2.1328635215759277\n",
      "step 1298 loss 2.130960702896118\n",
      "step 1299 loss 2.129059076309204\n",
      "step 1300 loss 2.1271615028381348\n",
      "step 1301 loss 2.125265121459961\n",
      "step 1302 loss 2.1233561038970947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1303 loss 2.1213836669921875\n",
      "step 1304 loss 2.1194024085998535\n",
      "step 1305 loss 2.1174163818359375\n",
      "step 1306 loss 2.1154255867004395\n",
      "step 1307 loss 2.1134307384490967\n",
      "step 1308 loss 2.111433267593384\n",
      "step 1309 loss 2.1094346046447754\n",
      "step 1310 loss 2.1074352264404297\n",
      "step 1311 loss 2.105435848236084\n",
      "step 1312 loss 2.1034348011016846\n",
      "step 1313 loss 2.1014349460601807\n",
      "step 1314 loss 2.099435806274414\n",
      "step 1315 loss 2.0974371433258057\n",
      "step 1316 loss 2.0954396724700928\n",
      "step 1317 loss 2.0934441089630127\n",
      "step 1318 loss 2.0914499759674072\n",
      "step 1319 loss 2.0894575119018555\n",
      "step 1320 loss 2.0874674320220947\n",
      "step 1321 loss 2.085479736328125\n",
      "step 1322 loss 2.0834946632385254\n",
      "step 1323 loss 2.0815114974975586\n",
      "step 1324 loss 2.0795319080352783\n",
      "step 1325 loss 2.0775537490844727\n",
      "step 1326 loss 2.07558012008667\n",
      "step 1327 loss 2.073608636856079\n",
      "step 1328 loss 2.0716400146484375\n",
      "step 1329 loss 2.0696749687194824\n",
      "step 1330 loss 2.0677123069763184\n",
      "step 1331 loss 2.0657525062561035\n",
      "step 1332 loss 2.0637965202331543\n",
      "step 1333 loss 2.0618436336517334\n",
      "step 1334 loss 2.0598936080932617\n",
      "step 1335 loss 2.0579471588134766\n",
      "step 1336 loss 2.0560028553009033\n",
      "step 1337 loss 2.054062604904175\n",
      "step 1338 loss 2.0521247386932373\n",
      "step 1339 loss 2.0501890182495117\n",
      "step 1340 loss 2.0482559204101562\n",
      "step 1341 loss 2.0463263988494873\n",
      "step 1342 loss 2.0443990230560303\n",
      "step 1343 loss 2.042475938796997\n",
      "step 1344 loss 2.0405547618865967\n",
      "step 1345 loss 2.0386369228363037\n",
      "step 1346 loss 2.036722421646118\n",
      "step 1347 loss 2.034811019897461\n",
      "step 1348 loss 2.0329031944274902\n",
      "step 1349 loss 2.0309977531433105\n",
      "step 1350 loss 2.029095411300659\n",
      "step 1351 loss 2.027195930480957\n",
      "step 1352 loss 2.0253005027770996\n",
      "step 1353 loss 2.023407220840454\n",
      "step 1354 loss 2.021516799926758\n",
      "step 1355 loss 2.019630193710327\n",
      "step 1356 loss 2.017746925354004\n",
      "step 1357 loss 2.0158655643463135\n",
      "step 1358 loss 2.013986825942993\n",
      "step 1359 loss 2.012112617492676\n",
      "step 1360 loss 2.0102405548095703\n",
      "step 1361 loss 2.0083718299865723\n",
      "step 1362 loss 2.0065057277679443\n",
      "step 1363 loss 2.0046422481536865\n",
      "step 1364 loss 2.0027823448181152\n",
      "step 1365 loss 2.000925064086914\n",
      "step 1366 loss 1.9990713596343994\n",
      "step 1367 loss 1.997219443321228\n",
      "step 1368 loss 1.9953712224960327\n",
      "step 1369 loss 1.9935259819030762\n",
      "step 1370 loss 1.9916833639144897\n",
      "step 1371 loss 1.9898428916931152\n",
      "step 1372 loss 1.9880056381225586\n",
      "step 1373 loss 1.986171007156372\n",
      "step 1374 loss 1.984339952468872\n",
      "step 1375 loss 1.982511281967163\n",
      "step 1376 loss 1.980686068534851\n",
      "step 1377 loss 1.9788627624511719\n",
      "step 1378 loss 1.9770420789718628\n",
      "step 1379 loss 1.9752235412597656\n",
      "step 1380 loss 1.9734086990356445\n",
      "step 1381 loss 1.9715969562530518\n",
      "step 1382 loss 1.9697868824005127\n",
      "step 1383 loss 1.9679808616638184\n",
      "step 1384 loss 1.966176152229309\n",
      "step 1385 loss 1.9643751382827759\n",
      "step 1386 loss 1.962576150894165\n",
      "step 1387 loss 1.9607807397842407\n",
      "step 1388 loss 1.9589871168136597\n",
      "step 1389 loss 1.95719575881958\n",
      "step 1390 loss 1.9554073810577393\n",
      "step 1391 loss 1.9536212682724\n",
      "step 1392 loss 1.951838493347168\n",
      "step 1393 loss 1.9500579833984375\n",
      "step 1394 loss 1.9482799768447876\n",
      "step 1395 loss 1.9465044736862183\n",
      "step 1396 loss 1.9447320699691772\n",
      "step 1397 loss 1.9429614543914795\n",
      "step 1398 loss 1.9411935806274414\n",
      "step 1399 loss 1.939428448677063\n",
      "step 1400 loss 1.9376646280288696\n",
      "step 1401 loss 1.9359047412872314\n",
      "step 1402 loss 1.934146761894226\n",
      "step 1403 loss 1.93239164352417\n",
      "step 1404 loss 1.9306389093399048\n",
      "step 1405 loss 1.9288885593414307\n",
      "step 1406 loss 1.927140712738037\n",
      "step 1407 loss 1.9253945350646973\n",
      "step 1408 loss 1.923651933670044\n",
      "step 1409 loss 1.921910047531128\n",
      "step 1410 loss 1.9201724529266357\n",
      "step 1411 loss 1.9184364080429077\n",
      "step 1412 loss 1.9167028665542603\n",
      "step 1413 loss 1.9149709939956665\n",
      "step 1414 loss 1.913241982460022\n",
      "step 1415 loss 1.9115158319473267\n",
      "step 1416 loss 1.9097923040390015\n",
      "step 1417 loss 1.9080699682235718\n",
      "step 1418 loss 1.9063507318496704\n",
      "step 1419 loss 1.9046335220336914\n",
      "step 1420 loss 1.902919054031372\n",
      "step 1421 loss 1.9012057781219482\n",
      "step 1422 loss 1.8994961977005005\n",
      "step 1423 loss 1.8977879285812378\n",
      "step 1424 loss 1.8960824012756348\n",
      "step 1425 loss 1.8943792581558228\n",
      "step 1426 loss 1.8926782608032227\n",
      "step 1427 loss 1.890979290008545\n",
      "step 1428 loss 1.8892827033996582\n",
      "step 1429 loss 1.8875881433486938\n",
      "step 1430 loss 1.8858953714370728\n",
      "step 1431 loss 1.8842042684555054\n",
      "step 1432 loss 1.8825160264968872\n",
      "step 1433 loss 1.880828857421875\n",
      "step 1434 loss 1.8791444301605225\n",
      "step 1435 loss 1.8774625062942505\n",
      "step 1436 loss 1.8757827281951904\n",
      "step 1437 loss 1.874104619026184\n",
      "step 1438 loss 1.872428297996521\n",
      "step 1439 loss 1.870754599571228\n",
      "step 1440 loss 1.8690838813781738\n",
      "step 1441 loss 1.867414116859436\n",
      "step 1442 loss 1.865747332572937\n",
      "step 1443 loss 1.8640813827514648\n",
      "step 1444 loss 1.8624194860458374\n",
      "step 1445 loss 1.8607579469680786\n",
      "step 1446 loss 1.8590987920761108\n",
      "step 1447 loss 1.8574427366256714\n",
      "step 1448 loss 1.8557038307189941\n",
      "step 1449 loss 1.8539245128631592\n",
      "step 1450 loss 1.85213303565979\n",
      "step 1451 loss 1.850328803062439\n",
      "step 1452 loss 1.8485149145126343\n",
      "step 1453 loss 1.8466928005218506\n",
      "step 1454 loss 1.844862937927246\n",
      "step 1455 loss 1.8430278301239014\n",
      "step 1456 loss 1.8411874771118164\n",
      "step 1457 loss 1.8393439054489136\n",
      "step 1458 loss 1.837496280670166\n",
      "step 1459 loss 1.8356475830078125\n",
      "step 1460 loss 1.8337957859039307\n",
      "step 1461 loss 1.831943392753601\n",
      "step 1462 loss 1.8300915956497192\n",
      "step 1463 loss 1.8282389640808105\n",
      "step 1464 loss 1.826387643814087\n",
      "step 1465 loss 1.8245363235473633\n",
      "step 1466 loss 1.8226864337921143\n",
      "step 1467 loss 1.8208377361297607\n",
      "step 1468 loss 1.8189915418624878\n",
      "step 1469 loss 1.8171461820602417\n",
      "step 1470 loss 1.8153033256530762\n",
      "step 1471 loss 1.813463568687439\n",
      "step 1472 loss 1.8116250038146973\n",
      "step 1473 loss 1.8097894191741943\n",
      "step 1474 loss 1.8079562187194824\n",
      "step 1475 loss 1.806126356124878\n",
      "step 1476 loss 1.804298996925354\n",
      "step 1477 loss 1.8024743795394897\n",
      "step 1478 loss 1.8006690740585327\n",
      "step 1479 loss 1.7988964319229126\n",
      "step 1480 loss 1.7971200942993164\n",
      "step 1481 loss 1.7953382730484009\n",
      "step 1482 loss 1.7935552597045898\n",
      "step 1483 loss 1.791768193244934\n",
      "step 1484 loss 1.7899792194366455\n",
      "step 1485 loss 1.7881890535354614\n",
      "step 1486 loss 1.7863980531692505\n",
      "step 1487 loss 1.7846059799194336\n",
      "step 1488 loss 1.7828147411346436\n",
      "step 1489 loss 1.781030297279358\n",
      "step 1490 loss 1.7792729139328003\n",
      "step 1491 loss 1.7775146961212158\n",
      "step 1492 loss 1.7757558822631836\n",
      "step 1493 loss 1.7739973068237305\n",
      "step 1494 loss 1.7722389698028564\n",
      "step 1495 loss 1.7704811096191406\n",
      "step 1496 loss 1.768723726272583\n",
      "step 1497 loss 1.7669678926467896\n",
      "step 1498 loss 1.765212893486023\n",
      "step 1499 loss 1.7634652853012085\n",
      "step 1500 loss 1.7617300748825073\n",
      "step 1501 loss 1.759992241859436\n",
      "step 1502 loss 1.758253574371338\n",
      "step 1503 loss 1.7565127611160278\n",
      "step 1504 loss 1.7547799348831177\n",
      "step 1505 loss 1.7530546188354492\n",
      "step 1506 loss 1.7513303756713867\n",
      "step 1507 loss 1.7496055364608765\n",
      "step 1508 loss 1.7478816509246826\n",
      "step 1509 loss 1.7461581230163574\n",
      "step 1510 loss 1.7444440126419067\n",
      "step 1511 loss 1.7427328824996948\n",
      "step 1512 loss 1.7410197257995605\n",
      "step 1513 loss 1.739304780960083\n",
      "step 1514 loss 1.7376046180725098\n",
      "step 1515 loss 1.735904574394226\n",
      "step 1516 loss 1.7342042922973633\n",
      "step 1517 loss 1.7325053215026855\n",
      "step 1518 loss 1.7308062314987183\n",
      "step 1519 loss 1.7291086912155151\n",
      "step 1520 loss 1.7274116277694702\n",
      "step 1521 loss 1.7257330417633057\n",
      "step 1522 loss 1.7240514755249023\n",
      "step 1523 loss 1.7223659753799438\n",
      "step 1524 loss 1.720678448677063\n",
      "step 1525 loss 1.7189913988113403\n",
      "step 1526 loss 1.7173174619674683\n",
      "step 1527 loss 1.7156444787979126\n",
      "step 1528 loss 1.7139722108840942\n",
      "step 1529 loss 1.7123011350631714\n",
      "step 1530 loss 1.7106300592422485\n",
      "step 1531 loss 1.7089711427688599\n",
      "step 1532 loss 1.707310438156128\n",
      "step 1533 loss 1.7056468725204468\n",
      "step 1534 loss 1.7039860486984253\n",
      "step 1535 loss 1.7023327350616455\n",
      "step 1536 loss 1.700680136680603\n",
      "step 1537 loss 1.6990292072296143\n",
      "step 1538 loss 1.6973789930343628\n",
      "step 1539 loss 1.6957327127456665\n",
      "step 1540 loss 1.694089651107788\n",
      "step 1541 loss 1.6924477815628052\n",
      "step 1542 loss 1.6908100843429565\n",
      "step 1543 loss 1.689173698425293\n",
      "step 1544 loss 1.6875380277633667\n",
      "step 1545 loss 1.6859091520309448\n",
      "step 1546 loss 1.6842793226242065\n",
      "step 1547 loss 1.6826505661010742\n",
      "step 1548 loss 1.681026577949524\n",
      "step 1549 loss 1.679404616355896\n",
      "step 1550 loss 1.6777839660644531\n",
      "step 1551 loss 1.6761651039123535\n",
      "step 1552 loss 1.6745495796203613\n",
      "step 1553 loss 1.6729360818862915\n",
      "step 1554 loss 1.6713206768035889\n",
      "step 1555 loss 1.669690728187561\n",
      "step 1556 loss 1.6680614948272705\n",
      "step 1557 loss 1.666435718536377\n",
      "step 1558 loss 1.664808988571167\n",
      "step 1559 loss 1.6631829738616943\n",
      "step 1560 loss 1.661556363105774\n",
      "step 1561 loss 1.6599313020706177\n",
      "step 1562 loss 1.6583088636398315\n",
      "step 1563 loss 1.656689167022705\n",
      "step 1564 loss 1.6550707817077637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1565 loss 1.6534533500671387\n",
      "step 1566 loss 1.6518349647521973\n",
      "step 1567 loss 1.6502186059951782\n",
      "step 1568 loss 1.6486105918884277\n",
      "step 1569 loss 1.646999478340149\n",
      "step 1570 loss 1.6453851461410522\n",
      "step 1571 loss 1.6437804698944092\n",
      "step 1572 loss 1.642177700996399\n",
      "step 1573 loss 1.6405752897262573\n",
      "step 1574 loss 1.6389731168746948\n",
      "step 1575 loss 1.63737154006958\n",
      "step 1576 loss 1.635770320892334\n",
      "step 1577 loss 1.6341701745986938\n",
      "step 1578 loss 1.6325852870941162\n",
      "step 1579 loss 1.630996584892273\n",
      "step 1580 loss 1.6294045448303223\n",
      "step 1581 loss 1.6278094053268433\n",
      "step 1582 loss 1.626225233078003\n",
      "step 1583 loss 1.6246459484100342\n",
      "step 1584 loss 1.623043417930603\n",
      "step 1585 loss 1.621437430381775\n",
      "step 1586 loss 1.619832158088684\n",
      "step 1587 loss 1.6182268857955933\n",
      "step 1588 loss 1.6166223287582397\n",
      "step 1589 loss 1.6150178909301758\n",
      "step 1590 loss 1.6134133338928223\n",
      "step 1591 loss 1.6118102073669434\n",
      "step 1592 loss 1.6102083921432495\n",
      "step 1593 loss 1.6086071729660034\n",
      "step 1594 loss 1.6070075035095215\n",
      "step 1595 loss 1.6054089069366455\n",
      "step 1596 loss 1.6038113832473755\n",
      "step 1597 loss 1.6022158861160278\n",
      "step 1598 loss 1.6006208658218384\n",
      "step 1599 loss 1.599031686782837\n",
      "step 1600 loss 1.5974386930465698\n",
      "step 1601 loss 1.5958501100540161\n",
      "step 1602 loss 1.5942635536193848\n",
      "step 1603 loss 1.5926775932312012\n",
      "step 1604 loss 1.5910946130752563\n",
      "step 1605 loss 1.589512586593628\n",
      "step 1606 loss 1.5879336595535278\n",
      "step 1607 loss 1.5863308906555176\n",
      "step 1608 loss 1.5847300291061401\n",
      "step 1609 loss 1.5831295251846313\n",
      "step 1610 loss 1.5815287828445435\n",
      "step 1611 loss 1.5799294710159302\n",
      "step 1612 loss 1.5783299207687378\n",
      "step 1613 loss 1.5767325162887573\n",
      "step 1614 loss 1.575136423110962\n",
      "step 1615 loss 1.573540210723877\n",
      "step 1616 loss 1.5719473361968994\n",
      "step 1617 loss 1.5703544616699219\n",
      "step 1618 loss 1.5687633752822876\n",
      "step 1619 loss 1.5671753883361816\n",
      "step 1620 loss 1.5655877590179443\n",
      "step 1621 loss 1.5640029907226562\n",
      "step 1622 loss 1.5624197721481323\n",
      "step 1623 loss 1.560837984085083\n",
      "step 1624 loss 1.559259057044983\n",
      "step 1625 loss 1.557681918144226\n",
      "step 1626 loss 1.5561059713363647\n",
      "step 1627 loss 1.554532766342163\n",
      "step 1628 loss 1.5529608726501465\n",
      "step 1629 loss 1.5513910055160522\n",
      "step 1630 loss 1.5498236417770386\n",
      "step 1631 loss 1.548258662223816\n",
      "step 1632 loss 1.5466946363449097\n",
      "step 1633 loss 1.5451337099075317\n",
      "step 1634 loss 1.5435746908187866\n",
      "step 1635 loss 1.5420176982879639\n",
      "step 1636 loss 1.5404630899429321\n",
      "step 1637 loss 1.5389103889465332\n",
      "step 1638 loss 1.5373597145080566\n",
      "step 1639 loss 1.5358103513717651\n",
      "step 1640 loss 1.5342637300491333\n",
      "step 1641 loss 1.5327202081680298\n",
      "step 1642 loss 1.5311779975891113\n",
      "step 1643 loss 1.5296378135681152\n",
      "step 1644 loss 1.528099536895752\n",
      "step 1645 loss 1.5265631675720215\n",
      "step 1646 loss 1.525028944015503\n",
      "step 1647 loss 1.523497462272644\n",
      "step 1648 loss 1.5219677686691284\n",
      "step 1649 loss 1.520440697669983\n",
      "step 1650 loss 1.518915057182312\n",
      "step 1651 loss 1.5173916816711426\n",
      "step 1652 loss 1.5158705711364746\n",
      "step 1653 loss 1.5143513679504395\n",
      "step 1654 loss 1.512833833694458\n",
      "step 1655 loss 1.5113192796707153\n",
      "step 1656 loss 1.509806513786316\n",
      "step 1657 loss 1.5082955360412598\n",
      "step 1658 loss 1.5067867040634155\n",
      "step 1659 loss 1.505279541015625\n",
      "step 1660 loss 1.503777265548706\n",
      "step 1661 loss 1.5022773742675781\n",
      "step 1662 loss 1.5007797479629517\n",
      "step 1663 loss 1.499284267425537\n",
      "step 1664 loss 1.4977914094924927\n",
      "step 1665 loss 1.4963001012802124\n",
      "step 1666 loss 1.4948108196258545\n",
      "step 1667 loss 1.4933241605758667\n",
      "step 1668 loss 1.4918391704559326\n",
      "step 1669 loss 1.4903563261032104\n",
      "step 1670 loss 1.4888757467269897\n",
      "step 1671 loss 1.487396240234375\n",
      "step 1672 loss 1.4859200716018677\n",
      "step 1673 loss 1.4844454526901245\n",
      "step 1674 loss 1.4829729795455933\n",
      "step 1675 loss 1.481501817703247\n",
      "step 1676 loss 1.4800329208374023\n",
      "step 1677 loss 1.4785631895065308\n",
      "step 1678 loss 1.4770936965942383\n",
      "step 1679 loss 1.4756252765655518\n",
      "step 1680 loss 1.474159598350525\n",
      "step 1681 loss 1.4726940393447876\n",
      "step 1682 loss 1.4712311029434204\n",
      "step 1683 loss 1.4697697162628174\n",
      "step 1684 loss 1.4683095216751099\n",
      "step 1685 loss 1.4668513536453247\n",
      "step 1686 loss 1.4653950929641724\n",
      "step 1687 loss 1.4639397859573364\n",
      "step 1688 loss 1.4624866247177124\n",
      "step 1689 loss 1.4610357284545898\n",
      "step 1690 loss 1.4595861434936523\n",
      "step 1691 loss 1.4581390619277954\n",
      "step 1692 loss 1.456693172454834\n",
      "step 1693 loss 1.4552496671676636\n",
      "step 1694 loss 1.4538071155548096\n",
      "step 1695 loss 1.4523669481277466\n",
      "step 1696 loss 1.4509286880493164\n",
      "step 1697 loss 1.4494918584823608\n",
      "step 1698 loss 1.4480562210083008\n",
      "step 1699 loss 1.4466238021850586\n",
      "step 1700 loss 1.4451926946640015\n",
      "step 1701 loss 1.4437627792358398\n",
      "step 1702 loss 1.4423338174819946\n",
      "step 1703 loss 1.440907597541809\n",
      "step 1704 loss 1.4394726753234863\n",
      "step 1705 loss 1.4380123615264893\n",
      "step 1706 loss 1.4365510940551758\n",
      "step 1707 loss 1.4350875616073608\n",
      "step 1708 loss 1.4336227178573608\n",
      "step 1709 loss 1.4321576356887817\n",
      "step 1710 loss 1.4306923151016235\n",
      "step 1711 loss 1.4292259216308594\n",
      "step 1712 loss 1.427761197090149\n",
      "step 1713 loss 1.426295518875122\n",
      "step 1714 loss 1.4248313903808594\n",
      "step 1715 loss 1.4233672618865967\n",
      "step 1716 loss 1.4219049215316772\n",
      "step 1717 loss 1.4204437732696533\n",
      "step 1718 loss 1.418983817100525\n",
      "step 1719 loss 1.4175246953964233\n",
      "step 1720 loss 1.4160680770874023\n",
      "step 1721 loss 1.4146124124526978\n",
      "step 1722 loss 1.4131584167480469\n",
      "step 1723 loss 1.4117064476013184\n",
      "step 1724 loss 1.4102554321289062\n",
      "step 1725 loss 1.4088058471679688\n",
      "step 1726 loss 1.407359004020691\n",
      "step 1727 loss 1.4059131145477295\n",
      "step 1728 loss 1.4044697284698486\n",
      "step 1729 loss 1.4030280113220215\n",
      "step 1730 loss 1.4015880823135376\n",
      "step 1731 loss 1.4001506567001343\n",
      "step 1732 loss 1.3987144231796265\n",
      "step 1733 loss 1.3972810506820679\n",
      "step 1734 loss 1.3958497047424316\n",
      "step 1735 loss 1.3944201469421387\n",
      "step 1736 loss 1.3929924964904785\n",
      "step 1737 loss 1.391566514968872\n",
      "step 1738 loss 1.390142798423767\n",
      "step 1739 loss 1.3887207508087158\n",
      "step 1740 loss 1.3873012065887451\n",
      "step 1741 loss 1.3858834505081177\n",
      "step 1742 loss 1.3844680786132812\n",
      "step 1743 loss 1.383054256439209\n",
      "step 1744 loss 1.381642460823059\n",
      "step 1745 loss 1.3802334070205688\n",
      "step 1746 loss 1.378787875175476\n",
      "step 1747 loss 1.3773213624954224\n",
      "step 1748 loss 1.3758496046066284\n",
      "step 1749 loss 1.3743747472763062\n",
      "step 1750 loss 1.372896432876587\n",
      "step 1751 loss 1.3714157342910767\n",
      "step 1752 loss 1.3699339628219604\n",
      "step 1753 loss 1.3684496879577637\n",
      "step 1754 loss 1.366965651512146\n",
      "step 1755 loss 1.3654803037643433\n",
      "step 1756 loss 1.3639960289001465\n",
      "step 1757 loss 1.3625118732452393\n",
      "step 1758 loss 1.361028790473938\n",
      "step 1759 loss 1.3595465421676636\n",
      "step 1760 loss 1.3580654859542847\n",
      "step 1761 loss 1.3565865755081177\n",
      "step 1762 loss 1.3551089763641357\n",
      "step 1763 loss 1.3536326885223389\n",
      "step 1764 loss 1.3521580696105957\n",
      "step 1765 loss 1.350684404373169\n",
      "step 1766 loss 1.3492119312286377\n",
      "step 1767 loss 1.347741723060608\n",
      "step 1768 loss 1.3462742567062378\n",
      "step 1769 loss 1.3448083400726318\n",
      "step 1770 loss 1.343345284461975\n",
      "step 1771 loss 1.341883897781372\n",
      "step 1772 loss 1.340425729751587\n",
      "step 1773 loss 1.3389692306518555\n",
      "step 1774 loss 1.3375155925750732\n",
      "step 1775 loss 1.3360644578933716\n",
      "step 1776 loss 1.3346158266067505\n",
      "step 1777 loss 1.3331698179244995\n",
      "step 1778 loss 1.3317261934280396\n",
      "step 1779 loss 1.330285668373108\n",
      "step 1780 loss 1.3288471698760986\n",
      "step 1781 loss 1.3274110555648804\n",
      "step 1782 loss 1.3259780406951904\n",
      "step 1783 loss 1.3245474100112915\n",
      "step 1784 loss 1.3231194019317627\n",
      "step 1785 loss 1.3216943740844727\n",
      "step 1786 loss 1.3202717304229736\n",
      "step 1787 loss 1.3188508749008179\n",
      "step 1788 loss 1.3174331188201904\n",
      "step 1789 loss 1.3160183429718018\n",
      "step 1790 loss 1.314605712890625\n",
      "step 1791 loss 1.313195824623108\n",
      "step 1792 loss 1.3117882013320923\n",
      "step 1793 loss 1.3103834390640259\n",
      "step 1794 loss 1.3089808225631714\n",
      "step 1795 loss 1.3075811862945557\n",
      "step 1796 loss 1.3061838150024414\n",
      "step 1797 loss 1.3047884702682495\n",
      "step 1798 loss 1.3033959865570068\n",
      "step 1799 loss 1.3020057678222656\n",
      "step 1800 loss 1.300618290901184\n",
      "step 1801 loss 1.299232840538025\n",
      "step 1802 loss 1.2978500127792358\n",
      "step 1803 loss 1.2964680194854736\n",
      "step 1804 loss 1.295087456703186\n",
      "step 1805 loss 1.2937097549438477\n",
      "step 1806 loss 1.2923338413238525\n",
      "step 1807 loss 1.2909595966339111\n",
      "step 1808 loss 1.2895879745483398\n",
      "step 1809 loss 1.2882182598114014\n",
      "step 1810 loss 1.286851167678833\n",
      "step 1811 loss 1.2854866981506348\n",
      "step 1812 loss 1.2841242551803589\n",
      "step 1813 loss 1.2827643156051636\n",
      "step 1814 loss 1.2814065217971802\n",
      "step 1815 loss 1.2800512313842773\n",
      "step 1816 loss 1.2786973714828491\n",
      "step 1817 loss 1.277346134185791\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1818 loss 1.2759966850280762\n",
      "step 1819 loss 1.2746496200561523\n",
      "step 1820 loss 1.2733047008514404\n",
      "step 1821 loss 1.2719621658325195\n",
      "step 1822 loss 1.2706218957901\n",
      "step 1823 loss 1.269282341003418\n",
      "step 1824 loss 1.2679466009140015\n",
      "step 1825 loss 1.266611933708191\n",
      "step 1826 loss 1.2652775049209595\n",
      "step 1827 loss 1.2639435529708862\n",
      "step 1828 loss 1.262611746788025\n",
      "step 1829 loss 1.2612814903259277\n",
      "step 1830 loss 1.2599529027938843\n",
      "step 1831 loss 1.258626103401184\n",
      "step 1832 loss 1.2573009729385376\n",
      "step 1833 loss 1.255977988243103\n",
      "step 1834 loss 1.2546565532684326\n",
      "step 1835 loss 1.2533366680145264\n",
      "step 1836 loss 1.252018690109253\n",
      "step 1837 loss 1.2507028579711914\n",
      "step 1838 loss 1.249388575553894\n",
      "step 1839 loss 1.2480764389038086\n",
      "step 1840 loss 1.2467656135559082\n",
      "step 1841 loss 1.2454568147659302\n",
      "step 1842 loss 1.2441500425338745\n",
      "step 1843 loss 1.2428442239761353\n",
      "step 1844 loss 1.241539478302002\n",
      "step 1845 loss 1.240236759185791\n",
      "step 1846 loss 1.2389353513717651\n",
      "step 1847 loss 1.2376362085342407\n",
      "step 1848 loss 1.2363386154174805\n",
      "step 1849 loss 1.235042929649353\n",
      "step 1850 loss 1.233748197555542\n",
      "step 1851 loss 1.2324557304382324\n",
      "step 1852 loss 1.2311652898788452\n",
      "step 1853 loss 1.2298762798309326\n",
      "step 1854 loss 1.2285889387130737\n",
      "step 1855 loss 1.2273032665252686\n",
      "step 1856 loss 1.2260198593139648\n",
      "step 1857 loss 1.2247376441955566\n",
      "step 1858 loss 1.2234574556350708\n",
      "step 1859 loss 1.2221789360046387\n",
      "step 1860 loss 1.2209020853042603\n",
      "step 1861 loss 1.2196269035339355\n",
      "step 1862 loss 1.2183539867401123\n",
      "step 1863 loss 1.2170822620391846\n",
      "step 1864 loss 1.2158117294311523\n",
      "step 1865 loss 1.2145440578460693\n",
      "step 1866 loss 1.2132768630981445\n",
      "step 1867 loss 1.2120126485824585\n",
      "step 1868 loss 1.2107491493225098\n",
      "step 1869 loss 1.209487795829773\n",
      "step 1870 loss 1.2082278728485107\n",
      "step 1871 loss 1.2069697380065918\n",
      "step 1872 loss 1.2057132720947266\n",
      "step 1873 loss 1.2044585943222046\n",
      "step 1874 loss 1.20320463180542\n",
      "step 1875 loss 1.201951265335083\n",
      "step 1876 loss 1.2006995677947998\n",
      "step 1877 loss 1.1994497776031494\n",
      "step 1878 loss 1.1982008218765259\n",
      "step 1879 loss 1.1969540119171143\n",
      "step 1880 loss 1.19570791721344\n",
      "step 1881 loss 1.1944644451141357\n",
      "step 1882 loss 1.1932220458984375\n",
      "step 1883 loss 1.1919810771942139\n",
      "step 1884 loss 1.190742015838623\n",
      "step 1885 loss 1.1895039081573486\n",
      "step 1886 loss 1.1882679462432861\n",
      "step 1887 loss 1.1870338916778564\n",
      "step 1888 loss 1.1858004331588745\n",
      "step 1889 loss 1.1845687627792358\n",
      "step 1890 loss 1.1833388805389404\n",
      "step 1891 loss 1.1821101903915405\n",
      "step 1892 loss 1.1808832883834839\n",
      "step 1893 loss 1.1796575784683228\n",
      "step 1894 loss 1.1784340143203735\n",
      "step 1895 loss 1.1772122383117676\n",
      "step 1896 loss 1.1759905815124512\n",
      "step 1897 loss 1.1747716665267944\n",
      "step 1898 loss 1.1735539436340332\n",
      "step 1899 loss 1.1723376512527466\n",
      "step 1900 loss 1.1711232662200928\n",
      "step 1901 loss 1.169909954071045\n",
      "step 1902 loss 1.1686985492706299\n",
      "step 1903 loss 1.167487621307373\n",
      "step 1904 loss 1.1662793159484863\n",
      "step 1905 loss 1.1650724411010742\n",
      "step 1906 loss 1.1638662815093994\n",
      "step 1907 loss 1.1626622676849365\n",
      "step 1908 loss 1.1614598035812378\n",
      "step 1909 loss 1.1602582931518555\n",
      "step 1910 loss 1.159058690071106\n",
      "step 1911 loss 1.1578607559204102\n",
      "step 1912 loss 1.1566632986068726\n",
      "step 1913 loss 1.1554681062698364\n",
      "step 1914 loss 1.154274344444275\n",
      "step 1915 loss 1.153082013130188\n",
      "step 1916 loss 1.1518909931182861\n",
      "step 1917 loss 1.1507014036178589\n",
      "step 1918 loss 1.1495137214660645\n",
      "step 1919 loss 1.1483263969421387\n",
      "step 1920 loss 1.1471413373947144\n",
      "step 1921 loss 1.1459574699401855\n",
      "step 1922 loss 1.1447746753692627\n",
      "step 1923 loss 1.1435942649841309\n",
      "step 1924 loss 1.1424146890640259\n",
      "step 1925 loss 1.1412358283996582\n",
      "step 1926 loss 1.1400593519210815\n",
      "step 1927 loss 1.1388840675354004\n",
      "step 1928 loss 1.1377097368240356\n",
      "step 1929 loss 1.1365374326705933\n",
      "step 1930 loss 1.1353665590286255\n",
      "step 1931 loss 1.1341966390609741\n",
      "step 1932 loss 1.1330283880233765\n",
      "step 1933 loss 1.1318613290786743\n",
      "step 1934 loss 1.1306957006454468\n",
      "step 1935 loss 1.1295145750045776\n",
      "step 1936 loss 1.12831711769104\n",
      "step 1937 loss 1.1271159648895264\n",
      "step 1938 loss 1.1259119510650635\n",
      "step 1939 loss 1.1247056722640991\n",
      "step 1940 loss 1.1234973669052124\n",
      "step 1941 loss 1.122287392616272\n",
      "step 1942 loss 1.121076226234436\n",
      "step 1943 loss 1.1198631525039673\n",
      "step 1944 loss 1.1186491250991821\n",
      "step 1945 loss 1.1174349784851074\n",
      "step 1946 loss 1.1162210702896118\n",
      "step 1947 loss 1.1150060892105103\n",
      "step 1948 loss 1.1137926578521729\n",
      "step 1949 loss 1.1125794649124146\n",
      "step 1950 loss 1.1113667488098145\n",
      "step 1951 loss 1.1101552248001099\n",
      "step 1952 loss 1.108944058418274\n",
      "step 1953 loss 1.1077345609664917\n",
      "step 1954 loss 1.1065261363983154\n",
      "step 1955 loss 1.1053186655044556\n",
      "step 1956 loss 1.1041123867034912\n",
      "step 1957 loss 1.102907657623291\n",
      "step 1958 loss 1.1017040014266968\n",
      "step 1959 loss 1.1005016565322876\n",
      "step 1960 loss 1.0993008613586426\n",
      "step 1961 loss 1.0981018543243408\n",
      "step 1962 loss 1.0969041585922241\n",
      "step 1963 loss 1.0957072973251343\n",
      "step 1964 loss 1.0945124626159668\n",
      "step 1965 loss 1.093319296836853\n",
      "step 1966 loss 1.0921269655227661\n",
      "step 1967 loss 1.0909359455108643\n",
      "step 1968 loss 1.0897465944290161\n",
      "step 1969 loss 1.088558554649353\n",
      "step 1970 loss 1.0873714685440063\n",
      "step 1971 loss 1.0861868858337402\n",
      "step 1972 loss 1.0850032567977905\n",
      "step 1973 loss 1.0838218927383423\n",
      "step 1974 loss 1.0826411247253418\n",
      "step 1975 loss 1.0814623832702637\n",
      "step 1976 loss 1.0802853107452393\n",
      "step 1977 loss 1.0791091918945312\n",
      "step 1978 loss 1.0779352188110352\n",
      "step 1979 loss 1.0767621994018555\n",
      "step 1980 loss 1.0755910873413086\n",
      "step 1981 loss 1.074421763420105\n",
      "step 1982 loss 1.0732535123825073\n",
      "step 1983 loss 1.0720871686935425\n",
      "step 1984 loss 1.0709223747253418\n",
      "step 1985 loss 1.0697587728500366\n",
      "step 1986 loss 1.0685973167419434\n",
      "step 1987 loss 1.067436695098877\n",
      "step 1988 loss 1.066278338432312\n",
      "step 1989 loss 1.065120816230774\n",
      "step 1990 loss 1.0639652013778687\n",
      "step 1991 loss 1.062811017036438\n",
      "step 1992 loss 1.061658263206482\n",
      "step 1993 loss 1.0605063438415527\n",
      "step 1994 loss 1.0593554973602295\n",
      "step 1995 loss 1.0582060813903809\n",
      "step 1996 loss 1.0570579767227173\n",
      "step 1997 loss 1.0559114217758179\n",
      "step 1998 loss 1.0547658205032349\n",
      "step 1999 loss 1.0536208152770996\n",
      "step 2000 loss 1.0524765253067017\n",
      "step 2001 loss 1.051334023475647\n",
      "step 2002 loss 1.0501922369003296\n",
      "step 2003 loss 1.0490522384643555\n",
      "step 2004 loss 1.0479130744934082\n",
      "step 2005 loss 1.0467735528945923\n",
      "step 2006 loss 1.045634150505066\n",
      "step 2007 loss 1.0444964170455933\n",
      "step 2008 loss 1.0433598756790161\n",
      "step 2009 loss 1.0422242879867554\n",
      "step 2010 loss 1.041089653968811\n",
      "step 2011 loss 1.0399563312530518\n",
      "step 2012 loss 1.038824200630188\n",
      "step 2013 loss 1.0376930236816406\n",
      "step 2014 loss 1.0365626811981201\n",
      "step 2015 loss 1.035434365272522\n",
      "step 2016 loss 1.0343071222305298\n",
      "step 2017 loss 1.0331811904907227\n",
      "step 2018 loss 1.0320448875427246\n",
      "step 2019 loss 1.0308828353881836\n",
      "step 2020 loss 1.0297164916992188\n",
      "step 2021 loss 1.0285470485687256\n",
      "step 2022 loss 1.0273747444152832\n",
      "step 2023 loss 1.0261999368667603\n",
      "step 2024 loss 1.0250223875045776\n",
      "step 2025 loss 1.0238436460494995\n",
      "step 2026 loss 1.022663950920105\n",
      "step 2027 loss 1.0214831829071045\n",
      "step 2028 loss 1.020302414894104\n",
      "step 2029 loss 1.0191221237182617\n",
      "step 2030 loss 1.0179412364959717\n",
      "step 2031 loss 1.0167617797851562\n",
      "step 2032 loss 1.0155819654464722\n",
      "step 2033 loss 1.0144041776657104\n",
      "step 2034 loss 1.0132266283035278\n",
      "step 2035 loss 1.0120502710342407\n",
      "step 2036 loss 1.0108751058578491\n",
      "step 2037 loss 1.0097006559371948\n",
      "step 2038 loss 1.0085262060165405\n",
      "step 2039 loss 1.0073503255844116\n",
      "step 2040 loss 1.0061753988265991\n",
      "step 2041 loss 1.0050015449523926\n",
      "step 2042 loss 1.0038293600082397\n",
      "step 2043 loss 1.0026578903198242\n",
      "step 2044 loss 1.001487374305725\n",
      "step 2045 loss 1.0003188848495483\n",
      "step 2046 loss 0.9991517663002014\n",
      "step 2047 loss 0.9979856610298157\n",
      "step 2048 loss 0.9968206882476807\n",
      "step 2049 loss 0.995657205581665\n",
      "step 2050 loss 0.9944950938224792\n",
      "step 2051 loss 0.9933347105979919\n",
      "step 2052 loss 0.9921761155128479\n",
      "step 2053 loss 0.9910188913345337\n",
      "step 2054 loss 0.9898627996444702\n",
      "step 2055 loss 0.9887087941169739\n",
      "step 2056 loss 0.9875556230545044\n",
      "step 2057 loss 0.9864044189453125\n",
      "step 2058 loss 0.985248863697052\n",
      "step 2059 loss 0.9839667677879333\n",
      "step 2060 loss 0.9826708436012268\n",
      "step 2061 loss 0.9813624024391174\n",
      "step 2062 loss 0.9800440073013306\n",
      "step 2063 loss 0.9787161946296692\n",
      "step 2064 loss 0.9773813486099243\n",
      "step 2065 loss 0.9760403633117676\n",
      "step 2066 loss 0.9746943712234497\n",
      "step 2067 loss 0.9733443260192871\n",
      "step 2068 loss 0.9719914197921753\n",
      "step 2069 loss 0.9706369042396545\n",
      "step 2070 loss 0.9692797064781189\n",
      "step 2071 loss 0.9679223895072937\n",
      "step 2072 loss 0.9665644764900208\n",
      "step 2073 loss 0.9652064442634583\n",
      "step 2074 loss 0.9638491868972778\n",
      "step 2075 loss 0.9624927043914795\n",
      "step 2076 loss 0.9611366391181946\n",
      "step 2077 loss 0.9597822427749634\n",
      "step 2078 loss 0.9584298729896545\n",
      "step 2079 loss 0.9570780992507935\n",
      "step 2080 loss 0.955727219581604\n",
      "step 2081 loss 0.9543775320053101\n",
      "step 2082 loss 0.9530301094055176\n",
      "step 2083 loss 0.951650083065033\n",
      "step 2084 loss 0.950255811214447\n",
      "step 2085 loss 0.9488595724105835\n",
      "step 2086 loss 0.9474613666534424\n",
      "step 2087 loss 0.946061372756958\n",
      "step 2088 loss 0.9446607828140259\n",
      "step 2089 loss 0.9432601928710938\n",
      "step 2090 loss 0.9418607950210571\n",
      "step 2091 loss 0.9404623508453369\n",
      "step 2092 loss 0.9390645027160645\n",
      "step 2093 loss 0.9376691579818726\n",
      "step 2094 loss 0.9362755417823792\n",
      "step 2095 loss 0.9348838925361633\n",
      "step 2096 loss 0.9334945678710938\n",
      "step 2097 loss 0.9321081638336182\n",
      "step 2098 loss 0.9307239055633545\n",
      "step 2099 loss 0.9293429851531982\n",
      "step 2100 loss 0.9279648661613464\n",
      "step 2101 loss 0.9265894889831543\n",
      "step 2102 loss 0.9252171516418457\n",
      "step 2103 loss 0.9238480925559998\n",
      "step 2104 loss 0.9224821329116821\n",
      "step 2105 loss 0.9211196899414062\n",
      "step 2106 loss 0.9197596907615662\n",
      "step 2107 loss 0.918404221534729\n",
      "step 2108 loss 0.9170510768890381\n",
      "step 2109 loss 0.9157016277313232\n",
      "step 2110 loss 0.9143552184104919\n",
      "step 2111 loss 0.9130123257637024\n",
      "step 2112 loss 0.9116726517677307\n",
      "step 2113 loss 0.910336434841156\n",
      "step 2114 loss 0.9090033173561096\n",
      "step 2115 loss 0.9076738953590393\n",
      "step 2116 loss 0.9063473343849182\n",
      "step 2117 loss 0.9050239324569702\n",
      "step 2118 loss 0.9037044644355774\n",
      "step 2119 loss 0.902387261390686\n",
      "step 2120 loss 0.9010739922523499\n",
      "step 2121 loss 0.899763286113739\n",
      "step 2122 loss 0.898455798625946\n",
      "step 2123 loss 0.8971505761146545\n",
      "step 2124 loss 0.8958478569984436\n",
      "step 2125 loss 0.8945485949516296\n",
      "step 2126 loss 0.8932521939277649\n",
      "step 2127 loss 0.8919585347175598\n",
      "step 2128 loss 0.8906680941581726\n",
      "step 2129 loss 0.8893808126449585\n",
      "step 2130 loss 0.8880898952484131\n",
      "step 2131 loss 0.8867701888084412\n",
      "step 2132 loss 0.8854498863220215\n",
      "step 2133 loss 0.8841317892074585\n",
      "step 2134 loss 0.8828524947166443\n",
      "step 2135 loss 0.8815767765045166\n",
      "step 2136 loss 0.8803038001060486\n",
      "step 2137 loss 0.8790138363838196\n",
      "step 2138 loss 0.8777151703834534\n",
      "step 2139 loss 0.8764373064041138\n",
      "step 2140 loss 0.8751609325408936\n",
      "step 2141 loss 0.8738868832588196\n",
      "step 2142 loss 0.8726146221160889\n",
      "step 2143 loss 0.8713443279266357\n",
      "step 2144 loss 0.8700762987136841\n",
      "step 2145 loss 0.8688100576400757\n",
      "step 2146 loss 0.8675455451011658\n",
      "step 2147 loss 0.8662837743759155\n",
      "step 2148 loss 0.8650234341621399\n",
      "step 2149 loss 0.8637657165527344\n",
      "step 2150 loss 0.8625096082687378\n",
      "step 2151 loss 0.8612556457519531\n",
      "step 2152 loss 0.8600032925605774\n",
      "step 2153 loss 0.8587523102760315\n",
      "step 2154 loss 0.8575006127357483\n",
      "step 2155 loss 0.856251060962677\n",
      "step 2156 loss 0.8550031781196594\n",
      "step 2157 loss 0.8537567853927612\n",
      "step 2158 loss 0.852512538433075\n",
      "step 2159 loss 0.8512704372406006\n",
      "step 2160 loss 0.8500297665596008\n",
      "step 2161 loss 0.8487907648086548\n",
      "step 2162 loss 0.8475543260574341\n",
      "step 2163 loss 0.8463197946548462\n",
      "step 2164 loss 0.8450865149497986\n",
      "step 2165 loss 0.8438554406166077\n",
      "step 2166 loss 0.8426266312599182\n",
      "step 2167 loss 0.8413995504379272\n",
      "step 2168 loss 0.8401742577552795\n",
      "step 2169 loss 0.8389511108398438\n",
      "step 2170 loss 0.8377301096916199\n",
      "step 2171 loss 0.8365103006362915\n",
      "step 2172 loss 0.8352925777435303\n",
      "step 2173 loss 0.834077000617981\n",
      "step 2174 loss 0.8328630328178406\n",
      "step 2175 loss 0.8316512107849121\n",
      "step 2176 loss 0.8304413557052612\n",
      "step 2177 loss 0.829232931137085\n",
      "step 2178 loss 0.8280264139175415\n",
      "step 2179 loss 0.82682204246521\n",
      "step 2180 loss 0.8256195187568665\n",
      "step 2181 loss 0.8244189620018005\n",
      "step 2182 loss 0.8232195973396301\n",
      "step 2183 loss 0.8220218420028687\n",
      "step 2184 loss 0.8208259344100952\n",
      "step 2185 loss 0.8196313381195068\n",
      "step 2186 loss 0.8184393048286438\n",
      "step 2187 loss 0.8172487020492554\n",
      "step 2188 loss 0.8160598874092102\n",
      "step 2189 loss 0.8148725628852844\n",
      "step 2190 loss 0.8136874437332153\n",
      "step 2191 loss 0.8125038146972656\n",
      "step 2192 loss 0.8113216161727905\n",
      "step 2193 loss 0.8101414442062378\n",
      "step 2194 loss 0.8089632987976074\n",
      "step 2195 loss 0.8077867031097412\n",
      "step 2196 loss 0.8066120743751526\n",
      "step 2197 loss 0.8054386973381042\n",
      "step 2198 loss 0.804267406463623\n",
      "step 2199 loss 0.8030974864959717\n",
      "step 2200 loss 0.801929235458374\n",
      "step 2201 loss 0.8007623553276062\n",
      "step 2202 loss 0.7995968461036682\n",
      "step 2203 loss 0.7984335422515869\n",
      "step 2204 loss 0.7972712516784668\n",
      "step 2205 loss 0.7961107492446899\n",
      "step 2206 loss 0.7949520349502563\n",
      "step 2207 loss 0.7937949299812317\n",
      "step 2208 loss 0.7926393151283264\n",
      "step 2209 loss 0.7914859652519226\n",
      "step 2210 loss 0.79033362865448\n",
      "step 2211 loss 0.7891829013824463\n",
      "step 2212 loss 0.7880338430404663\n",
      "step 2213 loss 0.7868868112564087\n",
      "step 2214 loss 0.7857410907745361\n",
      "step 2215 loss 0.7845973372459412\n",
      "step 2216 loss 0.7834550738334656\n",
      "step 2217 loss 0.782314121723175\n",
      "step 2218 loss 0.7811750769615173\n",
      "step 2219 loss 0.7800372242927551\n",
      "step 2220 loss 0.7789016962051392\n",
      "step 2221 loss 0.7777672410011292\n",
      "step 2222 loss 0.7766350507736206\n",
      "step 2223 loss 0.7755036950111389\n",
      "step 2224 loss 0.77437424659729\n",
      "step 2225 loss 0.7732464671134949\n",
      "step 2226 loss 0.7721202969551086\n",
      "step 2227 loss 0.7709954977035522\n",
      "step 2228 loss 0.7698724269866943\n",
      "step 2229 loss 0.768750786781311\n",
      "step 2230 loss 0.7676306366920471\n",
      "step 2231 loss 0.766512930393219\n",
      "step 2232 loss 0.7653958201408386\n",
      "step 2233 loss 0.7642805576324463\n",
      "step 2234 loss 0.7631666660308838\n",
      "step 2235 loss 0.7620546221733093\n",
      "step 2236 loss 0.7609438896179199\n",
      "step 2237 loss 0.7598342299461365\n",
      "step 2238 loss 0.7587264180183411\n",
      "step 2239 loss 0.7576196193695068\n",
      "step 2240 loss 0.7565143704414368\n",
      "step 2241 loss 0.7554107904434204\n",
      "step 2242 loss 0.7543084621429443\n",
      "step 2243 loss 0.7532078623771667\n",
      "step 2244 loss 0.752108633518219\n",
      "step 2245 loss 0.7510108947753906\n",
      "step 2246 loss 0.7499144077301025\n",
      "step 2247 loss 0.7488196492195129\n",
      "step 2248 loss 0.7477262616157532\n",
      "step 2249 loss 0.7466346621513367\n",
      "step 2250 loss 0.7455442547798157\n",
      "step 2251 loss 0.7444556355476379\n",
      "step 2252 loss 0.7433680891990662\n",
      "step 2253 loss 0.742281973361969\n",
      "step 2254 loss 0.7411975264549255\n",
      "step 2255 loss 0.7401147484779358\n",
      "step 2256 loss 0.7390329241752625\n",
      "step 2257 loss 0.7379525899887085\n",
      "step 2258 loss 0.7368738651275635\n",
      "step 2259 loss 0.7357885241508484\n",
      "step 2260 loss 0.734687089920044\n",
      "step 2261 loss 0.7335845232009888\n",
      "step 2262 loss 0.7324802279472351\n",
      "step 2263 loss 0.7313754558563232\n",
      "step 2264 loss 0.7302690148353577\n",
      "step 2265 loss 0.7291620373725891\n",
      "step 2266 loss 0.7280554175376892\n",
      "step 2267 loss 0.7269489765167236\n",
      "step 2268 loss 0.7258377075195312\n",
      "step 2269 loss 0.7247248888015747\n",
      "step 2270 loss 0.7236117720603943\n",
      "step 2271 loss 0.7224992513656616\n",
      "step 2272 loss 0.7213869690895081\n",
      "step 2273 loss 0.7202751040458679\n",
      "step 2274 loss 0.7191647291183472\n",
      "step 2275 loss 0.7180545926094055\n",
      "step 2276 loss 0.7169347405433655\n",
      "step 2277 loss 0.7158001661300659\n",
      "step 2278 loss 0.7146642208099365\n",
      "step 2279 loss 0.7135267853736877\n",
      "step 2280 loss 0.7123889923095703\n",
      "step 2281 loss 0.7112511396408081\n",
      "step 2282 loss 0.7101132273674011\n",
      "step 2283 loss 0.7089747786521912\n",
      "step 2284 loss 0.7078331708908081\n",
      "step 2285 loss 0.7066928148269653\n",
      "step 2286 loss 0.7055525183677673\n",
      "step 2287 loss 0.7044131755828857\n",
      "step 2288 loss 0.7032753825187683\n",
      "step 2289 loss 0.7021375298500061\n",
      "step 2290 loss 0.7009705901145935\n",
      "step 2291 loss 0.6997964382171631\n",
      "step 2292 loss 0.6985684037208557\n",
      "step 2293 loss 0.6973325610160828\n",
      "step 2294 loss 0.6960901021957397\n",
      "step 2295 loss 0.6948091387748718\n",
      "step 2296 loss 0.6934983730316162\n",
      "step 2297 loss 0.692177414894104\n",
      "step 2298 loss 0.690848171710968\n",
      "step 2299 loss 0.6895120143890381\n",
      "step 2300 loss 0.6881678700447083\n",
      "step 2301 loss 0.6868206858634949\n",
      "step 2302 loss 0.6854679584503174\n",
      "step 2303 loss 0.6841124892234802\n",
      "step 2304 loss 0.6827558875083923\n",
      "step 2305 loss 0.6813987493515015\n",
      "step 2306 loss 0.6800427436828613\n",
      "step 2307 loss 0.6786871552467346\n",
      "step 2308 loss 0.6773329377174377\n",
      "step 2309 loss 0.6759800910949707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2310 loss 0.6746299862861633\n",
      "step 2311 loss 0.6732813119888306\n",
      "step 2312 loss 0.6719355583190918\n",
      "step 2313 loss 0.6705920696258545\n",
      "step 2314 loss 0.669251561164856\n",
      "step 2315 loss 0.6679137349128723\n",
      "step 2316 loss 0.6665793061256409\n",
      "step 2317 loss 0.6652477979660034\n",
      "step 2318 loss 0.6639196276664734\n",
      "step 2319 loss 0.6625951528549194\n",
      "step 2320 loss 0.6612762212753296\n",
      "step 2321 loss 0.6599723696708679\n",
      "step 2322 loss 0.6586729288101196\n",
      "step 2323 loss 0.6573775410652161\n",
      "step 2324 loss 0.6560867428779602\n",
      "step 2325 loss 0.6547999382019043\n",
      "step 2326 loss 0.6535167098045349\n",
      "step 2327 loss 0.6522385478019714\n",
      "step 2328 loss 0.6509637236595154\n",
      "step 2329 loss 0.6496936678886414\n",
      "step 2330 loss 0.6484273672103882\n",
      "step 2331 loss 0.647165060043335\n",
      "step 2332 loss 0.6459069848060608\n",
      "step 2333 loss 0.6446528434753418\n",
      "step 2334 loss 0.6434028148651123\n",
      "step 2335 loss 0.6421564221382141\n",
      "step 2336 loss 0.6409140229225159\n",
      "step 2337 loss 0.6396751999855042\n",
      "step 2338 loss 0.6384403109550476\n",
      "step 2339 loss 0.6372091174125671\n",
      "step 2340 loss 0.6359826922416687\n",
      "step 2341 loss 0.6347612738609314\n",
      "step 2342 loss 0.6335441470146179\n",
      "step 2343 loss 0.6323301196098328\n",
      "step 2344 loss 0.6311197876930237\n",
      "step 2345 loss 0.6299132704734802\n",
      "step 2346 loss 0.628710150718689\n",
      "step 2347 loss 0.6275105476379395\n",
      "step 2348 loss 0.6263144016265869\n",
      "step 2349 loss 0.6251214742660522\n",
      "step 2350 loss 0.6239316463470459\n",
      "step 2351 loss 0.6227455735206604\n",
      "step 2352 loss 0.6215618848800659\n",
      "step 2353 loss 0.6203821897506714\n",
      "step 2354 loss 0.619205117225647\n",
      "step 2355 loss 0.6180222034454346\n",
      "step 2356 loss 0.6168391704559326\n",
      "step 2357 loss 0.6156577467918396\n",
      "step 2358 loss 0.6144800782203674\n",
      "step 2359 loss 0.6133066415786743\n",
      "step 2360 loss 0.6121354103088379\n",
      "step 2361 loss 0.6109665036201477\n",
      "step 2362 loss 0.6098011136054993\n",
      "step 2363 loss 0.6086373925209045\n",
      "step 2364 loss 0.6074769496917725\n",
      "step 2365 loss 0.6063187718391418\n",
      "step 2366 loss 0.6051634550094604\n",
      "step 2367 loss 0.6040090918540955\n",
      "step 2368 loss 0.6028571128845215\n",
      "step 2369 loss 0.6017073392868042\n",
      "step 2370 loss 0.600560188293457\n",
      "step 2371 loss 0.59941565990448\n",
      "step 2372 loss 0.5982736349105835\n",
      "step 2373 loss 0.5971342325210571\n",
      "step 2374 loss 0.5959973931312561\n",
      "step 2375 loss 0.5948633551597595\n",
      "step 2376 loss 0.5937313437461853\n",
      "step 2377 loss 0.5926022529602051\n",
      "step 2378 loss 0.5914762020111084\n",
      "step 2379 loss 0.5903538465499878\n",
      "step 2380 loss 0.5892340540885925\n",
      "step 2381 loss 0.5881165862083435\n",
      "step 2382 loss 0.587001621723175\n",
      "step 2383 loss 0.5858889222145081\n",
      "step 2384 loss 0.5847788453102112\n",
      "step 2385 loss 0.5836710333824158\n",
      "step 2386 loss 0.5825654864311218\n",
      "step 2387 loss 0.5814626812934875\n",
      "step 2388 loss 0.5803612470626831\n",
      "step 2389 loss 0.5792620182037354\n",
      "step 2390 loss 0.57816481590271\n",
      "step 2391 loss 0.5770699977874756\n",
      "step 2392 loss 0.5759773254394531\n",
      "step 2393 loss 0.5748874545097351\n",
      "step 2394 loss 0.5737993121147156\n",
      "step 2395 loss 0.5727132558822632\n",
      "step 2396 loss 0.5716303586959839\n",
      "step 2397 loss 0.5705492496490479\n",
      "step 2398 loss 0.5694704651832581\n",
      "step 2399 loss 0.5683941841125488\n",
      "step 2400 loss 0.5673196315765381\n",
      "step 2401 loss 0.5662477612495422\n",
      "step 2402 loss 0.5651779770851135\n",
      "step 2403 loss 0.5641096830368042\n",
      "step 2404 loss 0.5630433559417725\n",
      "step 2405 loss 0.5619803667068481\n",
      "step 2406 loss 0.5609190464019775\n",
      "step 2407 loss 0.5598600506782532\n",
      "step 2408 loss 0.5588035583496094\n",
      "step 2409 loss 0.5577489733695984\n",
      "step 2410 loss 0.5566970109939575\n",
      "step 2411 loss 0.555647075176239\n",
      "step 2412 loss 0.554598867893219\n",
      "step 2413 loss 0.5535526871681213\n",
      "step 2414 loss 0.5525071620941162\n",
      "step 2415 loss 0.5514640808105469\n",
      "step 2416 loss 0.5504224896430969\n",
      "step 2417 loss 0.5493830442428589\n",
      "step 2418 loss 0.5483457446098328\n",
      "step 2419 loss 0.547310471534729\n",
      "step 2420 loss 0.5462771654129028\n",
      "step 2421 loss 0.5452459454536438\n",
      "step 2422 loss 0.5442168712615967\n",
      "step 2423 loss 0.5431897640228271\n",
      "step 2424 loss 0.5421647429466248\n",
      "step 2425 loss 0.5411415100097656\n",
      "step 2426 loss 0.5401208400726318\n",
      "step 2427 loss 0.5391106605529785\n",
      "step 2428 loss 0.538108766078949\n",
      "step 2429 loss 0.5371093153953552\n",
      "step 2430 loss 0.5361117720603943\n",
      "step 2431 loss 0.5351165533065796\n",
      "step 2432 loss 0.5341228246688843\n",
      "step 2433 loss 0.5331316590309143\n",
      "step 2434 loss 0.5321420431137085\n",
      "step 2435 loss 0.5311526656150818\n",
      "step 2436 loss 0.5301647782325745\n",
      "step 2437 loss 0.5291792154312134\n",
      "step 2438 loss 0.5281952619552612\n",
      "step 2439 loss 0.5272131562232971\n",
      "step 2440 loss 0.5262328386306763\n",
      "step 2441 loss 0.5252545475959778\n",
      "step 2442 loss 0.524277925491333\n",
      "step 2443 loss 0.5233036279678345\n",
      "step 2444 loss 0.5223307609558105\n",
      "step 2445 loss 0.521359920501709\n",
      "step 2446 loss 0.5203909873962402\n",
      "step 2447 loss 0.5194224715232849\n",
      "step 2448 loss 0.5184556841850281\n",
      "step 2449 loss 0.5174906849861145\n",
      "step 2450 loss 0.5165268778800964\n",
      "step 2451 loss 0.5155646800994873\n",
      "step 2452 loss 0.514604389667511\n",
      "step 2453 loss 0.5136455297470093\n",
      "step 2454 loss 0.5126883387565613\n",
      "step 2455 loss 0.5117327570915222\n",
      "step 2456 loss 0.5107787847518921\n",
      "step 2457 loss 0.5098265409469604\n",
      "step 2458 loss 0.5088757872581482\n",
      "step 2459 loss 0.5079266428947449\n",
      "step 2460 loss 0.5069788098335266\n",
      "step 2461 loss 0.5060330033302307\n",
      "step 2462 loss 0.5050885081291199\n",
      "step 2463 loss 0.5041458606719971\n",
      "step 2464 loss 0.5032045841217041\n",
      "step 2465 loss 0.502265214920044\n",
      "step 2466 loss 0.5013267993927002\n",
      "step 2467 loss 0.5003899931907654\n",
      "step 2468 loss 0.4994547963142395\n",
      "step 2469 loss 0.4985208809375763\n",
      "step 2470 loss 0.49758896231651306\n",
      "step 2471 loss 0.4966583549976349\n",
      "step 2472 loss 0.4957294464111328\n",
      "step 2473 loss 0.4948020875453949\n",
      "step 2474 loss 0.49387505650520325\n",
      "step 2475 loss 0.4929482936859131\n",
      "step 2476 loss 0.4920230507850647\n",
      "step 2477 loss 0.4910999536514282\n",
      "step 2478 loss 0.4901781380176544\n",
      "step 2479 loss 0.4892582595348358\n",
      "step 2480 loss 0.4883396029472351\n",
      "step 2481 loss 0.48742276430130005\n",
      "step 2482 loss 0.486506849527359\n",
      "step 2483 loss 0.4855926036834717\n",
      "step 2484 loss 0.48468026518821716\n",
      "step 2485 loss 0.4837692975997925\n",
      "step 2486 loss 0.4828597903251648\n",
      "step 2487 loss 0.48195183277130127\n",
      "step 2488 loss 0.48104432225227356\n",
      "step 2489 loss 0.48013877868652344\n",
      "step 2490 loss 0.47923439741134644\n",
      "step 2491 loss 0.4783318340778351\n",
      "step 2492 loss 0.47743046283721924\n",
      "step 2493 loss 0.4765307605266571\n",
      "step 2494 loss 0.4756326377391815\n",
      "step 2495 loss 0.47473570704460144\n",
      "step 2496 loss 0.4738404154777527\n",
      "step 2497 loss 0.4729467034339905\n",
      "step 2498 loss 0.4720543324947357\n",
      "step 2499 loss 0.47116371989250183\n",
      "step 2500 loss 0.4702746272087097\n",
      "step 2501 loss 0.46938690543174744\n",
      "step 2502 loss 0.4685005843639374\n",
      "step 2503 loss 0.46761608123779297\n",
      "step 2504 loss 0.46673300862312317\n",
      "step 2505 loss 0.46585121750831604\n",
      "step 2506 loss 0.4649711549282074\n",
      "step 2507 loss 0.4640924334526062\n",
      "step 2508 loss 0.46321582794189453\n",
      "step 2509 loss 0.46233999729156494\n",
      "step 2510 loss 0.46146613359451294\n",
      "step 2511 loss 0.46059367060661316\n",
      "step 2512 loss 0.4597229063510895\n",
      "step 2513 loss 0.45885345339775085\n",
      "step 2514 loss 0.45798546075820923\n",
      "step 2515 loss 0.4571187496185303\n",
      "step 2516 loss 0.4562523663043976\n",
      "step 2517 loss 0.45538756251335144\n",
      "step 2518 loss 0.45452433824539185\n",
      "step 2519 loss 0.4536624848842621\n",
      "step 2520 loss 0.4528014361858368\n",
      "step 2521 loss 0.4519425630569458\n",
      "step 2522 loss 0.4510847330093384\n",
      "step 2523 loss 0.45022866129875183\n",
      "step 2524 loss 0.44937369227409363\n",
      "step 2525 loss 0.4485204815864563\n",
      "step 2526 loss 0.4476686716079712\n",
      "step 2527 loss 0.446818083524704\n",
      "step 2528 loss 0.4459690749645233\n",
      "step 2529 loss 0.44512179493904114\n",
      "step 2530 loss 0.44427573680877686\n",
      "step 2531 loss 0.4434310793876648\n",
      "step 2532 loss 0.44258835911750793\n",
      "step 2533 loss 0.4417467415332794\n",
      "step 2534 loss 0.4409065246582031\n",
      "step 2535 loss 0.4400681257247925\n",
      "step 2536 loss 0.4392307996749878\n",
      "step 2537 loss 0.4383949339389801\n",
      "step 2538 loss 0.4375605881214142\n",
      "step 2539 loss 0.43672776222229004\n",
      "step 2540 loss 0.4358963668346405\n",
      "step 2541 loss 0.43506595492362976\n",
      "step 2542 loss 0.4342362582683563\n",
      "step 2543 loss 0.43340790271759033\n",
      "step 2544 loss 0.43258094787597656\n",
      "step 2545 loss 0.4317552149295807\n",
      "step 2546 loss 0.4309314489364624\n",
      "step 2547 loss 0.4301091134548187\n",
      "step 2548 loss 0.4292878806591034\n",
      "step 2549 loss 0.4284683167934418\n",
      "step 2550 loss 0.4276505410671234\n",
      "step 2551 loss 0.4268334209918976\n",
      "step 2552 loss 0.4260183572769165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2553 loss 0.42520445585250854\n",
      "step 2554 loss 0.42439204454421997\n",
      "step 2555 loss 0.4235811233520508\n",
      "step 2556 loss 0.4227716028690338\n",
      "step 2557 loss 0.42196372151374817\n",
      "step 2558 loss 0.4211571514606476\n",
      "step 2559 loss 0.4203520715236664\n",
      "step 2560 loss 0.41954681277275085\n",
      "step 2561 loss 0.41874271631240845\n",
      "step 2562 loss 0.4179399907588959\n",
      "step 2563 loss 0.41713854670524597\n",
      "step 2564 loss 0.4163375198841095\n",
      "step 2565 loss 0.4155356287956238\n",
      "step 2566 loss 0.4147341847419739\n",
      "step 2567 loss 0.4139339327812195\n",
      "step 2568 loss 0.41313526034355164\n",
      "step 2569 loss 0.41233712434768677\n",
      "step 2570 loss 0.4115401804447174\n",
      "step 2571 loss 0.4107445180416107\n",
      "step 2572 loss 0.4099500775337219\n",
      "step 2573 loss 0.40915703773498535\n",
      "step 2574 loss 0.40836453437805176\n",
      "step 2575 loss 0.40757331252098083\n",
      "step 2576 loss 0.4067833423614502\n",
      "step 2577 loss 0.4059946835041046\n",
      "step 2578 loss 0.4052071273326874\n",
      "step 2579 loss 0.4044208526611328\n",
      "step 2580 loss 0.4036356210708618\n",
      "step 2581 loss 0.4028524160385132\n",
      "step 2582 loss 0.4020700752735138\n",
      "step 2583 loss 0.4012894332408905\n",
      "step 2584 loss 0.4005099833011627\n",
      "step 2585 loss 0.39973199367523193\n",
      "step 2586 loss 0.39895516633987427\n",
      "step 2587 loss 0.39818018674850464\n",
      "step 2588 loss 0.3974064290523529\n",
      "step 2589 loss 0.39663413166999817\n",
      "step 2590 loss 0.3958633542060852\n",
      "step 2591 loss 0.3950938284397125\n",
      "step 2592 loss 0.39432576298713684\n",
      "step 2593 loss 0.39355918765068054\n",
      "step 2594 loss 0.3927941918373108\n",
      "step 2595 loss 0.3920309543609619\n",
      "step 2596 loss 0.3912692368030548\n",
      "step 2597 loss 0.3905089497566223\n",
      "step 2598 loss 0.38975030183792114\n",
      "step 2599 loss 0.3889927268028259\n",
      "step 2600 loss 0.3882368803024292\n",
      "step 2601 loss 0.3874821364879608\n",
      "step 2602 loss 0.3867272436618805\n",
      "step 2603 loss 0.3859739303588867\n",
      "step 2604 loss 0.38522201776504517\n",
      "step 2605 loss 0.38447150588035583\n",
      "step 2606 loss 0.3837221562862396\n",
      "step 2607 loss 0.382974237203598\n",
      "step 2608 loss 0.38222765922546387\n",
      "step 2609 loss 0.38148266077041626\n",
      "step 2610 loss 0.3807387053966522\n",
      "step 2611 loss 0.37999647855758667\n",
      "step 2612 loss 0.3792557120323181\n",
      "step 2613 loss 0.3785161077976227\n",
      "step 2614 loss 0.37777790427207947\n",
      "step 2615 loss 0.3770412504673004\n",
      "step 2616 loss 0.3762846291065216\n",
      "step 2617 loss 0.3755136728286743\n",
      "step 2618 loss 0.3747403621673584\n",
      "step 2619 loss 0.37396466732025146\n",
      "step 2620 loss 0.37318703532218933\n",
      "step 2621 loss 0.3724079132080078\n",
      "step 2622 loss 0.3716280460357666\n",
      "step 2623 loss 0.3708474338054657\n",
      "step 2624 loss 0.3700662851333618\n",
      "step 2625 loss 0.3692856729030609\n",
      "step 2626 loss 0.3685050308704376\n",
      "step 2627 loss 0.3677251935005188\n",
      "step 2628 loss 0.36694562435150146\n",
      "step 2629 loss 0.3661666512489319\n",
      "step 2630 loss 0.3653884530067444\n",
      "step 2631 loss 0.3646145761013031\n",
      "step 2632 loss 0.363841712474823\n",
      "step 2633 loss 0.3630707561969757\n",
      "step 2634 loss 0.3623008728027344\n",
      "step 2635 loss 0.361532062292099\n",
      "step 2636 loss 0.360765278339386\n",
      "step 2637 loss 0.3599946200847626\n",
      "step 2638 loss 0.35919201374053955\n",
      "step 2639 loss 0.35838717222213745\n",
      "step 2640 loss 0.3575807213783264\n",
      "step 2641 loss 0.35677292943000793\n",
      "step 2642 loss 0.35596421360969543\n",
      "step 2643 loss 0.3551551103591919\n",
      "step 2644 loss 0.3543456196784973\n",
      "step 2645 loss 0.3535361886024475\n",
      "step 2646 loss 0.35272717475891113\n",
      "step 2647 loss 0.35191860795021057\n",
      "step 2648 loss 0.35111069679260254\n",
      "step 2649 loss 0.3503037989139557\n",
      "step 2650 loss 0.3494977653026581\n",
      "step 2651 loss 0.348692923784256\n",
      "step 2652 loss 0.3478895127773285\n",
      "step 2653 loss 0.3470873236656189\n",
      "step 2654 loss 0.34628674387931824\n",
      "step 2655 loss 0.3454875648021698\n",
      "step 2656 loss 0.3446899652481079\n",
      "step 2657 loss 0.3438939154148102\n",
      "step 2658 loss 0.3430997431278229\n",
      "step 2659 loss 0.3423071801662445\n",
      "step 2660 loss 0.341516375541687\n",
      "step 2661 loss 0.3407275378704071\n",
      "step 2662 loss 0.33994045853614807\n",
      "step 2663 loss 0.3391551971435547\n",
      "step 2664 loss 0.33837220072746277\n",
      "step 2665 loss 0.3375909924507141\n",
      "step 2666 loss 0.3368116021156311\n",
      "step 2667 loss 0.3360343277454376\n",
      "step 2668 loss 0.33525916934013367\n",
      "step 2669 loss 0.3344860374927521\n",
      "step 2670 loss 0.33371472358703613\n",
      "step 2671 loss 0.33294564485549927\n",
      "step 2672 loss 0.3321789503097534\n",
      "step 2673 loss 0.33140450716018677\n",
      "step 2674 loss 0.33062782883644104\n",
      "step 2675 loss 0.3298512399196625\n",
      "step 2676 loss 0.32907572388648987\n",
      "step 2677 loss 0.3283008635044098\n",
      "step 2678 loss 0.3275274336338043\n",
      "step 2679 loss 0.32675543427467346\n",
      "step 2680 loss 0.32598477602005005\n",
      "step 2681 loss 0.32521578669548035\n",
      "step 2682 loss 0.3244481682777405\n",
      "step 2683 loss 0.32368236780166626\n",
      "step 2684 loss 0.3229183256626129\n",
      "step 2685 loss 0.32215622067451477\n",
      "step 2686 loss 0.32139572501182556\n",
      "step 2687 loss 0.3206372857093811\n",
      "step 2688 loss 0.31987956166267395\n",
      "step 2689 loss 0.3191220760345459\n",
      "step 2690 loss 0.3183663487434387\n",
      "step 2691 loss 0.31761232018470764\n",
      "step 2692 loss 0.3168600797653198\n",
      "step 2693 loss 0.3161100149154663\n",
      "step 2694 loss 0.3153618574142456\n",
      "step 2695 loss 0.3146154284477234\n",
      "step 2696 loss 0.31387126445770264\n",
      "step 2697 loss 0.3131289780139923\n",
      "step 2698 loss 0.3123885691165924\n",
      "step 2699 loss 0.31165027618408203\n",
      "step 2700 loss 0.3109140396118164\n",
      "step 2701 loss 0.3101794421672821\n",
      "step 2702 loss 0.3094474673271179\n",
      "step 2703 loss 0.30871713161468506\n",
      "step 2704 loss 0.3079887926578522\n",
      "step 2705 loss 0.3072628676891327\n",
      "step 2706 loss 0.3065386116504669\n",
      "step 2707 loss 0.30581653118133545\n",
      "step 2708 loss 0.30509626865386963\n",
      "step 2709 loss 0.3043783903121948\n",
      "step 2710 loss 0.30366238951683044\n",
      "step 2711 loss 0.302948534488678\n",
      "step 2712 loss 0.3022366464138031\n",
      "step 2713 loss 0.30152636766433716\n",
      "step 2714 loss 0.3008177578449249\n",
      "step 2715 loss 0.3001108467578888\n",
      "step 2716 loss 0.29940590262413025\n",
      "step 2717 loss 0.29870304465293884\n",
      "step 2718 loss 0.2980019152164459\n",
      "step 2719 loss 0.29730290174484253\n",
      "step 2720 loss 0.29660606384277344\n",
      "step 2721 loss 0.29591065645217896\n",
      "step 2722 loss 0.29521769285202026\n",
      "step 2723 loss 0.294526606798172\n",
      "step 2724 loss 0.29383718967437744\n",
      "step 2725 loss 0.2931500971317291\n",
      "step 2726 loss 0.29246488213539124\n",
      "step 2727 loss 0.2917814254760742\n",
      "step 2728 loss 0.2911001145839691\n",
      "step 2729 loss 0.2904203236103058\n",
      "step 2730 loss 0.28974291682243347\n",
      "step 2731 loss 0.28906717896461487\n",
      "step 2732 loss 0.2883920669555664\n",
      "step 2733 loss 0.2877078950405121\n",
      "step 2734 loss 0.2870256006717682\n",
      "step 2735 loss 0.28634414076805115\n",
      "step 2736 loss 0.2856637239456177\n",
      "step 2737 loss 0.2849847376346588\n",
      "step 2738 loss 0.2843070924282074\n",
      "step 2739 loss 0.28363120555877686\n",
      "step 2740 loss 0.28295689821243286\n",
      "step 2741 loss 0.2822836935520172\n",
      "step 2742 loss 0.2816125154495239\n",
      "step 2743 loss 0.28094303607940674\n",
      "step 2744 loss 0.28027498722076416\n",
      "step 2745 loss 0.2796086370944977\n",
      "step 2746 loss 0.2789439857006073\n",
      "step 2747 loss 0.278281033039093\n",
      "step 2748 loss 0.27761998772621155\n",
      "step 2749 loss 0.27696049213409424\n",
      "step 2750 loss 0.2763030230998993\n",
      "step 2751 loss 0.27564698457717896\n",
      "step 2752 loss 0.27499303221702576\n",
      "step 2753 loss 0.27434080839157104\n",
      "step 2754 loss 0.27369019389152527\n",
      "step 2755 loss 0.27304160594940186\n",
      "step 2756 loss 0.2723945379257202\n",
      "step 2757 loss 0.27174967527389526\n",
      "step 2758 loss 0.27110642194747925\n",
      "step 2759 loss 0.27046501636505127\n",
      "step 2760 loss 0.2698255479335785\n",
      "step 2761 loss 0.26918938755989075\n",
      "step 2762 loss 0.2685551345348358\n",
      "step 2763 loss 0.26792284846305847\n",
      "step 2764 loss 0.2672922611236572\n",
      "step 2765 loss 0.2666638195514679\n",
      "step 2766 loss 0.26603689789772034\n",
      "step 2767 loss 0.2654120922088623\n",
      "step 2768 loss 0.2647891640663147\n",
      "step 2769 loss 0.26416802406311035\n",
      "step 2770 loss 0.26354867219924927\n",
      "step 2771 loss 0.26293128728866577\n",
      "step 2772 loss 0.26231539249420166\n",
      "step 2773 loss 0.2617015838623047\n",
      "step 2774 loss 0.261089563369751\n",
      "step 2775 loss 0.26047906279563904\n",
      "step 2776 loss 0.2598705291748047\n",
      "step 2777 loss 0.25926390290260315\n",
      "step 2778 loss 0.25865882635116577\n",
      "step 2779 loss 0.2580556571483612\n",
      "step 2780 loss 0.2574540674686432\n",
      "step 2781 loss 0.2568545639514923\n",
      "step 2782 loss 0.2562564015388489\n",
      "step 2783 loss 0.2556602358818054\n",
      "step 2784 loss 0.25506553053855896\n",
      "step 2785 loss 0.25447285175323486\n",
      "step 2786 loss 0.25388190150260925\n",
      "step 2787 loss 0.2532927989959717\n",
      "step 2788 loss 0.2526950538158417\n",
      "step 2789 loss 0.2520918548107147\n",
      "step 2790 loss 0.25148865580558777\n",
      "step 2791 loss 0.25088587403297424\n",
      "step 2792 loss 0.25028344988822937\n",
      "step 2793 loss 0.24968159198760986\n",
      "step 2794 loss 0.24908064305782318\n",
      "step 2795 loss 0.24848075211048126\n",
      "step 2796 loss 0.2478816956281662\n",
      "step 2797 loss 0.24728374183177948\n",
      "step 2798 loss 0.24668723344802856\n",
      "step 2799 loss 0.24609211087226868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2800 loss 0.24549803137779236\n",
      "step 2801 loss 0.24490566551685333\n",
      "step 2802 loss 0.24431468546390533\n",
      "step 2803 loss 0.24372521042823792\n",
      "step 2804 loss 0.24313724040985107\n",
      "step 2805 loss 0.2425510734319687\n",
      "step 2806 loss 0.2419661283493042\n",
      "step 2807 loss 0.24138282239437103\n",
      "step 2808 loss 0.24080124497413635\n",
      "step 2809 loss 0.24021793901920319\n",
      "step 2810 loss 0.23960262537002563\n",
      "step 2811 loss 0.2389853298664093\n",
      "step 2812 loss 0.2383667230606079\n",
      "step 2813 loss 0.23774723708629608\n",
      "step 2814 loss 0.2371266782283783\n",
      "step 2815 loss 0.23650570213794708\n",
      "step 2816 loss 0.235884889960289\n",
      "step 2817 loss 0.235264390707016\n",
      "step 2818 loss 0.2346443235874176\n",
      "step 2819 loss 0.2340250164270401\n",
      "step 2820 loss 0.23340660333633423\n",
      "step 2821 loss 0.23278921842575073\n",
      "step 2822 loss 0.23217326402664185\n",
      "step 2823 loss 0.23155830800533295\n",
      "step 2824 loss 0.23094497621059418\n",
      "step 2825 loss 0.23033300042152405\n",
      "step 2826 loss 0.22972263395786285\n",
      "step 2827 loss 0.22911393642425537\n",
      "step 2828 loss 0.2285068929195404\n",
      "step 2829 loss 0.22790172696113586\n",
      "step 2830 loss 0.22729824483394623\n",
      "step 2831 loss 0.22669661045074463\n",
      "step 2832 loss 0.22609704732894897\n",
      "step 2833 loss 0.2255035787820816\n",
      "step 2834 loss 0.2249210774898529\n",
      "step 2835 loss 0.22434130311012268\n",
      "step 2836 loss 0.22376412153244019\n",
      "step 2837 loss 0.2231898307800293\n",
      "step 2838 loss 0.22261808812618256\n",
      "step 2839 loss 0.22204887866973877\n",
      "step 2840 loss 0.22148260474205017\n",
      "step 2841 loss 0.22091850638389587\n",
      "step 2842 loss 0.2203570157289505\n",
      "step 2843 loss 0.2197977900505066\n",
      "step 2844 loss 0.21924133598804474\n",
      "step 2845 loss 0.21868686378002167\n",
      "step 2846 loss 0.21813514828681946\n",
      "step 2847 loss 0.21758534014225006\n",
      "step 2848 loss 0.21703772246837616\n",
      "step 2849 loss 0.21649250388145447\n",
      "step 2850 loss 0.21594953536987305\n",
      "step 2851 loss 0.21540871262550354\n",
      "step 2852 loss 0.214869886636734\n",
      "step 2853 loss 0.21433305740356445\n",
      "step 2854 loss 0.21379859745502472\n",
      "step 2855 loss 0.21326588094234467\n",
      "step 2856 loss 0.21273529529571533\n",
      "step 2857 loss 0.2122069150209427\n",
      "step 2858 loss 0.21168026328086853\n",
      "step 2859 loss 0.2111556977033615\n",
      "step 2860 loss 0.2106330543756485\n",
      "step 2861 loss 0.21011234819889069\n",
      "step 2862 loss 0.2095935046672821\n",
      "step 2863 loss 0.20907670259475708\n",
      "step 2864 loss 0.20856185257434845\n",
      "step 2865 loss 0.20804880559444427\n",
      "step 2866 loss 0.20753765106201172\n",
      "step 2867 loss 0.20702829957008362\n",
      "step 2868 loss 0.20652103424072266\n",
      "step 2869 loss 0.20601525902748108\n",
      "step 2870 loss 0.20551176369190216\n",
      "step 2871 loss 0.20500975847244263\n",
      "step 2872 loss 0.20450975000858307\n",
      "step 2873 loss 0.20401136577129364\n",
      "step 2874 loss 0.20351500809192657\n",
      "step 2875 loss 0.2030201554298401\n",
      "step 2876 loss 0.2025272697210312\n",
      "step 2877 loss 0.20203618705272675\n",
      "step 2878 loss 0.20154671370983124\n",
      "step 2879 loss 0.20105919241905212\n",
      "step 2880 loss 0.20057323575019836\n",
      "step 2881 loss 0.20008918642997742\n",
      "step 2882 loss 0.19960667192935944\n",
      "step 2883 loss 0.19912602007389069\n",
      "step 2884 loss 0.19864723086357117\n",
      "step 2885 loss 0.1981702446937561\n",
      "step 2886 loss 0.19769491255283356\n",
      "step 2887 loss 0.19722118973731995\n",
      "step 2888 loss 0.1967490315437317\n",
      "step 2889 loss 0.19627822935581207\n",
      "step 2890 loss 0.195809006690979\n",
      "step 2891 loss 0.19534160196781158\n",
      "step 2892 loss 0.1948758363723755\n",
      "step 2893 loss 0.19441162049770355\n",
      "step 2894 loss 0.19394882023334503\n",
      "step 2895 loss 0.19348789751529694\n",
      "step 2896 loss 0.19302856922149658\n",
      "step 2897 loss 0.1925707459449768\n",
      "step 2898 loss 0.19211441278457642\n",
      "step 2899 loss 0.19165988266468048\n",
      "step 2900 loss 0.19120681285858154\n",
      "step 2901 loss 0.19075536727905273\n",
      "step 2902 loss 0.19030563533306122\n",
      "step 2903 loss 0.1898573338985443\n",
      "step 2904 loss 0.18941046297550201\n",
      "step 2905 loss 0.18896536529064178\n",
      "step 2906 loss 0.18852172791957855\n",
      "step 2907 loss 0.18807964026927948\n",
      "step 2908 loss 0.18763908743858337\n",
      "step 2909 loss 0.1872001588344574\n",
      "step 2910 loss 0.18676267564296722\n",
      "step 2911 loss 0.18632672727108002\n",
      "step 2912 loss 0.1858922243118286\n",
      "step 2913 loss 0.18545620143413544\n",
      "step 2914 loss 0.18501384556293488\n",
      "step 2915 loss 0.18457233905792236\n",
      "step 2916 loss 0.18413116037845612\n",
      "step 2917 loss 0.18369095027446747\n",
      "step 2918 loss 0.18325158953666687\n",
      "step 2919 loss 0.1828129142522812\n",
      "step 2920 loss 0.1823754757642746\n",
      "step 2921 loss 0.1819390058517456\n",
      "step 2922 loss 0.18150383234024048\n",
      "step 2923 loss 0.18106962740421295\n",
      "step 2924 loss 0.18063674867153168\n",
      "step 2925 loss 0.18020527064800262\n",
      "step 2926 loss 0.17977483570575714\n",
      "step 2927 loss 0.17934578657150269\n",
      "step 2928 loss 0.17891818284988403\n",
      "step 2929 loss 0.17849186062812805\n",
      "step 2930 loss 0.17806671559810638\n",
      "step 2931 loss 0.17764317989349365\n",
      "step 2932 loss 0.1772209107875824\n",
      "step 2933 loss 0.1767999678850174\n",
      "step 2934 loss 0.17638041079044342\n",
      "step 2935 loss 0.17596237361431122\n",
      "step 2936 loss 0.17554567754268646\n",
      "step 2937 loss 0.17513035237789154\n",
      "step 2938 loss 0.17471638321876526\n",
      "step 2939 loss 0.1743040531873703\n",
      "step 2940 loss 0.1738930642604828\n",
      "step 2941 loss 0.17348343133926392\n",
      "step 2942 loss 0.17307525873184204\n",
      "step 2943 loss 0.17266857624053955\n",
      "step 2944 loss 0.17226333916187286\n",
      "step 2945 loss 0.1718595176935196\n",
      "step 2946 loss 0.1714572012424469\n",
      "step 2947 loss 0.17105580866336823\n",
      "step 2948 loss 0.1706557273864746\n",
      "step 2949 loss 0.17025715112686157\n",
      "step 2950 loss 0.1698596030473709\n",
      "step 2951 loss 0.16946369409561157\n",
      "step 2952 loss 0.16906896233558655\n",
      "step 2953 loss 0.16867578029632568\n",
      "step 2954 loss 0.1682838648557663\n",
      "step 2955 loss 0.1678934097290039\n",
      "step 2956 loss 0.16750453412532806\n",
      "step 2957 loss 0.16711685061454773\n",
      "step 2958 loss 0.16673052310943604\n",
      "step 2959 loss 0.1663457155227661\n",
      "step 2960 loss 0.16596214473247528\n",
      "step 2961 loss 0.16557998955249786\n",
      "step 2962 loss 0.16519920527935028\n",
      "step 2963 loss 0.16481979191303253\n",
      "step 2964 loss 0.1644417941570282\n",
      "step 2965 loss 0.16406501829624176\n",
      "step 2966 loss 0.16368967294692993\n",
      "step 2967 loss 0.16331572830677032\n",
      "step 2968 loss 0.16294287145137787\n",
      "step 2969 loss 0.16257163882255554\n",
      "step 2970 loss 0.1622016578912735\n",
      "step 2971 loss 0.16183285415172577\n",
      "step 2972 loss 0.16146555542945862\n",
      "step 2973 loss 0.161099374294281\n",
      "step 2974 loss 0.16073471307754517\n",
      "step 2975 loss 0.16037112474441528\n",
      "step 2976 loss 0.16000889241695404\n",
      "step 2977 loss 0.15964795649051666\n",
      "step 2978 loss 0.1592884063720703\n",
      "step 2979 loss 0.15893009305000305\n",
      "step 2980 loss 0.1585729718208313\n",
      "step 2981 loss 0.15821722149848938\n",
      "step 2982 loss 0.15786276757717133\n",
      "step 2983 loss 0.1575094759464264\n",
      "step 2984 loss 0.15715749561786652\n",
      "step 2985 loss 0.15680678188800812\n",
      "step 2986 loss 0.15645723044872284\n",
      "step 2987 loss 0.1561090648174286\n",
      "step 2988 loss 0.15576204657554626\n",
      "step 2989 loss 0.15541620552539825\n",
      "step 2990 loss 0.15507178008556366\n",
      "step 2991 loss 0.15472835302352905\n",
      "step 2992 loss 0.15438631176948547\n",
      "step 2993 loss 0.15404540300369263\n",
      "step 2994 loss 0.15370571613311768\n",
      "step 2995 loss 0.1533673107624054\n",
      "step 2996 loss 0.15302997827529907\n",
      "step 2997 loss 0.15269382297992706\n",
      "step 2998 loss 0.1523590236902237\n",
      "step 2999 loss 0.15202529728412628\n",
      "step 3000 loss 0.15169291198253632\n",
      "step 3001 loss 0.15136153995990753\n",
      "step 3002 loss 0.15103131532669067\n",
      "step 3003 loss 0.15070228278636932\n",
      "step 3004 loss 0.1503744274377823\n",
      "step 3005 loss 0.15004782378673553\n",
      "step 3006 loss 0.14972232282161713\n",
      "step 3007 loss 0.14939793944358826\n",
      "step 3008 loss 0.14907477796077728\n",
      "step 3009 loss 0.14875277876853943\n",
      "step 3010 loss 0.14843176305294037\n",
      "step 3011 loss 0.14811202883720398\n",
      "step 3012 loss 0.14779333770275116\n",
      "step 3013 loss 0.14747583866119385\n",
      "step 3014 loss 0.14715948700904846\n",
      "step 3015 loss 0.14684410393238068\n",
      "step 3016 loss 0.14652986824512482\n",
      "step 3017 loss 0.14621689915657043\n",
      "step 3018 loss 0.14590492844581604\n",
      "step 3019 loss 0.14559410512447357\n",
      "step 3020 loss 0.14528436958789825\n",
      "step 3021 loss 0.14497558772563934\n",
      "step 3022 loss 0.1446680724620819\n",
      "step 3023 loss 0.1443614512681961\n",
      "step 3024 loss 0.1440560668706894\n",
      "step 3025 loss 0.14375169575214386\n",
      "step 3026 loss 0.1434483826160431\n",
      "step 3027 loss 0.14314618706703186\n",
      "step 3028 loss 0.14284513890743256\n",
      "step 3029 loss 0.1425449401140213\n",
      "step 3030 loss 0.1422460675239563\n",
      "step 3031 loss 0.14194810390472412\n",
      "step 3032 loss 0.1416510045528412\n",
      "step 3033 loss 0.14135512709617615\n",
      "step 3034 loss 0.1410602480173111\n",
      "step 3035 loss 0.14076633751392365\n",
      "step 3036 loss 0.14047351479530334\n",
      "step 3037 loss 0.14018172025680542\n",
      "step 3038 loss 0.13989093899726868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3039 loss 0.13960117101669312\n",
      "step 3040 loss 0.1393127143383026\n",
      "step 3041 loss 0.1390252560377121\n",
      "step 3042 loss 0.13873864710330963\n",
      "step 3043 loss 0.13845305144786835\n",
      "step 3044 loss 0.13816870748996735\n",
      "step 3045 loss 0.13788513839244843\n",
      "step 3046 loss 0.13760244846343994\n",
      "step 3047 loss 0.13732098042964935\n",
      "step 3048 loss 0.13704025745391846\n",
      "step 3049 loss 0.13676045835018158\n",
      "step 3050 loss 0.13648183643817902\n",
      "step 3051 loss 0.13620410859584808\n",
      "step 3052 loss 0.13592717051506042\n",
      "step 3053 loss 0.13565142452716827\n",
      "step 3054 loss 0.13537642359733582\n",
      "step 3055 loss 0.13510242104530334\n",
      "step 3056 loss 0.13482947647571564\n",
      "step 3057 loss 0.13455747067928314\n",
      "step 3058 loss 0.1342865228652954\n",
      "step 3059 loss 0.13401645421981812\n",
      "step 3060 loss 0.13374727964401245\n",
      "step 3061 loss 0.13347896933555603\n",
      "step 3062 loss 0.13321159780025482\n",
      "step 3063 loss 0.13294526934623718\n",
      "step 3064 loss 0.13267973065376282\n",
      "step 3065 loss 0.13241508603096008\n",
      "step 3066 loss 0.13215139508247375\n",
      "step 3067 loss 0.13188853859901428\n",
      "step 3068 loss 0.13162663578987122\n",
      "step 3069 loss 0.1313655972480774\n",
      "step 3070 loss 0.13110549747943878\n",
      "step 3071 loss 0.13084623217582703\n",
      "step 3072 loss 0.13058781623840332\n",
      "step 3073 loss 0.13033035397529602\n",
      "step 3074 loss 0.130073681473732\n",
      "step 3075 loss 0.12981808185577393\n",
      "step 3076 loss 0.12956306338310242\n",
      "step 3077 loss 0.12930908799171448\n",
      "step 3078 loss 0.12905599176883698\n",
      "step 3079 loss 0.12880365550518036\n",
      "step 3080 loss 0.1285521388053894\n",
      "step 3081 loss 0.12830166518688202\n",
      "step 3082 loss 0.12805187702178955\n",
      "step 3083 loss 0.12780290842056274\n",
      "step 3084 loss 0.12755483388900757\n",
      "step 3085 loss 0.12730759382247925\n",
      "step 3086 loss 0.12706123292446136\n",
      "step 3087 loss 0.1268155872821808\n",
      "step 3088 loss 0.1265709102153778\n",
      "step 3089 loss 0.1263270378112793\n",
      "step 3090 loss 0.12608389556407928\n",
      "step 3091 loss 0.12584155797958374\n",
      "step 3092 loss 0.1256001889705658\n",
      "step 3093 loss 0.12535957992076874\n",
      "step 3094 loss 0.12511970102787018\n",
      "step 3095 loss 0.12488076835870743\n",
      "step 3096 loss 0.12464264780282974\n",
      "step 3097 loss 0.1244087815284729\n",
      "step 3098 loss 0.12417618930339813\n",
      "step 3099 loss 0.1239444762468338\n",
      "step 3100 loss 0.12371370196342468\n",
      "step 3101 loss 0.12348383665084839\n",
      "step 3102 loss 0.12325487285852432\n",
      "step 3103 loss 0.12302682548761368\n",
      "step 3104 loss 0.12279964238405228\n",
      "step 3105 loss 0.12257346510887146\n",
      "step 3106 loss 0.12234807014465332\n",
      "step 3107 loss 0.12212362140417099\n",
      "step 3108 loss 0.12190002202987671\n",
      "step 3109 loss 0.12167719006538391\n",
      "step 3110 loss 0.12145528942346573\n",
      "step 3111 loss 0.1212342232465744\n",
      "step 3112 loss 0.1210138127207756\n",
      "step 3113 loss 0.12079433351755142\n",
      "step 3114 loss 0.12057548761367798\n",
      "step 3115 loss 0.12035747617483139\n",
      "step 3116 loss 0.12014017254114151\n",
      "step 3117 loss 0.1199236661195755\n",
      "step 3118 loss 0.11970788240432739\n",
      "step 3119 loss 0.11949285864830017\n",
      "step 3120 loss 0.11927846819162369\n",
      "step 3121 loss 0.11906490474939346\n",
      "step 3122 loss 0.11885211616754532\n",
      "step 3123 loss 0.11863985657691956\n",
      "step 3124 loss 0.11842850595712662\n",
      "step 3125 loss 0.11821785569190979\n",
      "step 3126 loss 0.11800769716501236\n",
      "step 3127 loss 0.11779843270778656\n",
      "step 3128 loss 0.11758986115455627\n",
      "step 3129 loss 0.11738193780183792\n",
      "step 3130 loss 0.11717469990253448\n",
      "step 3131 loss 0.11696822941303253\n",
      "step 3132 loss 0.11676234006881714\n",
      "step 3133 loss 0.11655719578266144\n",
      "step 3134 loss 0.11635280400514603\n",
      "step 3135 loss 0.11614900827407837\n",
      "step 3136 loss 0.11594583839178085\n",
      "step 3137 loss 0.11574335396289825\n",
      "step 3138 loss 0.11554165184497833\n",
      "step 3139 loss 0.11534057557582855\n",
      "step 3140 loss 0.11513999849557877\n",
      "step 3141 loss 0.11494026333093643\n",
      "step 3142 loss 0.11474111676216125\n",
      "step 3143 loss 0.11454259604215622\n",
      "step 3144 loss 0.11434484273195267\n",
      "step 3145 loss 0.11414769291877747\n",
      "step 3146 loss 0.11395110934972763\n",
      "step 3147 loss 0.1137552335858345\n",
      "step 3148 loss 0.11355993896722794\n",
      "step 3149 loss 0.11336534470319748\n",
      "step 3150 loss 0.11317136883735657\n",
      "step 3151 loss 0.11297807842493057\n",
      "step 3152 loss 0.11278533190488815\n",
      "step 3153 loss 0.11259326338768005\n",
      "step 3154 loss 0.1124018207192421\n",
      "step 3155 loss 0.11221101880073547\n",
      "step 3156 loss 0.11202080547809601\n",
      "step 3157 loss 0.11183121055364609\n",
      "step 3158 loss 0.11164221912622452\n",
      "step 3159 loss 0.11145390570163727\n",
      "step 3160 loss 0.11126607656478882\n",
      "step 3161 loss 0.11107898503541946\n",
      "step 3162 loss 0.11089242994785309\n",
      "step 3163 loss 0.11070647090673447\n",
      "step 3164 loss 0.11052114516496658\n",
      "step 3165 loss 0.11033643782138824\n",
      "step 3166 loss 0.11015228927135468\n",
      "step 3167 loss 0.10996875911951065\n",
      "step 3168 loss 0.1097857728600502\n",
      "step 3169 loss 0.10960342735052109\n",
      "step 3170 loss 0.10942168533802032\n",
      "step 3171 loss 0.1092405617237091\n",
      "step 3172 loss 0.10905995965003967\n",
      "step 3173 loss 0.10887996852397919\n",
      "step 3174 loss 0.10870049148797989\n",
      "step 3175 loss 0.10852167010307312\n",
      "step 3176 loss 0.10834339261054993\n",
      "step 3177 loss 0.10816571861505508\n",
      "step 3178 loss 0.10798856616020203\n",
      "step 3179 loss 0.10781201720237732\n",
      "step 3180 loss 0.10763603448867798\n",
      "step 3181 loss 0.1074606329202652\n",
      "step 3182 loss 0.10728580504655838\n",
      "step 3183 loss 0.10711149126291275\n",
      "step 3184 loss 0.1069377139210701\n",
      "step 3185 loss 0.10676451772451401\n",
      "step 3186 loss 0.10659196972846985\n",
      "step 3187 loss 0.10641979426145554\n",
      "step 3188 loss 0.10624837875366211\n",
      "step 3189 loss 0.10607736557722092\n",
      "step 3190 loss 0.10590700060129166\n",
      "step 3191 loss 0.10573703050613403\n",
      "step 3192 loss 0.10556770116090775\n",
      "step 3193 loss 0.10539893805980682\n",
      "step 3194 loss 0.10523063689470291\n",
      "step 3195 loss 0.10506291687488556\n",
      "step 3196 loss 0.10489576309919357\n",
      "step 3197 loss 0.10472913086414337\n",
      "step 3198 loss 0.10456299036741257\n",
      "step 3199 loss 0.10439740866422653\n",
      "step 3200 loss 0.1042323037981987\n",
      "step 3201 loss 0.10406780242919922\n",
      "step 3202 loss 0.10390371084213257\n",
      "step 3203 loss 0.10374022275209427\n",
      "step 3204 loss 0.10357727855443954\n",
      "step 3205 loss 0.10341474413871765\n",
      "step 3206 loss 0.10325285792350769\n",
      "step 3207 loss 0.10309141874313354\n",
      "step 3208 loss 0.10293039679527283\n",
      "step 3209 loss 0.10277000069618225\n",
      "step 3210 loss 0.10261013358831406\n",
      "step 3211 loss 0.10245069861412048\n",
      "step 3212 loss 0.10229175537824631\n",
      "step 3213 loss 0.10213346779346466\n",
      "step 3214 loss 0.1019754633307457\n",
      "step 3215 loss 0.10181805491447449\n",
      "step 3216 loss 0.10166113078594208\n",
      "step 3217 loss 0.1015048399567604\n",
      "step 3218 loss 0.10134880244731903\n",
      "step 3219 loss 0.10119345039129257\n",
      "step 3220 loss 0.10103841125965118\n",
      "step 3221 loss 0.10088393837213516\n",
      "step 3222 loss 0.1007300540804863\n",
      "step 3223 loss 0.10057653486728668\n",
      "step 3224 loss 0.10042360424995422\n",
      "step 3225 loss 0.10027094930410385\n",
      "step 3226 loss 0.10011895000934601\n",
      "step 3227 loss 0.09996739029884338\n",
      "step 3228 loss 0.09981629997491837\n",
      "step 3229 loss 0.09966571629047394\n",
      "step 3230 loss 0.09951551258563995\n",
      "step 3231 loss 0.09936581552028656\n",
      "step 3232 loss 0.09921663999557495\n",
      "step 3233 loss 0.09906791895627975\n",
      "step 3234 loss 0.0989195853471756\n",
      "step 3235 loss 0.09877176582813263\n",
      "step 3236 loss 0.09862440079450607\n",
      "step 3237 loss 0.09847753494977951\n",
      "step 3238 loss 0.09833115339279175\n",
      "step 3239 loss 0.09818500280380249\n",
      "step 3240 loss 0.09803958982229233\n",
      "step 3241 loss 0.09789447486400604\n",
      "step 3242 loss 0.09774978458881378\n",
      "step 3243 loss 0.0976056456565857\n",
      "step 3244 loss 0.09746187925338745\n",
      "step 3245 loss 0.0973186269402504\n",
      "step 3246 loss 0.09717579931020737\n",
      "step 3247 loss 0.09703338891267776\n",
      "step 3248 loss 0.0968913659453392\n",
      "step 3249 loss 0.09674985706806183\n",
      "step 3250 loss 0.09660880267620087\n",
      "step 3251 loss 0.09646814316511154\n",
      "step 3252 loss 0.09632791578769684\n",
      "step 3253 loss 0.09618818014860153\n",
      "step 3254 loss 0.09604887664318085\n",
      "step 3255 loss 0.09590983390808105\n",
      "step 3256 loss 0.09577138721942902\n",
      "step 3257 loss 0.09563331305980682\n",
      "step 3258 loss 0.09549566358327866\n",
      "step 3259 loss 0.0953584685921669\n",
      "step 3260 loss 0.09522166103124619\n",
      "step 3261 loss 0.0950852558016777\n",
      "step 3262 loss 0.09494934976100922\n",
      "step 3263 loss 0.09481378644704819\n",
      "step 3264 loss 0.09467866271734238\n",
      "step 3265 loss 0.09454397112131119\n",
      "step 3266 loss 0.09440963715314865\n",
      "step 3267 loss 0.09427574276924133\n",
      "step 3268 loss 0.09414217621088028\n",
      "step 3269 loss 0.0940091609954834\n",
      "step 3270 loss 0.09387651085853577\n",
      "step 3271 loss 0.09374427050352097\n",
      "step 3272 loss 0.09361237287521362\n",
      "step 3273 loss 0.09348096698522568\n",
      "step 3274 loss 0.09334991127252579\n",
      "step 3275 loss 0.09321924299001694\n",
      "step 3276 loss 0.09308896213769913\n",
      "step 3277 loss 0.0929592102766037\n",
      "step 3278 loss 0.0928296372294426\n",
      "step 3279 loss 0.09270055592060089\n",
      "step 3280 loss 0.09257186949253082\n",
      "step 3281 loss 0.09244360029697418\n",
      "step 3282 loss 0.0923156812787056\n",
      "step 3283 loss 0.09218817949295044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3284 loss 0.09206108003854752\n",
      "step 3285 loss 0.09193437546491623\n",
      "step 3286 loss 0.09180795401334763\n",
      "step 3287 loss 0.09168199449777603\n",
      "step 3288 loss 0.09155641496181488\n",
      "step 3289 loss 0.0914311408996582\n",
      "step 3290 loss 0.09130635112524033\n",
      "step 3291 loss 0.09118181467056274\n",
      "step 3292 loss 0.09105775505304337\n",
      "step 3293 loss 0.09093400090932846\n",
      "step 3294 loss 0.09081075340509415\n",
      "step 3295 loss 0.09068771451711655\n",
      "step 3296 loss 0.09056509286165237\n",
      "step 3297 loss 0.09044285118579865\n",
      "step 3298 loss 0.09032095223665237\n",
      "step 3299 loss 0.09019942581653595\n",
      "step 3300 loss 0.09007833153009415\n",
      "step 3301 loss 0.08995760232210159\n",
      "step 3302 loss 0.0898372083902359\n",
      "step 3303 loss 0.08971711248159409\n",
      "step 3304 loss 0.08959737420082092\n",
      "step 3305 loss 0.08947811275720596\n",
      "step 3306 loss 0.08935903757810593\n",
      "step 3307 loss 0.08924045413732529\n",
      "step 3308 loss 0.08912219107151031\n",
      "step 3309 loss 0.08900418132543564\n",
      "step 3310 loss 0.08888666331768036\n",
      "step 3311 loss 0.08876941353082657\n",
      "step 3312 loss 0.08865254372358322\n",
      "step 3313 loss 0.08853599429130554\n",
      "step 3314 loss 0.08841980248689651\n",
      "step 3315 loss 0.08830400556325912\n",
      "step 3316 loss 0.08818843960762024\n",
      "step 3317 loss 0.08807332813739777\n",
      "step 3318 loss 0.08795850723981857\n",
      "step 3319 loss 0.08784400671720505\n",
      "step 3320 loss 0.08772987127304077\n",
      "step 3321 loss 0.08761606365442276\n",
      "step 3322 loss 0.08750265091657639\n",
      "step 3323 loss 0.08738946914672852\n",
      "step 3324 loss 0.08727678656578064\n",
      "step 3325 loss 0.08716423064470291\n",
      "step 3326 loss 0.08705213665962219\n",
      "step 3327 loss 0.08694037795066833\n",
      "step 3328 loss 0.08682885766029358\n",
      "step 3329 loss 0.08671777695417404\n",
      "step 3330 loss 0.08660691976547241\n",
      "step 3331 loss 0.08649639040231705\n",
      "step 3332 loss 0.08638624846935272\n",
      "step 3333 loss 0.08627641946077347\n",
      "step 3334 loss 0.0861668810248375\n",
      "step 3335 loss 0.08605638146400452\n",
      "step 3336 loss 0.08594558387994766\n",
      "step 3337 loss 0.08583477139472961\n",
      "step 3338 loss 0.08572417497634888\n",
      "step 3339 loss 0.08561374992132187\n",
      "step 3340 loss 0.08550343662500381\n",
      "step 3341 loss 0.08539342135190964\n",
      "step 3342 loss 0.08528351038694382\n",
      "step 3343 loss 0.0851738452911377\n",
      "step 3344 loss 0.08506441116333008\n",
      "step 3345 loss 0.08495527505874634\n",
      "step 3346 loss 0.08484629541635513\n",
      "step 3347 loss 0.08473766595125198\n",
      "step 3348 loss 0.08462923765182495\n",
      "step 3349 loss 0.08452105522155762\n",
      "step 3350 loss 0.08441314101219177\n",
      "step 3351 loss 0.08430548012256622\n",
      "step 3352 loss 0.08419809490442276\n",
      "step 3353 loss 0.08409097045660019\n",
      "step 3354 loss 0.08398415893316269\n",
      "step 3355 loss 0.08387752622365952\n",
      "step 3356 loss 0.0837712362408638\n",
      "step 3357 loss 0.0836651399731636\n",
      "step 3358 loss 0.08355947583913803\n",
      "step 3359 loss 0.08345396816730499\n",
      "step 3360 loss 0.08334875106811523\n",
      "step 3361 loss 0.08324391394853592\n",
      "step 3362 loss 0.08313927054405212\n",
      "step 3363 loss 0.08303485810756683\n",
      "step 3364 loss 0.08293083310127258\n",
      "step 3365 loss 0.08282705396413803\n",
      "step 3366 loss 0.0827234759926796\n",
      "step 3367 loss 0.0826202780008316\n",
      "step 3368 loss 0.08251731842756271\n",
      "step 3369 loss 0.08241463452577591\n",
      "step 3370 loss 0.08231225609779358\n",
      "step 3371 loss 0.08221013844013214\n",
      "step 3372 loss 0.08210832625627518\n",
      "step 3373 loss 0.08200681954622269\n",
      "step 3374 loss 0.08190557360649109\n",
      "step 3375 loss 0.08180460333824158\n",
      "step 3376 loss 0.08170381188392639\n",
      "step 3377 loss 0.08160339295864105\n",
      "step 3378 loss 0.08150327950716019\n",
      "step 3379 loss 0.08140338957309723\n",
      "step 3380 loss 0.08130374550819397\n",
      "step 3381 loss 0.08120447397232056\n",
      "step 3382 loss 0.08110538125038147\n",
      "step 3383 loss 0.08100660145282745\n",
      "step 3384 loss 0.08090806752443314\n",
      "step 3385 loss 0.08080983906984329\n",
      "step 3386 loss 0.08071184903383255\n",
      "step 3387 loss 0.0806141123175621\n",
      "step 3388 loss 0.08051668107509613\n",
      "step 3389 loss 0.08041946589946747\n",
      "step 3390 loss 0.08032259345054626\n",
      "step 3391 loss 0.08022592216730118\n",
      "step 3392 loss 0.08012952655553818\n",
      "step 3393 loss 0.08003337681293488\n",
      "step 3394 loss 0.07993752509355545\n",
      "step 3395 loss 0.0798419862985611\n",
      "step 3396 loss 0.07974658906459808\n",
      "step 3397 loss 0.07965155690908432\n",
      "step 3398 loss 0.0795566663146019\n",
      "step 3399 loss 0.07946214079856873\n",
      "step 3400 loss 0.07936783879995346\n",
      "step 3401 loss 0.0792737528681755\n",
      "step 3402 loss 0.07917992770671844\n",
      "step 3403 loss 0.07908641546964645\n",
      "step 3404 loss 0.07899311929941177\n",
      "step 3405 loss 0.07890012860298157\n",
      "step 3406 loss 0.0788072869181633\n",
      "step 3407 loss 0.07871473580598831\n",
      "step 3408 loss 0.07862246036529541\n",
      "step 3409 loss 0.07853043079376221\n",
      "step 3410 loss 0.0784386396408081\n",
      "step 3411 loss 0.07834705710411072\n",
      "step 3412 loss 0.078255794942379\n",
      "step 3413 loss 0.07816468924283981\n",
      "step 3414 loss 0.0780738815665245\n",
      "step 3415 loss 0.0779832974076271\n",
      "step 3416 loss 0.07789298892021179\n",
      "step 3417 loss 0.077802874147892\n",
      "step 3418 loss 0.0777130052447319\n",
      "step 3419 loss 0.0776234120130539\n",
      "step 3420 loss 0.0775340273976326\n",
      "step 3421 loss 0.07744487375020981\n",
      "step 3422 loss 0.0773560032248497\n",
      "step 3423 loss 0.0772673711180687\n",
      "step 3424 loss 0.0771789625287056\n",
      "step 3425 loss 0.07709076255559921\n",
      "step 3426 loss 0.07700279355049133\n",
      "step 3427 loss 0.07691510021686554\n",
      "step 3428 loss 0.07682759314775467\n",
      "step 3429 loss 0.0767403095960617\n",
      "step 3430 loss 0.07665335386991501\n",
      "step 3431 loss 0.07656648010015488\n",
      "step 3432 loss 0.0764799565076828\n",
      "step 3433 loss 0.07639359682798386\n",
      "step 3434 loss 0.07630747556686401\n",
      "step 3435 loss 0.07622162997722626\n",
      "step 3436 loss 0.07613590359687805\n",
      "step 3437 loss 0.07605051249265671\n",
      "step 3438 loss 0.07596534490585327\n",
      "step 3439 loss 0.075880266726017\n",
      "step 3440 loss 0.0757955014705658\n",
      "step 3441 loss 0.0757109746336937\n",
      "step 3442 loss 0.07562670856714249\n",
      "step 3443 loss 0.07554259896278381\n",
      "step 3444 loss 0.07545873522758484\n",
      "step 3445 loss 0.0753750279545784\n",
      "step 3446 loss 0.07529158145189285\n",
      "step 3447 loss 0.07520828396081924\n",
      "step 3448 loss 0.0751253142952919\n",
      "step 3449 loss 0.07504253089427948\n",
      "step 3450 loss 0.07495995610952377\n",
      "step 3451 loss 0.0748775452375412\n",
      "step 3452 loss 0.07479539513587952\n",
      "step 3453 loss 0.07471352070569992\n",
      "step 3454 loss 0.07463175803422928\n",
      "step 3455 loss 0.07455022633075714\n",
      "step 3456 loss 0.07446892559528351\n",
      "step 3457 loss 0.0743878111243248\n",
      "step 3458 loss 0.07430688291788101\n",
      "step 3459 loss 0.0742262676358223\n",
      "step 3460 loss 0.07414579391479492\n",
      "step 3461 loss 0.07406549155712128\n",
      "step 3462 loss 0.07398548722267151\n",
      "step 3463 loss 0.07390563935041428\n",
      "step 3464 loss 0.07382598519325256\n",
      "step 3465 loss 0.07374653220176697\n",
      "step 3466 loss 0.07366722077131271\n",
      "step 3467 loss 0.07358822226524353\n",
      "step 3468 loss 0.07350941747426987\n",
      "step 3469 loss 0.07343073934316635\n",
      "step 3470 loss 0.07335227727890015\n",
      "step 3471 loss 0.07327409833669662\n",
      "step 3472 loss 0.07319604605436325\n",
      "step 3473 loss 0.07311820238828659\n",
      "step 3474 loss 0.07304059714078903\n",
      "step 3475 loss 0.0729631632566452\n",
      "step 3476 loss 0.07288586348295212\n",
      "step 3477 loss 0.0728088840842247\n",
      "step 3478 loss 0.07273203879594803\n",
      "step 3479 loss 0.07265514135360718\n",
      "step 3480 loss 0.07257838547229767\n",
      "step 3481 loss 0.07250188291072845\n",
      "step 3482 loss 0.07242543995380402\n",
      "step 3483 loss 0.07234924286603928\n",
      "step 3484 loss 0.07227317988872528\n",
      "step 3485 loss 0.07219726592302322\n",
      "step 3486 loss 0.07212154567241669\n",
      "step 3487 loss 0.07204599678516388\n",
      "step 3488 loss 0.07197056710720062\n",
      "step 3489 loss 0.07189539819955826\n",
      "step 3490 loss 0.07182035595178604\n",
      "step 3491 loss 0.07174552977085114\n",
      "step 3492 loss 0.07167089730501175\n",
      "step 3493 loss 0.07159634679555893\n",
      "step 3494 loss 0.071522057056427\n",
      "step 3495 loss 0.07144789397716522\n",
      "step 3496 loss 0.07137390226125717\n",
      "step 3497 loss 0.07130011916160583\n",
      "step 3498 loss 0.07122648507356644\n",
      "step 3499 loss 0.07115306705236435\n",
      "step 3500 loss 0.0710798054933548\n",
      "step 3501 loss 0.0710066631436348\n",
      "step 3502 loss 0.07093378156423569\n",
      "step 3503 loss 0.07086099684238434\n",
      "step 3504 loss 0.07078848034143448\n",
      "step 3505 loss 0.0707160010933876\n",
      "step 3506 loss 0.0706438198685646\n",
      "step 3507 loss 0.07057178020477295\n",
      "step 3508 loss 0.07049987465143204\n",
      "step 3509 loss 0.07042817771434784\n",
      "step 3510 loss 0.07035663723945618\n",
      "step 3511 loss 0.07028531283140182\n",
      "step 3512 loss 0.07021410018205643\n",
      "step 3513 loss 0.07014304399490356\n",
      "step 3514 loss 0.0700722336769104\n",
      "step 3515 loss 0.07000149041414261\n",
      "step 3516 loss 0.06993100047111511\n",
      "step 3517 loss 0.06986062228679657\n",
      "step 3518 loss 0.06979039311408997\n",
      "step 3519 loss 0.06972037255764008\n",
      "step 3520 loss 0.0696505531668663\n",
      "step 3521 loss 0.06958083063364029\n",
      "step 3522 loss 0.0695112869143486\n",
      "step 3523 loss 0.06944192945957184\n",
      "step 3524 loss 0.06937275826931\n",
      "step 3525 loss 0.06930365413427353\n",
      "step 3526 loss 0.06923480331897736\n",
      "step 3527 loss 0.06916607916355133\n",
      "step 3528 loss 0.06909752637147903\n",
      "step 3529 loss 0.06902917474508286\n",
      "step 3530 loss 0.06896093487739563\n",
      "step 3531 loss 0.06889285892248154\n",
      "step 3532 loss 0.06882496923208237\n",
      "step 3533 loss 0.06875722110271454\n",
      "step 3534 loss 0.06868961453437805\n",
      "step 3535 loss 0.06862223893404007\n",
      "step 3536 loss 0.06855486333370209\n",
      "step 3537 loss 0.06848795711994171\n",
      "step 3538 loss 0.06842131167650223\n",
      "step 3539 loss 0.06835489720106125\n",
      "step 3540 loss 0.068288654088974\n",
      "step 3541 loss 0.06822258234024048\n",
      "step 3542 loss 0.0681566447019577\n",
      "step 3543 loss 0.06809088587760925\n",
      "step 3544 loss 0.06802532076835632\n",
      "step 3545 loss 0.06795991212129593\n",
      "step 3546 loss 0.06789465993642807\n",
      "step 3547 loss 0.06782953441143036\n",
      "step 3548 loss 0.06776461750268936\n",
      "step 3549 loss 0.0676998496055603\n",
      "step 3550 loss 0.06763525307178497\n",
      "step 3551 loss 0.06757073849439621\n",
      "step 3552 loss 0.06750641763210297\n",
      "step 3553 loss 0.06744232773780823\n",
      "step 3554 loss 0.06737835705280304\n",
      "step 3555 loss 0.06731447577476501\n",
      "step 3556 loss 0.06725073605775833\n",
      "step 3557 loss 0.06718718260526657\n",
      "step 3558 loss 0.06712380796670914\n",
      "step 3559 loss 0.06706054508686066\n",
      "step 3560 loss 0.06699752062559128\n",
      "step 3561 loss 0.06693452596664429\n",
      "step 3562 loss 0.06687173247337341\n",
      "step 3563 loss 0.0668090209364891\n",
      "step 3564 loss 0.06674650311470032\n",
      "step 3565 loss 0.06668411940336227\n",
      "step 3566 loss 0.06662192195653915\n",
      "step 3567 loss 0.06655985116958618\n",
      "step 3568 loss 0.06649789214134216\n",
      "step 3569 loss 0.0664360523223877\n",
      "step 3570 loss 0.06637439131736755\n",
      "step 3571 loss 0.06631292402744293\n",
      "step 3572 loss 0.0662514939904213\n",
      "step 3573 loss 0.06619027256965637\n",
      "step 3574 loss 0.066129170358181\n",
      "step 3575 loss 0.06606817990541458\n",
      "step 3576 loss 0.06600736826658249\n",
      "step 3577 loss 0.06594665348529816\n",
      "step 3578 loss 0.06588611751794815\n",
      "step 3579 loss 0.06582563370466232\n",
      "step 3580 loss 0.06576540321111679\n",
      "step 3581 loss 0.06570526212453842\n",
      "step 3582 loss 0.06564521044492722\n",
      "step 3583 loss 0.06558533757925034\n",
      "step 3584 loss 0.0655255913734436\n",
      "step 3585 loss 0.06546598672866821\n",
      "step 3586 loss 0.06540655344724655\n",
      "step 3587 loss 0.06534712761640549\n",
      "step 3588 loss 0.06528789550065994\n",
      "step 3589 loss 0.06522881239652634\n",
      "step 3590 loss 0.06516988575458527\n",
      "step 3591 loss 0.06511107087135315\n",
      "step 3592 loss 0.06505239754915237\n",
      "step 3593 loss 0.06499380618333817\n",
      "step 3594 loss 0.06493540108203888\n",
      "step 3595 loss 0.06487705558538437\n",
      "step 3596 loss 0.06481888145208359\n",
      "step 3597 loss 0.06476087868213654\n",
      "step 3598 loss 0.06470295041799545\n",
      "step 3599 loss 0.06464515626430511\n",
      "step 3600 loss 0.06458741426467896\n",
      "step 3601 loss 0.06452992558479309\n",
      "step 3602 loss 0.064472496509552\n",
      "step 3603 loss 0.06441519409418106\n",
      "step 3604 loss 0.06435804069042206\n",
      "step 3605 loss 0.0643010213971138\n",
      "step 3606 loss 0.0642440915107727\n",
      "step 3607 loss 0.06418728083372116\n",
      "step 3608 loss 0.06413058936595917\n",
      "step 3609 loss 0.06407402455806732\n",
      "step 3610 loss 0.06401759386062622\n",
      "step 3611 loss 0.06396129727363586\n",
      "step 3612 loss 0.06390515714883804\n",
      "step 3613 loss 0.0638490691781044\n",
      "step 3614 loss 0.0637931376695633\n",
      "step 3615 loss 0.06373731791973114\n",
      "step 3616 loss 0.06368159502744675\n",
      "step 3617 loss 0.06362608075141907\n",
      "step 3618 loss 0.06357064098119736\n",
      "step 3619 loss 0.06351524591445923\n",
      "step 3620 loss 0.06346001476049423\n",
      "step 3621 loss 0.06340489536523819\n",
      "step 3622 loss 0.06334993988275528\n",
      "step 3623 loss 0.06329507380723953\n",
      "step 3624 loss 0.06324029713869095\n",
      "step 3625 loss 0.0631856918334961\n",
      "step 3626 loss 0.0631311684846878\n",
      "step 3627 loss 0.06307676434516907\n",
      "step 3628 loss 0.06302248686552048\n",
      "step 3629 loss 0.06296828389167786\n",
      "step 3630 loss 0.06291425973176956\n",
      "step 3631 loss 0.06286031752824783\n",
      "step 3632 loss 0.06280647218227386\n",
      "step 3633 loss 0.06275279819965363\n",
      "step 3634 loss 0.06269918382167816\n",
      "step 3635 loss 0.06264569610357285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3636 loss 0.06259232014417648\n",
      "step 3637 loss 0.06253905594348907\n",
      "step 3638 loss 0.062485914677381516\n",
      "step 3639 loss 0.062432870268821716\n",
      "step 3640 loss 0.062379952520132065\n",
      "step 3641 loss 0.06232711300253868\n",
      "step 3642 loss 0.062274474650621414\n",
      "step 3643 loss 0.06222185492515564\n",
      "step 3644 loss 0.06216935068368912\n",
      "step 3645 loss 0.06211702898144722\n",
      "step 3646 loss 0.06206473335623741\n",
      "step 3647 loss 0.06201261654496193\n",
      "step 3648 loss 0.06196054443717003\n",
      "step 3649 loss 0.06190858036279678\n",
      "step 3650 loss 0.06185675784945488\n",
      "step 3651 loss 0.061805058270692825\n",
      "step 3652 loss 0.061753466725349426\n",
      "step 3653 loss 0.061701949685811996\n",
      "step 3654 loss 0.061650510877370834\n",
      "step 3655 loss 0.06159922480583191\n",
      "step 3656 loss 0.061548031866550446\n",
      "step 3657 loss 0.06149693951010704\n",
      "step 3658 loss 0.06144598126411438\n",
      "step 3659 loss 0.061395131051540375\n",
      "step 3660 loss 0.061344366520643234\n",
      "step 3661 loss 0.061293698847293854\n",
      "step 3662 loss 0.06124313175678253\n",
      "step 3663 loss 0.06119268015027046\n",
      "step 3664 loss 0.06114235892891884\n",
      "step 3665 loss 0.06109210476279259\n",
      "step 3666 loss 0.0610419325530529\n",
      "step 3667 loss 0.06099192798137665\n",
      "step 3668 loss 0.06094199791550636\n",
      "step 3669 loss 0.06089213490486145\n",
      "step 3670 loss 0.06084239110350609\n",
      "step 3671 loss 0.06079275161027908\n",
      "step 3672 loss 0.06074322760105133\n",
      "step 3673 loss 0.06069381162524223\n",
      "step 3674 loss 0.0606444850564003\n",
      "step 3675 loss 0.060595206916332245\n",
      "step 3676 loss 0.06054612249135971\n",
      "step 3677 loss 0.06049710139632225\n",
      "step 3678 loss 0.060448117554187775\n",
      "step 3679 loss 0.060399312525987625\n",
      "step 3680 loss 0.060350559651851654\n",
      "step 3681 loss 0.06030193343758583\n",
      "step 3682 loss 0.06025337055325508\n",
      "step 3683 loss 0.06020493805408478\n",
      "step 3684 loss 0.06015663221478462\n",
      "step 3685 loss 0.06010834872722626\n",
      "step 3686 loss 0.060060176998376846\n",
      "step 3687 loss 0.06001216918230057\n",
      "step 3688 loss 0.059964217245578766\n",
      "step 3689 loss 0.059916310012340546\n",
      "step 3690 loss 0.05986854061484337\n",
      "step 3691 loss 0.059820856899023056\n",
      "step 3692 loss 0.0597732812166214\n",
      "step 3693 loss 0.05972582846879959\n",
      "step 3694 loss 0.059678416699171066\n",
      "step 3695 loss 0.0596311092376709\n",
      "step 3696 loss 0.05958396941423416\n",
      "step 3697 loss 0.05953681468963623\n",
      "step 3698 loss 0.059489812701940536\n",
      "step 3699 loss 0.05944288894534111\n",
      "step 3700 loss 0.05939604714512825\n",
      "step 3701 loss 0.059349313378334045\n",
      "step 3702 loss 0.0593026727437973\n",
      "step 3703 loss 0.05925614386796951\n",
      "step 3704 loss 0.05920968949794769\n",
      "step 3705 loss 0.05916331708431244\n",
      "step 3706 loss 0.059116993099451065\n",
      "step 3707 loss 0.05907082557678223\n",
      "step 3708 loss 0.059024713933467865\n",
      "step 3709 loss 0.05897874757647514\n",
      "step 3710 loss 0.05893277749419212\n",
      "step 3711 loss 0.05888696759939194\n",
      "step 3712 loss 0.05884122848510742\n",
      "step 3713 loss 0.058795616030693054\n",
      "step 3714 loss 0.05874999985098839\n",
      "step 3715 loss 0.058704543858766556\n",
      "step 3716 loss 0.05865917727351189\n",
      "step 3717 loss 0.0586138591170311\n",
      "step 3718 loss 0.05856866016983986\n",
      "step 3719 loss 0.05852353200316429\n",
      "step 3720 loss 0.05847852677106857\n",
      "step 3721 loss 0.05843356251716614\n",
      "step 3722 loss 0.058388739824295044\n",
      "step 3723 loss 0.05834388732910156\n",
      "step 3724 loss 0.05829918012022972\n",
      "step 3725 loss 0.05825459584593773\n",
      "step 3726 loss 0.0582101047039032\n",
      "step 3727 loss 0.05816567316651344\n",
      "step 3728 loss 0.058121319860219955\n",
      "step 3729 loss 0.05807707831263542\n",
      "step 3730 loss 0.058032866567373276\n",
      "step 3731 loss 0.05798877775669098\n",
      "step 3732 loss 0.05794482305645943\n",
      "step 3733 loss 0.057900868356227875\n",
      "step 3734 loss 0.057857055217027664\n",
      "step 3735 loss 0.057813309133052826\n",
      "step 3736 loss 0.05776958912611008\n",
      "step 3737 loss 0.05772601068019867\n",
      "step 3738 loss 0.05768255889415741\n",
      "step 3739 loss 0.05763908848166466\n",
      "step 3740 loss 0.05759574845433235\n",
      "step 3741 loss 0.05755245313048363\n",
      "step 3742 loss 0.05750931799411774\n",
      "step 3743 loss 0.05746620520949364\n",
      "step 3744 loss 0.057423222810029984\n",
      "step 3745 loss 0.05738026648759842\n",
      "step 3746 loss 0.05733739957213402\n",
      "step 3747 loss 0.05729464069008827\n",
      "step 3748 loss 0.05725196748971939\n",
      "step 3749 loss 0.057209327816963196\n",
      "step 3750 loss 0.05716685205698013\n",
      "step 3751 loss 0.05712435767054558\n",
      "step 3752 loss 0.05708199366927147\n",
      "step 3753 loss 0.05703972652554512\n",
      "step 3754 loss 0.05699746683239937\n",
      "step 3755 loss 0.05695533752441406\n",
      "step 3756 loss 0.05691329017281532\n",
      "step 3757 loss 0.05687134712934494\n",
      "step 3758 loss 0.056829385459423065\n",
      "step 3759 loss 0.05678759142756462\n",
      "step 3760 loss 0.056745871901512146\n",
      "step 3761 loss 0.056704215705394745\n",
      "step 3762 loss 0.05666259303689003\n",
      "step 3763 loss 0.05662110447883606\n",
      "step 3764 loss 0.05657961964607239\n",
      "step 3765 loss 0.05653824657201767\n",
      "step 3766 loss 0.0564969964325428\n",
      "step 3767 loss 0.05645580217242241\n",
      "step 3768 loss 0.05641471967101097\n",
      "step 3769 loss 0.05637361854314804\n",
      "step 3770 loss 0.056332625448703766\n",
      "step 3771 loss 0.056291718035936356\n",
      "step 3772 loss 0.05625089630484581\n",
      "step 3773 loss 0.056210145354270935\n",
      "step 3774 loss 0.05616949126124382\n",
      "step 3775 loss 0.056128863245248795\n",
      "step 3776 loss 0.05608829855918884\n",
      "step 3777 loss 0.056047871708869934\n",
      "step 3778 loss 0.056007470935583115\n",
      "step 3779 loss 0.05596713721752167\n",
      "step 3780 loss 0.05592693015933037\n",
      "step 3781 loss 0.055886756628751755\n",
      "step 3782 loss 0.05584662780165672\n",
      "step 3783 loss 0.05580662563443184\n",
      "step 3784 loss 0.05576668307185173\n",
      "step 3785 loss 0.05572681128978729\n",
      "step 3786 loss 0.055687032639980316\n",
      "step 3787 loss 0.05564727634191513\n",
      "step 3788 loss 0.05560761317610741\n",
      "step 3789 loss 0.055568017065525055\n",
      "step 3790 loss 0.05552846938371658\n",
      "step 3791 loss 0.05548909306526184\n",
      "step 3792 loss 0.05544968694448471\n",
      "step 3793 loss 0.055410388857126236\n",
      "step 3794 loss 0.05537116900086403\n",
      "step 3795 loss 0.05533201992511749\n",
      "step 3796 loss 0.05529293045401573\n",
      "step 3797 loss 0.05525388568639755\n",
      "step 3798 loss 0.05521491914987564\n",
      "step 3799 loss 0.055176086723804474\n",
      "step 3800 loss 0.05513723939657211\n",
      "step 3801 loss 0.0550985187292099\n",
      "step 3802 loss 0.055059801787137985\n",
      "step 3803 loss 0.05502118915319443\n",
      "step 3804 loss 0.05498267337679863\n",
      "step 3805 loss 0.05494419485330582\n",
      "step 3806 loss 0.05490577220916748\n",
      "step 3807 loss 0.05486747622489929\n",
      "step 3808 loss 0.054829176515340805\n",
      "step 3809 loss 0.054790984839200974\n",
      "step 3810 loss 0.05475287139415741\n",
      "step 3811 loss 0.05471475422382355\n",
      "step 3812 loss 0.05467678979039192\n",
      "step 3813 loss 0.05463884770870209\n",
      "step 3814 loss 0.05460100248456001\n",
      "step 3815 loss 0.05456319823861122\n",
      "step 3816 loss 0.0545254722237587\n",
      "step 3817 loss 0.054487790912389755\n",
      "step 3818 loss 0.05445025488734245\n",
      "step 3819 loss 0.05441267788410187\n",
      "step 3820 loss 0.05437520146369934\n",
      "step 3821 loss 0.05433784797787666\n",
      "step 3822 loss 0.05430047959089279\n",
      "step 3823 loss 0.054263222962617874\n",
      "step 3824 loss 0.05422600358724594\n",
      "step 3825 loss 0.054188843816518784\n",
      "step 3826 loss 0.05415179580450058\n",
      "step 3827 loss 0.054114799946546555\n",
      "step 3828 loss 0.05407784506678581\n",
      "step 3829 loss 0.05404099076986313\n",
      "step 3830 loss 0.05400410667061806\n",
      "step 3831 loss 0.05396739020943642\n",
      "step 3832 loss 0.05393071100115776\n",
      "step 3833 loss 0.05389409139752388\n",
      "step 3834 loss 0.05385752022266388\n",
      "step 3835 loss 0.05382101982831955\n",
      "step 3836 loss 0.05378458648920059\n",
      "step 3837 loss 0.053748197853565216\n",
      "step 3838 loss 0.053711891174316406\n",
      "step 3839 loss 0.05367565155029297\n",
      "step 3840 loss 0.05363943427801132\n",
      "step 3841 loss 0.053603339940309525\n",
      "step 3842 loss 0.05356726050376892\n",
      "step 3843 loss 0.053531233221292496\n",
      "step 3844 loss 0.05349531024694443\n",
      "step 3845 loss 0.05345945060253143\n",
      "step 3846 loss 0.05342359468340874\n",
      "step 3847 loss 0.0533878467977047\n",
      "step 3848 loss 0.05335216224193573\n",
      "step 3849 loss 0.05331648513674736\n",
      "step 3850 loss 0.05328097939491272\n",
      "step 3851 loss 0.053245414048433304\n",
      "step 3852 loss 0.053209930658340454\n",
      "step 3853 loss 0.05317459627985954\n",
      "step 3854 loss 0.05313921347260475\n",
      "step 3855 loss 0.053103942424058914\n",
      "step 3856 loss 0.05306870862841606\n",
      "step 3857 loss 0.05303354188799858\n",
      "step 3858 loss 0.05299847573041916\n",
      "step 3859 loss 0.052963439375162125\n",
      "step 3860 loss 0.052928466349840164\n",
      "step 3861 loss 0.05289353430271149\n",
      "step 3862 loss 0.05285864695906639\n",
      "step 3863 loss 0.052823882550001144\n",
      "step 3864 loss 0.052789106965065\n",
      "step 3865 loss 0.05275445058941841\n",
      "step 3866 loss 0.05271977558732033\n",
      "step 3867 loss 0.0526852123439312\n",
      "step 3868 loss 0.052650708705186844\n",
      "step 3869 loss 0.05261624976992607\n",
      "step 3870 loss 0.05258186161518097\n",
      "step 3871 loss 0.052547477185726166\n",
      "step 3872 loss 0.052513208240270615\n",
      "step 3873 loss 0.05247897282242775\n",
      "step 3874 loss 0.05244480445981026\n",
      "step 3875 loss 0.052410706877708435\n",
      "step 3876 loss 0.052376650273799896\n",
      "step 3877 loss 0.05234263837337494\n",
      "step 3878 loss 0.052308712154626846\n",
      "step 3879 loss 0.05227477848529816\n",
      "step 3880 loss 0.05224096402525902\n",
      "step 3881 loss 0.05220719426870346\n",
      "step 3882 loss 0.05217346176505089\n",
      "step 3883 loss 0.05213978886604309\n",
      "step 3884 loss 0.05210619047284126\n",
      "step 3885 loss 0.05207264423370361\n",
      "step 3886 loss 0.052039097994565964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3887 loss 0.05200566351413727\n",
      "step 3888 loss 0.05197225511074066\n",
      "step 3889 loss 0.051938943564891815\n",
      "step 3890 loss 0.05190564692020416\n",
      "step 3891 loss 0.05187241733074188\n",
      "step 3892 loss 0.051839232444763184\n",
      "step 3893 loss 0.05180610716342926\n",
      "step 3894 loss 0.05177305266261101\n",
      "step 3895 loss 0.05174003541469574\n",
      "step 3896 loss 0.051707033067941666\n",
      "step 3897 loss 0.05167418345808983\n",
      "step 3898 loss 0.051641300320625305\n",
      "step 3899 loss 0.05160851031541824\n",
      "step 3900 loss 0.05157576501369476\n",
      "step 3901 loss 0.05154307931661606\n",
      "step 3902 loss 0.05151040479540825\n",
      "step 3903 loss 0.0514778234064579\n",
      "step 3904 loss 0.05144529789686203\n",
      "step 3905 loss 0.051412809640169144\n",
      "step 3906 loss 0.051380373537540436\n",
      "step 3907 loss 0.051347989588975906\n",
      "step 3908 loss 0.051315680146217346\n",
      "step 3909 loss 0.05128338187932968\n",
      "step 3910 loss 0.05125116929411888\n",
      "step 3911 loss 0.051218997687101364\n",
      "step 3912 loss 0.051186900585889816\n",
      "step 3913 loss 0.051154784858226776\n",
      "step 3914 loss 0.05112280324101448\n",
      "step 3915 loss 0.05109081789851189\n",
      "step 3916 loss 0.05105890333652496\n",
      "step 3917 loss 0.05102705582976341\n",
      "step 3918 loss 0.050995245575904846\n",
      "step 3919 loss 0.050963450223207474\n",
      "step 3920 loss 0.050931740552186966\n",
      "step 3921 loss 0.050900086760520935\n",
      "step 3922 loss 0.0508684441447258\n",
      "step 3923 loss 0.05083687603473663\n",
      "step 3924 loss 0.050805382430553436\n",
      "step 3925 loss 0.05077391490340233\n",
      "step 3926 loss 0.050742510706186295\n",
      "step 3927 loss 0.05071117356419563\n",
      "step 3928 loss 0.05067982152104378\n",
      "step 3929 loss 0.050648536533117294\n",
      "step 3930 loss 0.050617340952157974\n",
      "step 3931 loss 0.050586190074682236\n",
      "step 3932 loss 0.050555065274238586\n",
      "step 3933 loss 0.05052400007843971\n",
      "step 3934 loss 0.05049298331141472\n",
      "step 3935 loss 0.05046199634671211\n",
      "step 3936 loss 0.05043108016252518\n",
      "step 3937 loss 0.05040022358298302\n",
      "step 3938 loss 0.05036940425634384\n",
      "step 3939 loss 0.050338614732027054\n",
      "step 3940 loss 0.05030786618590355\n",
      "step 3941 loss 0.05027717724442482\n",
      "step 3942 loss 0.050246551632881165\n",
      "step 3943 loss 0.050215937197208405\n",
      "step 3944 loss 0.05018541216850281\n",
      "step 3945 loss 0.05015496164560318\n",
      "step 3946 loss 0.050124481320381165\n",
      "step 3947 loss 0.05009407922625542\n",
      "step 3948 loss 0.05006374791264534\n",
      "step 3949 loss 0.050033435225486755\n",
      "step 3950 loss 0.05000322312116623\n",
      "step 3951 loss 0.04997299611568451\n",
      "step 3952 loss 0.04994281753897667\n",
      "step 3953 loss 0.04991269111633301\n",
      "step 3954 loss 0.04988260194659233\n",
      "step 3955 loss 0.049852609634399414\n",
      "step 3956 loss 0.049822643399238586\n",
      "step 3957 loss 0.04979269206523895\n",
      "step 3958 loss 0.049762822687625885\n",
      "step 3959 loss 0.04973297566175461\n",
      "step 3960 loss 0.049703169614076614\n",
      "step 3961 loss 0.04967343807220459\n",
      "step 3962 loss 0.04964374378323555\n",
      "step 3963 loss 0.04961405694484711\n",
      "step 3964 loss 0.04958444461226463\n",
      "step 3965 loss 0.04955487325787544\n",
      "step 3966 loss 0.04952535033226013\n",
      "step 3967 loss 0.0494958832859993\n",
      "step 3968 loss 0.04946646839380264\n",
      "step 3969 loss 0.049437083303928375\n",
      "step 3970 loss 0.0494077205657959\n",
      "step 3971 loss 0.04937842860817909\n",
      "step 3972 loss 0.04934919998049736\n",
      "step 3973 loss 0.04931996390223503\n",
      "step 3974 loss 0.049290791153907776\n",
      "step 3975 loss 0.04926170036196709\n",
      "step 3976 loss 0.0492325983941555\n",
      "step 3977 loss 0.049203578382730484\n",
      "step 3978 loss 0.04917459562420845\n",
      "step 3979 loss 0.04914567619562149\n",
      "step 3980 loss 0.04911675304174423\n",
      "step 3981 loss 0.049087896943092346\n",
      "step 3982 loss 0.049059104174375534\n",
      "step 3983 loss 0.04903031513094902\n",
      "step 3984 loss 0.04900160804390907\n",
      "step 3985 loss 0.04897289350628853\n",
      "step 3986 loss 0.048944272100925446\n",
      "step 3987 loss 0.04891572147607803\n",
      "step 3988 loss 0.04888711869716644\n",
      "step 3989 loss 0.04885859414935112\n",
      "step 3990 loss 0.04883015528321266\n",
      "step 3991 loss 0.0488017275929451\n",
      "step 3992 loss 0.048773352056741714\n",
      "step 3993 loss 0.04874502122402191\n",
      "step 3994 loss 0.0487167090177536\n",
      "step 3995 loss 0.04868842661380768\n",
      "step 3996 loss 0.04866025596857071\n",
      "step 3997 loss 0.04863205924630165\n",
      "step 3998 loss 0.04860392585396767\n",
      "step 3999 loss 0.04857582971453667\n",
      "step 4000 loss 0.04854778200387955\n",
      "step 4001 loss 0.0485198050737381\n",
      "step 4002 loss 0.048491835594177246\n",
      "step 4003 loss 0.04846389591693878\n",
      "step 4004 loss 0.04843604937195778\n",
      "step 4005 loss 0.048408232629299164\n",
      "step 4006 loss 0.048380397260189056\n",
      "step 4007 loss 0.04835266247391701\n",
      "step 4008 loss 0.048324912786483765\n",
      "step 4009 loss 0.04829725623130798\n",
      "step 4010 loss 0.04826962575316429\n",
      "step 4011 loss 0.04824202507734299\n",
      "step 4012 loss 0.04821445047855377\n",
      "step 4013 loss 0.048186980187892914\n",
      "step 4014 loss 0.048159461468458176\n",
      "step 4015 loss 0.0481320284307003\n",
      "step 4016 loss 0.0481046624481678\n",
      "step 4017 loss 0.0480772964656353\n",
      "step 4018 loss 0.04804997518658638\n",
      "step 4019 loss 0.04802275821566582\n",
      "step 4020 loss 0.04799551144242287\n",
      "step 4021 loss 0.047968313097953796\n",
      "step 4022 loss 0.0479411780834198\n",
      "step 4023 loss 0.04791407659649849\n",
      "step 4024 loss 0.047886982560157776\n",
      "step 4025 loss 0.04785994440317154\n",
      "step 4026 loss 0.04783293232321739\n",
      "step 4027 loss 0.04780598729848862\n",
      "step 4028 loss 0.047779060900211334\n",
      "step 4029 loss 0.047752171754837036\n",
      "step 4030 loss 0.04772534221410751\n",
      "step 4031 loss 0.04769856110215187\n",
      "step 4032 loss 0.047671783715486526\n",
      "step 4033 loss 0.04764508828520775\n",
      "step 4034 loss 0.047618426382541656\n",
      "step 4035 loss 0.047591742128133774\n",
      "step 4036 loss 0.047565169632434845\n",
      "step 4037 loss 0.04753858968615532\n",
      "step 4038 loss 0.04751204326748848\n",
      "step 4039 loss 0.047485578805208206\n",
      "step 4040 loss 0.047459136694669724\n",
      "step 4041 loss 0.04743272811174393\n",
      "step 4042 loss 0.04740634560585022\n",
      "step 4043 loss 0.04738001897931099\n",
      "step 4044 loss 0.04735371470451355\n",
      "step 4045 loss 0.04732746258378029\n",
      "step 4046 loss 0.04730123654007912\n",
      "step 4047 loss 0.04727501794695854\n",
      "step 4048 loss 0.047248877584934235\n",
      "step 4049 loss 0.0472228080034256\n",
      "step 4050 loss 0.04719670116901398\n",
      "step 4051 loss 0.04717068746685982\n",
      "step 4052 loss 0.04714466258883476\n",
      "step 4053 loss 0.04711871221661568\n",
      "step 4054 loss 0.047092799097299576\n",
      "step 4055 loss 0.04706688970327377\n",
      "step 4056 loss 0.047041043639183044\n",
      "step 4057 loss 0.047015201300382614\n",
      "step 4058 loss 0.04698944464325905\n",
      "step 4059 loss 0.04696372151374817\n",
      "step 4060 loss 0.046938009560108185\n",
      "step 4061 loss 0.046912334859371185\n",
      "step 4062 loss 0.04688673093914986\n",
      "step 4063 loss 0.04686112701892853\n",
      "step 4064 loss 0.04683553799986839\n",
      "step 4065 loss 0.04681001231074333\n",
      "step 4066 loss 0.04678453505039215\n",
      "step 4067 loss 0.04675910994410515\n",
      "step 4068 loss 0.04673366993665695\n",
      "step 4069 loss 0.04670829698443413\n",
      "step 4070 loss 0.04668295755982399\n",
      "step 4071 loss 0.046657636761665344\n",
      "step 4072 loss 0.04663239046931267\n",
      "step 4073 loss 0.04660715162754059\n",
      "step 4074 loss 0.0465819351375103\n",
      "step 4075 loss 0.04655679687857628\n",
      "step 4076 loss 0.046531639993190765\n",
      "step 4077 loss 0.04650658741593361\n",
      "step 4078 loss 0.04648151248693466\n",
      "step 4079 loss 0.046456459909677505\n",
      "step 4080 loss 0.04643149673938751\n",
      "step 4081 loss 0.046406522393226624\n",
      "step 4082 loss 0.04638161510229111\n",
      "step 4083 loss 0.046356748789548874\n",
      "step 4084 loss 0.04633187875151634\n",
      "step 4085 loss 0.046307068318128586\n",
      "step 4086 loss 0.04628231003880501\n",
      "step 4087 loss 0.04625754430890083\n",
      "step 4088 loss 0.04623283073306084\n",
      "step 4089 loss 0.04620817303657532\n",
      "step 4090 loss 0.0461835153400898\n",
      "step 4091 loss 0.04615892469882965\n",
      "step 4092 loss 0.0461343489587307\n",
      "step 4093 loss 0.046109817922115326\n",
      "step 4094 loss 0.046085335314273834\n",
      "step 4095 loss 0.046060819178819656\n",
      "step 4096 loss 0.046036384999752045\n",
      "step 4097 loss 0.04601195454597473\n",
      "step 4098 loss 0.04598763585090637\n",
      "step 4099 loss 0.04596325755119324\n",
      "step 4100 loss 0.04593898728489876\n",
      "step 4101 loss 0.04591471701860428\n",
      "step 4102 loss 0.04589048773050308\n",
      "step 4103 loss 0.04586626589298248\n",
      "step 4104 loss 0.04584211856126785\n",
      "step 4105 loss 0.04581794887781143\n",
      "step 4106 loss 0.04579389467835426\n",
      "step 4107 loss 0.04576979950070381\n",
      "step 4108 loss 0.04574578255414963\n",
      "step 4109 loss 0.04572177305817604\n",
      "step 4110 loss 0.04569781944155693\n",
      "step 4111 loss 0.04567386955022812\n",
      "step 4112 loss 0.04564995691180229\n",
      "step 4113 loss 0.04562614485621452\n",
      "step 4114 loss 0.04560227692127228\n",
      "step 4115 loss 0.0455784909427166\n",
      "step 4116 loss 0.04555468633770943\n",
      "step 4117 loss 0.04553094878792763\n",
      "step 4118 loss 0.04550724849104881\n",
      "step 4119 loss 0.04548358544707298\n",
      "step 4120 loss 0.04545992985367775\n",
      "step 4121 loss 0.04543634504079819\n",
      "step 4122 loss 0.04541277885437012\n",
      "step 4123 loss 0.04538924619555473\n",
      "step 4124 loss 0.04536570608615875\n",
      "step 4125 loss 0.04534221813082695\n",
      "step 4126 loss 0.04531878978013992\n",
      "step 4127 loss 0.045295342803001404\n",
      "step 4128 loss 0.045271988958120346\n",
      "step 4129 loss 0.04524862393736839\n",
      "step 4130 loss 0.04522526264190674\n",
      "step 4131 loss 0.04520202800631523\n",
      "step 4132 loss 0.045178767293691635\n",
      "step 4133 loss 0.04515552520751953\n",
      "step 4134 loss 0.04513233155012131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4135 loss 0.04510917514562607\n",
      "step 4136 loss 0.04508604854345322\n",
      "step 4137 loss 0.04506295174360275\n",
      "step 4138 loss 0.04503987357020378\n",
      "step 4139 loss 0.04501684010028839\n",
      "step 4140 loss 0.04499382898211479\n",
      "step 4141 loss 0.04497085511684418\n",
      "step 4142 loss 0.044947899878025055\n",
      "step 4143 loss 0.04492500424385071\n",
      "step 4144 loss 0.04490213468670845\n",
      "step 4145 loss 0.04487927258014679\n",
      "step 4146 loss 0.04485644772648811\n",
      "step 4147 loss 0.04483365640044212\n",
      "step 4148 loss 0.04481092467904091\n",
      "step 4149 loss 0.044788166880607605\n",
      "step 4150 loss 0.04476545378565788\n",
      "step 4151 loss 0.04474281892180443\n",
      "step 4152 loss 0.04472016170620918\n",
      "step 4153 loss 0.04469755291938782\n",
      "step 4154 loss 0.04467499256134033\n",
      "step 4155 loss 0.044652462005615234\n",
      "step 4156 loss 0.04462992772459984\n",
      "step 4157 loss 0.04460744932293892\n",
      "step 4158 loss 0.04458498954772949\n",
      "step 4159 loss 0.044562578201293945\n",
      "step 4160 loss 0.044540178030729294\n",
      "step 4161 loss 0.04451780393719673\n",
      "step 4162 loss 0.044495467096567154\n",
      "step 4163 loss 0.044473160058259964\n",
      "step 4164 loss 0.04445086792111397\n",
      "step 4165 loss 0.044428642839193344\n",
      "step 4166 loss 0.044406428933143616\n",
      "step 4167 loss 0.04438425227999687\n",
      "step 4168 loss 0.04436207190155983\n",
      "step 4169 loss 0.04433996230363846\n",
      "step 4170 loss 0.04431786388158798\n",
      "step 4171 loss 0.04429580643773079\n",
      "step 4172 loss 0.0442737452685833\n",
      "step 4173 loss 0.04425176605582237\n",
      "step 4174 loss 0.044229768216609955\n",
      "step 4175 loss 0.04420779272913933\n",
      "step 4176 loss 0.04418588802218437\n",
      "step 4177 loss 0.04416397958993912\n",
      "step 4178 loss 0.044142141938209534\n",
      "step 4179 loss 0.044120293110609055\n",
      "step 4180 loss 0.04409848153591156\n",
      "step 4181 loss 0.044076692312955856\n",
      "step 4182 loss 0.04405497387051582\n",
      "step 4183 loss 0.044033244252204895\n",
      "step 4184 loss 0.04401156306266785\n",
      "step 4185 loss 0.0439898818731308\n",
      "step 4186 loss 0.043968282639980316\n",
      "step 4187 loss 0.04394666105508804\n",
      "step 4188 loss 0.043925076723098755\n",
      "step 4189 loss 0.04390353336930275\n",
      "step 4190 loss 0.04388202726840973\n",
      "step 4191 loss 0.04386052116751671\n",
      "step 4192 loss 0.04383905231952667\n",
      "step 4193 loss 0.04381753131747246\n",
      "step 4194 loss 0.04379604384303093\n",
      "step 4195 loss 0.04377460479736328\n",
      "step 4196 loss 0.04375314712524414\n",
      "step 4197 loss 0.04373174533247948\n",
      "step 4198 loss 0.04371033236384392\n",
      "step 4199 loss 0.04368895664811134\n",
      "step 4200 loss 0.04366762563586235\n",
      "step 4201 loss 0.04364631697535515\n",
      "step 4202 loss 0.04362499341368675\n",
      "step 4203 loss 0.04360374063253403\n",
      "step 4204 loss 0.043582458049058914\n",
      "step 4205 loss 0.04356126859784126\n",
      "step 4206 loss 0.043540049344301224\n",
      "step 4207 loss 0.04351888597011566\n",
      "step 4208 loss 0.04349774494767189\n",
      "step 4209 loss 0.04347661882638931\n",
      "step 4210 loss 0.04345554858446121\n",
      "step 4211 loss 0.04343445599079132\n",
      "step 4212 loss 0.0434134304523468\n",
      "step 4213 loss 0.04339243471622467\n",
      "step 4214 loss 0.043371450155973434\n",
      "step 4215 loss 0.0433504693210125\n",
      "step 4216 loss 0.04332953318953514\n",
      "step 4217 loss 0.04330863058567047\n",
      "step 4218 loss 0.04328777641057968\n",
      "step 4219 loss 0.043266914784908295\n",
      "step 4220 loss 0.0432460643351078\n",
      "step 4221 loss 0.043225258588790894\n",
      "step 4222 loss 0.043204501271247864\n",
      "step 4223 loss 0.04318378120660782\n",
      "step 4224 loss 0.04316306486725807\n",
      "step 4225 loss 0.043142348527908325\n",
      "step 4226 loss 0.043121691793203354\n",
      "step 4227 loss 0.04310110956430435\n",
      "step 4228 loss 0.04308048263192177\n",
      "step 4229 loss 0.04305990785360336\n",
      "step 4230 loss 0.043039340525865555\n",
      "step 4231 loss 0.043018829077482224\n",
      "step 4232 loss 0.04299831762909889\n",
      "step 4233 loss 0.04297786206007004\n",
      "step 4234 loss 0.04295741021633148\n",
      "step 4235 loss 0.04293697699904442\n",
      "step 4236 loss 0.042916618287563324\n",
      "step 4237 loss 0.042896222323179245\n",
      "step 4238 loss 0.04287591576576233\n",
      "step 4239 loss 0.042855605483055115\n",
      "step 4240 loss 0.0428353026509285\n",
      "step 4241 loss 0.042815037071704865\n",
      "step 4242 loss 0.04279481992125511\n",
      "step 4243 loss 0.042774591594934464\n",
      "step 4244 loss 0.04275442287325859\n",
      "step 4245 loss 0.042734261602163315\n",
      "step 4246 loss 0.04271414130926132\n",
      "step 4247 loss 0.04269406199455261\n",
      "step 4248 loss 0.042673978954553604\n",
      "step 4249 loss 0.04265390336513519\n",
      "step 4250 loss 0.04263392463326454\n",
      "step 4251 loss 0.042613908648490906\n",
      "step 4252 loss 0.04259393736720085\n",
      "step 4253 loss 0.04257402569055557\n",
      "step 4254 loss 0.04255407676100731\n",
      "step 4255 loss 0.042534198611974716\n",
      "step 4256 loss 0.042514342814683914\n",
      "step 4257 loss 0.04249448701739311\n",
      "step 4258 loss 0.042474664747714996\n",
      "step 4259 loss 0.04245489090681076\n",
      "step 4260 loss 0.042435139417648315\n",
      "step 4261 loss 0.042415402829647064\n",
      "step 4262 loss 0.04239566624164581\n",
      "step 4263 loss 0.04237602278590202\n",
      "step 4264 loss 0.04235638305544853\n",
      "step 4265 loss 0.04233673959970474\n",
      "step 4266 loss 0.04231710359454155\n",
      "step 4267 loss 0.04229752719402313\n",
      "step 4268 loss 0.042277976870536804\n",
      "step 4269 loss 0.042258452624082565\n",
      "step 4270 loss 0.04223892092704773\n",
      "step 4271 loss 0.042219456285238266\n",
      "step 4272 loss 0.0422000028192997\n",
      "step 4273 loss 0.04218057170510292\n",
      "step 4274 loss 0.042161207646131516\n",
      "step 4275 loss 0.04214179143309593\n",
      "step 4276 loss 0.04212243854999542\n",
      "step 4277 loss 0.042103126645088196\n",
      "step 4278 loss 0.04208384081721306\n",
      "step 4279 loss 0.042064543813467026\n",
      "step 4280 loss 0.04204528406262398\n",
      "step 4281 loss 0.0420260913670063\n",
      "step 4282 loss 0.04200687259435654\n",
      "step 4283 loss 0.041987694799900055\n",
      "step 4284 loss 0.04196852818131447\n",
      "step 4285 loss 0.04194943606853485\n",
      "step 4286 loss 0.04193030670285225\n",
      "step 4287 loss 0.04191121459007263\n",
      "step 4288 loss 0.04189218580722809\n",
      "step 4289 loss 0.04187316074967384\n",
      "step 4290 loss 0.04185416176915169\n",
      "step 4291 loss 0.04183516651391983\n",
      "step 4292 loss 0.04181620478630066\n",
      "step 4293 loss 0.04179726541042328\n",
      "step 4294 loss 0.04177836328744888\n",
      "step 4295 loss 0.04175947606563568\n",
      "step 4296 loss 0.041740644723176956\n",
      "step 4297 loss 0.041721802204847336\n",
      "step 4298 loss 0.0417029894888401\n",
      "step 4299 loss 0.041684187948703766\n",
      "step 4300 loss 0.04166543111205101\n",
      "step 4301 loss 0.04164670780301094\n",
      "step 4302 loss 0.04162799194455147\n",
      "step 4303 loss 0.04160929098725319\n",
      "step 4304 loss 0.041590653359889984\n",
      "step 4305 loss 0.04157201200723648\n",
      "step 4306 loss 0.04155335575342178\n",
      "step 4307 loss 0.04153480380773544\n",
      "step 4308 loss 0.04151621460914612\n",
      "step 4309 loss 0.04149770736694336\n",
      "step 4310 loss 0.04147917404770851\n",
      "step 4311 loss 0.04146067053079605\n",
      "step 4312 loss 0.04144219309091568\n",
      "step 4313 loss 0.04142376035451889\n",
      "step 4314 loss 0.041405320167541504\n",
      "step 4315 loss 0.04138694331049919\n",
      "step 4316 loss 0.041368551552295685\n",
      "step 4317 loss 0.04135023429989815\n",
      "step 4318 loss 0.04133187234401703\n",
      "step 4319 loss 0.04131359979510307\n",
      "step 4320 loss 0.04129529371857643\n",
      "step 4321 loss 0.04127703607082367\n",
      "step 4322 loss 0.0412587933242321\n",
      "step 4323 loss 0.04124061018228531\n",
      "step 4324 loss 0.04122241958975792\n",
      "step 4325 loss 0.04120425134897232\n",
      "step 4326 loss 0.04118608310818672\n",
      "step 4327 loss 0.04116799309849739\n",
      "step 4328 loss 0.04114990308880806\n",
      "step 4329 loss 0.04113183170557022\n",
      "step 4330 loss 0.04111376777291298\n",
      "step 4331 loss 0.04109574854373932\n",
      "step 4332 loss 0.041077736765146255\n",
      "step 4333 loss 0.041059769690036774\n",
      "step 4334 loss 0.041041821241378784\n",
      "step 4335 loss 0.04102390259504318\n",
      "step 4336 loss 0.04100596532225609\n",
      "step 4337 loss 0.04098809137940407\n",
      "step 4338 loss 0.04097023606300354\n",
      "step 4339 loss 0.0409524142742157\n",
      "step 4340 loss 0.04093458130955696\n",
      "step 4341 loss 0.040916796773672104\n",
      "step 4342 loss 0.040899038314819336\n",
      "step 4343 loss 0.04088127613067627\n",
      "step 4344 loss 0.04086355119943619\n",
      "step 4345 loss 0.0408458411693573\n",
      "step 4346 loss 0.0408281646668911\n",
      "step 4347 loss 0.04081049934029579\n",
      "step 4348 loss 0.040792860090732574\n",
      "step 4349 loss 0.04077526554465294\n",
      "step 4350 loss 0.040757689625024796\n",
      "step 4351 loss 0.04074009880423546\n",
      "step 4352 loss 0.0407225601375103\n",
      "step 4353 loss 0.04070504754781723\n",
      "step 4354 loss 0.04068756103515625\n",
      "step 4355 loss 0.04067005217075348\n",
      "step 4356 loss 0.040652595460414886\n",
      "step 4357 loss 0.04063519462943077\n",
      "step 4358 loss 0.04061777889728546\n",
      "step 4359 loss 0.04060038551688194\n",
      "step 4360 loss 0.04058301821351051\n",
      "step 4361 loss 0.04056568071246147\n",
      "step 4362 loss 0.040548354387283325\n",
      "step 4363 loss 0.04053107276558876\n",
      "step 4364 loss 0.0405137725174427\n",
      "step 4365 loss 0.04049653932452202\n",
      "step 4366 loss 0.04047928377985954\n",
      "step 4367 loss 0.04046207293868065\n",
      "step 4368 loss 0.040444888174533844\n",
      "step 4369 loss 0.040427714586257935\n",
      "step 4370 loss 0.04041053727269173\n",
      "step 4371 loss 0.04039347544312477\n",
      "step 4372 loss 0.04037635773420334\n",
      "step 4373 loss 0.040359292179346085\n",
      "step 4374 loss 0.040342267602682114\n",
      "step 4375 loss 0.04032519459724426\n",
      "step 4376 loss 0.04030818119645119\n",
      "step 4377 loss 0.0402911938726902\n",
      "step 4378 loss 0.0402742400765419\n",
      "step 4379 loss 0.04025724530220032\n",
      "step 4380 loss 0.0402403362095356\n",
      "step 4381 loss 0.04022342711687088\n",
      "step 4382 loss 0.04020654782652855\n",
      "step 4383 loss 0.040189698338508606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4384 loss 0.04017286375164986\n",
      "step 4385 loss 0.04015602171421051\n",
      "step 4386 loss 0.04013925418257713\n",
      "step 4387 loss 0.04012245684862137\n",
      "step 4388 loss 0.04010571539402008\n",
      "step 4389 loss 0.04008897393941879\n",
      "step 4390 loss 0.04007226601243019\n",
      "step 4391 loss 0.04005558043718338\n",
      "step 4392 loss 0.04003891348838806\n",
      "step 4393 loss 0.04002225771546364\n",
      "step 4394 loss 0.0400056466460228\n",
      "step 4395 loss 0.03998903930187225\n",
      "step 4396 loss 0.03997243940830231\n",
      "step 4397 loss 0.03995586931705475\n",
      "step 4398 loss 0.039939336478710175\n",
      "step 4399 loss 0.03992282226681709\n",
      "step 4400 loss 0.03990631923079491\n",
      "step 4401 loss 0.039889849722385406\n",
      "step 4402 loss 0.03987337276339531\n",
      "step 4403 loss 0.039856936782598495\n",
      "step 4404 loss 0.03984053432941437\n",
      "step 4405 loss 0.03982413187623024\n",
      "step 4406 loss 0.039807748049497604\n",
      "step 4407 loss 0.039791397750377655\n",
      "step 4408 loss 0.039775069802999496\n",
      "step 4409 loss 0.039758745580911636\n",
      "step 4410 loss 0.03974243998527527\n",
      "step 4411 loss 0.03972618654370308\n",
      "step 4412 loss 0.03970995172858238\n",
      "step 4413 loss 0.039693716913461685\n",
      "step 4414 loss 0.03967749699950218\n",
      "step 4415 loss 0.03966132923960686\n",
      "step 4416 loss 0.03964513912796974\n",
      "step 4417 loss 0.039628997445106506\n",
      "step 4418 loss 0.039612896740436554\n",
      "step 4419 loss 0.039596784859895706\n",
      "step 4420 loss 0.039580680429935455\n",
      "step 4421 loss 0.039564620703458786\n",
      "step 4422 loss 0.039548564702272415\n",
      "step 4423 loss 0.039532579481601715\n",
      "step 4424 loss 0.03951657563447952\n",
      "step 4425 loss 0.03950054571032524\n",
      "step 4426 loss 0.03948463127017021\n",
      "step 4427 loss 0.03946869075298309\n",
      "step 4428 loss 0.03945276886224747\n",
      "step 4429 loss 0.03943685442209244\n",
      "step 4430 loss 0.039420973509550095\n",
      "step 4431 loss 0.039405111223459244\n",
      "step 4432 loss 0.03938925266265869\n",
      "step 4433 loss 0.03937344625592232\n",
      "step 4434 loss 0.03935764729976654\n",
      "step 4435 loss 0.03934185579419136\n",
      "step 4436 loss 0.03932612016797066\n",
      "step 4437 loss 0.03931034728884697\n",
      "step 4438 loss 0.03929464891552925\n",
      "step 4439 loss 0.03927891328930855\n",
      "step 4440 loss 0.039263248443603516\n",
      "step 4441 loss 0.0392475351691246\n",
      "step 4442 loss 0.0392317958176136\n",
      "step 4443 loss 0.0392160639166832\n",
      "step 4444 loss 0.039200346916913986\n",
      "step 4445 loss 0.03918460011482239\n",
      "step 4446 loss 0.03916886821389198\n",
      "step 4447 loss 0.039153117686510086\n",
      "step 4448 loss 0.039137452840805054\n",
      "step 4449 loss 0.03912175074219704\n",
      "step 4450 loss 0.03910604864358902\n",
      "step 4451 loss 0.039090391248464584\n",
      "step 4452 loss 0.039074692875146866\n",
      "step 4453 loss 0.03905905783176422\n",
      "step 4454 loss 0.03904341906309128\n",
      "step 4455 loss 0.03902779892086983\n",
      "step 4456 loss 0.03901218622922897\n",
      "step 4457 loss 0.03899658843874931\n",
      "step 4458 loss 0.03898102417588234\n",
      "step 4459 loss 0.038965459913015366\n",
      "step 4460 loss 0.038949910551309586\n",
      "step 4461 loss 0.038934364914894104\n",
      "step 4462 loss 0.0389188677072525\n",
      "step 4463 loss 0.0389033779501915\n",
      "step 4464 loss 0.038887929171323776\n",
      "step 4465 loss 0.03887244686484337\n",
      "step 4466 loss 0.038857001811265945\n",
      "step 4467 loss 0.03884158283472061\n",
      "step 4468 loss 0.038826171308755875\n",
      "step 4469 loss 0.038810811936855316\n",
      "step 4470 loss 0.03879541531205177\n",
      "step 4471 loss 0.0387800931930542\n",
      "step 4472 loss 0.038764748722314835\n",
      "step 4473 loss 0.03874943405389786\n",
      "step 4474 loss 0.03873412311077118\n",
      "step 4475 loss 0.038718849420547485\n",
      "step 4476 loss 0.0387035608291626\n",
      "step 4477 loss 0.038688261061906815\n",
      "step 4478 loss 0.03867284581065178\n",
      "step 4479 loss 0.038657426834106445\n",
      "step 4480 loss 0.03864201158285141\n",
      "step 4481 loss 0.03862656280398369\n",
      "step 4482 loss 0.038611169904470444\n",
      "step 4483 loss 0.03859575092792511\n",
      "step 4484 loss 0.03858033940196037\n",
      "step 4485 loss 0.03856492415070534\n",
      "step 4486 loss 0.038549523800611496\n",
      "step 4487 loss 0.03853415325284004\n",
      "step 4488 loss 0.03851877525448799\n",
      "step 4489 loss 0.03850339725613594\n",
      "step 4490 loss 0.038488052785396576\n",
      "step 4491 loss 0.038472700864076614\n",
      "step 4492 loss 0.03845738619565964\n",
      "step 4493 loss 0.03844207897782326\n",
      "step 4494 loss 0.038426756858825684\n",
      "step 4495 loss 0.038411494344472885\n",
      "step 4496 loss 0.03839621692895889\n",
      "step 4497 loss 0.038380954414606094\n",
      "step 4498 loss 0.038365717977285385\n",
      "step 4499 loss 0.03835050016641617\n",
      "step 4500 loss 0.03833528980612755\n",
      "step 4501 loss 0.03832010179758072\n",
      "step 4502 loss 0.03830492123961449\n",
      "step 4503 loss 0.03828975930809975\n",
      "step 4504 loss 0.03827463462948799\n",
      "step 4505 loss 0.038259509950876236\n",
      "step 4506 loss 0.03824441134929657\n",
      "step 4507 loss 0.03822933882474899\n",
      "step 4508 loss 0.03821425884962082\n",
      "step 4509 loss 0.03819920867681503\n",
      "step 4510 loss 0.03818416967988014\n",
      "step 4511 loss 0.03816913068294525\n",
      "step 4512 loss 0.038154155015945435\n",
      "step 4513 loss 0.03813914954662323\n",
      "step 4514 loss 0.038124218583106995\n",
      "step 4515 loss 0.038109272718429565\n",
      "step 4516 loss 0.03809434920549393\n",
      "step 4517 loss 0.03807944431900978\n",
      "step 4518 loss 0.03806452453136444\n",
      "step 4519 loss 0.03804967924952507\n",
      "step 4520 loss 0.0380348265171051\n",
      "step 4521 loss 0.03801998496055603\n",
      "step 4522 loss 0.038005173206329346\n",
      "step 4523 loss 0.03799038007855415\n",
      "step 4524 loss 0.03797558695077896\n",
      "step 4525 loss 0.03796083480119705\n",
      "step 4526 loss 0.037946101278066635\n",
      "step 4527 loss 0.037931375205516815\n",
      "step 4528 loss 0.03791667893528938\n",
      "step 4529 loss 0.037901975214481354\n",
      "step 4530 loss 0.037887319922447205\n",
      "step 4531 loss 0.037872642278671265\n",
      "step 4532 loss 0.037858009338378906\n",
      "step 4533 loss 0.03784340247511864\n",
      "step 4534 loss 0.03782879188656807\n",
      "step 4535 loss 0.037814248353242874\n",
      "step 4536 loss 0.037799667567014694\n",
      "step 4537 loss 0.0377851203083992\n",
      "step 4538 loss 0.0377705916762352\n",
      "step 4539 loss 0.037756096571683884\n",
      "step 4540 loss 0.03774159774184227\n",
      "step 4541 loss 0.03772712126374245\n",
      "step 4542 loss 0.037712693214416504\n",
      "step 4543 loss 0.03769824653863907\n",
      "step 4544 loss 0.03768382966518402\n",
      "step 4545 loss 0.03766942396759987\n",
      "step 4546 loss 0.03765504062175751\n",
      "step 4547 loss 0.037640683352947235\n",
      "step 4548 loss 0.03762633726000786\n",
      "step 4549 loss 0.037612006068229675\n",
      "step 4550 loss 0.03759771212935448\n",
      "step 4551 loss 0.03758342191576958\n",
      "step 4552 loss 0.037569135427474976\n",
      "step 4553 loss 0.03755488991737366\n",
      "step 4554 loss 0.03754065930843353\n",
      "step 4555 loss 0.03752643242478371\n",
      "step 4556 loss 0.037512216717004776\n",
      "step 4557 loss 0.03749803453683853\n",
      "step 4558 loss 0.037483859807252884\n",
      "step 4559 loss 0.037469714879989624\n",
      "step 4560 loss 0.037455588579177856\n",
      "step 4561 loss 0.03744148463010788\n",
      "step 4562 loss 0.03742735832929611\n",
      "step 4563 loss 0.03741329163312912\n",
      "step 4564 loss 0.03739923983812332\n",
      "step 4565 loss 0.03738516941666603\n",
      "step 4566 loss 0.037371158599853516\n",
      "step 4567 loss 0.037357147783041\n",
      "step 4568 loss 0.03734315559267998\n",
      "step 4569 loss 0.037329159677028656\n",
      "step 4570 loss 0.03731520101428032\n",
      "step 4571 loss 0.03730126470327377\n",
      "step 4572 loss 0.03728735074400902\n",
      "step 4573 loss 0.03727345168590546\n",
      "step 4574 loss 0.037259552627801895\n",
      "step 4575 loss 0.03724568337202072\n",
      "step 4576 loss 0.03723182529211044\n",
      "step 4577 loss 0.037218011915683746\n",
      "step 4578 loss 0.03720417991280556\n",
      "step 4579 loss 0.03719034790992737\n",
      "step 4580 loss 0.03717658668756485\n",
      "step 4581 loss 0.03716282546520233\n",
      "step 4582 loss 0.03714906424283981\n",
      "step 4583 loss 0.03713534399867058\n",
      "step 4584 loss 0.03712160885334015\n",
      "step 4585 loss 0.03710790351033211\n",
      "step 4586 loss 0.037094224244356155\n",
      "step 4587 loss 0.037080541253089905\n",
      "step 4588 loss 0.037066902965307236\n",
      "step 4589 loss 0.03705326467752457\n",
      "step 4590 loss 0.03703964874148369\n",
      "step 4591 loss 0.0370260514318943\n",
      "step 4592 loss 0.03701246902346611\n",
      "step 4593 loss 0.036998897790908813\n",
      "step 4594 loss 0.036985356360673904\n",
      "step 4595 loss 0.036971818655729294\n",
      "step 4596 loss 0.036958321928977966\n",
      "step 4597 loss 0.03694482147693634\n",
      "step 4598 loss 0.03693133592605591\n",
      "step 4599 loss 0.036917880177497864\n",
      "step 4600 loss 0.03690440580248833\n",
      "step 4601 loss 0.03689099848270416\n",
      "step 4602 loss 0.036877576261758804\n",
      "step 4603 loss 0.036864157766103745\n",
      "step 4604 loss 0.03685079514980316\n",
      "step 4605 loss 0.03683745488524437\n",
      "step 4606 loss 0.03682409226894379\n",
      "step 4607 loss 0.0368107408285141\n",
      "step 4608 loss 0.036797426640987396\n",
      "step 4609 loss 0.03678413853049278\n",
      "step 4610 loss 0.036770857870578766\n",
      "step 4611 loss 0.03675760701298714\n",
      "step 4612 loss 0.0367443710565567\n",
      "step 4613 loss 0.03673112019896507\n",
      "step 4614 loss 0.03671788051724434\n",
      "step 4615 loss 0.03670470789074898\n",
      "step 4616 loss 0.03669153153896332\n",
      "step 4617 loss 0.03667835891246796\n",
      "step 4618 loss 0.03666521608829498\n",
      "step 4619 loss 0.03665207326412201\n",
      "step 4620 loss 0.03663894534111023\n",
      "step 4621 loss 0.036625832319259644\n",
      "step 4622 loss 0.036612749099731445\n",
      "step 4623 loss 0.03659970685839653\n",
      "step 4624 loss 0.036586642265319824\n",
      "step 4625 loss 0.036573611199855804\n",
      "step 4626 loss 0.03656059131026268\n",
      "step 4627 loss 0.03654758632183075\n",
      "step 4628 loss 0.03653460368514061\n",
      "step 4629 loss 0.036521635949611664\n",
      "step 4630 loss 0.036508675664663315\n",
      "step 4631 loss 0.03649573400616646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4632 loss 0.03648281842470169\n",
      "step 4633 loss 0.03646990656852722\n",
      "step 4634 loss 0.03645702823996544\n",
      "step 4635 loss 0.03644416108727455\n",
      "step 4636 loss 0.03643130883574486\n",
      "step 4637 loss 0.036418452858924866\n",
      "step 4638 loss 0.03640562668442726\n",
      "step 4639 loss 0.036392807960510254\n",
      "step 4640 loss 0.036380019038915634\n",
      "step 4641 loss 0.03636723384261131\n",
      "step 4642 loss 0.03635447844862938\n",
      "step 4643 loss 0.036341723054647446\n",
      "step 4644 loss 0.03632901608943939\n",
      "step 4645 loss 0.036316290497779846\n",
      "step 4646 loss 0.03630360588431358\n",
      "step 4647 loss 0.036290887743234634\n",
      "step 4648 loss 0.036278244107961655\n",
      "step 4649 loss 0.03626556694507599\n",
      "step 4650 loss 0.0362529382109642\n",
      "step 4651 loss 0.036240335553884506\n",
      "step 4652 loss 0.036227747797966\n",
      "step 4653 loss 0.03621513396501541\n",
      "step 4654 loss 0.0362025648355484\n",
      "step 4655 loss 0.036190010607242584\n",
      "step 4656 loss 0.03617746755480766\n",
      "step 4657 loss 0.036164943128824234\n",
      "step 4658 loss 0.036152433604002\n",
      "step 4659 loss 0.03613993152976036\n",
      "step 4660 loss 0.03612745925784111\n",
      "step 4661 loss 0.036114998161792755\n",
      "step 4662 loss 0.0361025333404541\n",
      "step 4663 loss 0.03609011322259903\n",
      "step 4664 loss 0.036077700555324554\n",
      "step 4665 loss 0.036065276712179184\n",
      "step 4666 loss 0.0360528901219368\n",
      "step 4667 loss 0.0360405407845974\n",
      "step 4668 loss 0.0360281839966774\n",
      "step 4669 loss 0.03601585328578949\n",
      "step 4670 loss 0.036003515124320984\n",
      "step 4671 loss 0.03599120303988457\n",
      "step 4672 loss 0.035978950560092926\n",
      "step 4673 loss 0.035966645926237106\n",
      "step 4674 loss 0.03595438227057457\n",
      "step 4675 loss 0.03594214469194412\n",
      "step 4676 loss 0.035929903388023376\n",
      "step 4677 loss 0.03591768816113472\n",
      "step 4678 loss 0.03590548038482666\n",
      "step 4679 loss 0.03589329868555069\n",
      "step 4680 loss 0.03588112071156502\n",
      "step 4681 loss 0.03586895018815994\n",
      "step 4682 loss 0.035856835544109344\n",
      "step 4683 loss 0.035844702273607254\n",
      "step 4684 loss 0.03583259508013725\n",
      "step 4685 loss 0.035820502787828445\n",
      "step 4686 loss 0.03580842167139053\n",
      "step 4687 loss 0.03579634800553322\n",
      "step 4688 loss 0.03578430041670799\n",
      "step 4689 loss 0.03577226772904396\n",
      "step 4690 loss 0.03576025366783142\n",
      "step 4691 loss 0.03574826195836067\n",
      "step 4692 loss 0.035736266523599625\n",
      "step 4693 loss 0.03572427108883858\n",
      "step 4694 loss 0.03571230545639992\n",
      "step 4695 loss 0.03570035845041275\n",
      "step 4696 loss 0.03568843752145767\n",
      "step 4697 loss 0.03567655012011528\n",
      "step 4698 loss 0.0356646291911602\n",
      "step 4699 loss 0.03565274551510811\n",
      "step 4700 loss 0.03564087301492691\n",
      "step 4701 loss 0.03562901169061661\n",
      "step 4702 loss 0.03561718761920929\n",
      "step 4703 loss 0.03560535982251167\n",
      "step 4704 loss 0.03559354692697525\n",
      "step 4705 loss 0.03558176010847092\n",
      "step 4706 loss 0.03556997701525688\n",
      "step 4707 loss 0.03555822744965553\n",
      "step 4708 loss 0.035546425729990005\n",
      "step 4709 loss 0.035534728318452835\n",
      "step 4710 loss 0.03552298620343208\n",
      "step 4711 loss 0.03551128879189491\n",
      "step 4712 loss 0.035499583929777145\n",
      "step 4713 loss 0.03548792377114296\n",
      "step 4714 loss 0.03547624498605728\n",
      "step 4715 loss 0.035464607179164886\n",
      "step 4716 loss 0.035452958196401596\n",
      "step 4717 loss 0.03544135019183159\n",
      "step 4718 loss 0.03542975336313248\n",
      "step 4719 loss 0.03541814908385277\n",
      "step 4720 loss 0.03540658950805664\n",
      "step 4721 loss 0.035395022481679916\n",
      "step 4722 loss 0.03538348153233528\n",
      "step 4723 loss 0.035371918231248856\n",
      "step 4724 loss 0.03536041080951691\n",
      "step 4725 loss 0.03534891456365585\n",
      "step 4726 loss 0.035337429493665695\n",
      "step 4727 loss 0.035325951874256134\n",
      "step 4728 loss 0.03531450405716896\n",
      "step 4729 loss 0.03530304506421089\n",
      "step 4730 loss 0.035291608422994614\n",
      "step 4731 loss 0.03528017923235893\n",
      "step 4732 loss 0.03526879847049713\n",
      "step 4733 loss 0.035257428884506226\n",
      "step 4734 loss 0.03524603322148323\n",
      "step 4735 loss 0.03523467481136322\n",
      "step 4736 loss 0.0352233424782753\n",
      "step 4737 loss 0.03521198406815529\n",
      "step 4738 loss 0.035200685262680054\n",
      "step 4739 loss 0.03518939018249512\n",
      "step 4740 loss 0.03517810255289078\n",
      "step 4741 loss 0.03516683354973793\n",
      "step 4742 loss 0.03515556454658508\n",
      "step 4743 loss 0.03514431044459343\n",
      "step 4744 loss 0.03513307496905327\n",
      "step 4745 loss 0.0351218543946743\n",
      "step 4746 loss 0.03511064872145653\n",
      "step 4747 loss 0.035099469125270844\n",
      "step 4748 loss 0.03508828952908516\n",
      "step 4749 loss 0.03507714346051216\n",
      "step 4750 loss 0.03506599739193916\n",
      "step 4751 loss 0.03505485877394676\n",
      "step 4752 loss 0.035043731331825256\n",
      "step 4753 loss 0.03503262251615524\n",
      "step 4754 loss 0.035021547228097916\n",
      "step 4755 loss 0.0350104458630085\n",
      "step 4756 loss 0.03499940410256386\n",
      "step 4757 loss 0.03498834744095802\n",
      "step 4758 loss 0.03497730568051338\n",
      "step 4759 loss 0.03496626392006874\n",
      "step 4760 loss 0.03495529294013977\n",
      "step 4761 loss 0.03494427725672722\n",
      "step 4762 loss 0.03493328019976616\n",
      "step 4763 loss 0.03492233157157898\n",
      "step 4764 loss 0.03491136059165001\n",
      "step 4765 loss 0.034900449216365814\n",
      "step 4766 loss 0.034889522939920425\n",
      "step 4767 loss 0.03487860783934593\n",
      "step 4768 loss 0.03486771881580353\n",
      "step 4769 loss 0.03485681861639023\n",
      "step 4770 loss 0.034845948219299316\n",
      "step 4771 loss 0.0348350889980793\n",
      "step 4772 loss 0.03482424467802048\n",
      "step 4773 loss 0.03481340780854225\n",
      "step 4774 loss 0.03480261564254761\n",
      "step 4775 loss 0.03479178249835968\n",
      "step 4776 loss 0.03478102385997772\n",
      "step 4777 loss 0.034770261496305466\n",
      "step 4778 loss 0.034759510308504105\n",
      "step 4779 loss 0.03474874421954155\n",
      "step 4780 loss 0.0347379595041275\n",
      "step 4781 loss 0.0347270630300045\n",
      "step 4782 loss 0.034716177731752396\n",
      "step 4783 loss 0.034705325961112976\n",
      "step 4784 loss 0.03469441458582878\n",
      "step 4785 loss 0.03468357026576996\n",
      "step 4786 loss 0.03467268496751785\n",
      "step 4787 loss 0.03466179221868515\n",
      "step 4788 loss 0.03465091809630394\n",
      "step 4789 loss 0.034640077501535416\n",
      "step 4790 loss 0.0346292145550251\n",
      "step 4791 loss 0.03461838141083717\n",
      "step 4792 loss 0.03460754454135895\n",
      "step 4793 loss 0.034596726298332214\n",
      "step 4794 loss 0.03458589315414429\n",
      "step 4795 loss 0.03457508981227875\n",
      "step 4796 loss 0.034564293920993805\n",
      "step 4797 loss 0.03455352783203125\n",
      "step 4798 loss 0.03454272449016571\n",
      "step 4799 loss 0.03453196957707405\n",
      "step 4800 loss 0.0345211997628212\n",
      "step 4801 loss 0.03451045975089073\n",
      "step 4802 loss 0.03449973464012146\n",
      "step 4803 loss 0.034489020705223083\n",
      "step 4804 loss 0.034478310495615005\n",
      "step 4805 loss 0.03446762636303902\n",
      "step 4806 loss 0.03445693105459213\n",
      "step 4807 loss 0.03444626182317734\n",
      "step 4808 loss 0.03443561866879463\n",
      "step 4809 loss 0.03442494943737984\n",
      "step 4810 loss 0.03441433981060982\n",
      "step 4811 loss 0.0344037190079689\n",
      "step 4812 loss 0.03439312055706978\n",
      "step 4813 loss 0.03438252583146095\n",
      "step 4814 loss 0.03437192365527153\n",
      "step 4815 loss 0.034361377358436584\n",
      "step 4816 loss 0.03435077145695686\n",
      "step 4817 loss 0.03433997556567192\n",
      "step 4818 loss 0.03432914614677429\n",
      "step 4819 loss 0.03431833162903786\n",
      "step 4820 loss 0.03430749848484993\n",
      "step 4821 loss 0.034296635538339615\n",
      "step 4822 loss 0.03428582102060318\n",
      "step 4823 loss 0.03427496924996376\n",
      "step 4824 loss 0.03426412492990494\n",
      "step 4825 loss 0.034253284335136414\n",
      "step 4826 loss 0.03424243628978729\n",
      "step 4827 loss 0.03423161059617996\n",
      "step 4828 loss 0.03422076627612114\n",
      "step 4829 loss 0.03420993313193321\n",
      "step 4830 loss 0.03419913351535797\n",
      "step 4831 loss 0.03418829292058945\n",
      "step 4832 loss 0.0341775044798851\n",
      "step 4833 loss 0.03416672348976135\n",
      "step 4834 loss 0.03415592759847641\n",
      "step 4835 loss 0.03414514660835266\n",
      "step 4836 loss 0.0341343879699707\n",
      "step 4837 loss 0.03412364050745964\n",
      "step 4838 loss 0.03411289304494858\n",
      "step 4839 loss 0.034102171659469604\n",
      "step 4840 loss 0.03409142419695854\n",
      "step 4841 loss 0.03408074751496315\n",
      "step 4842 loss 0.03407004848122597\n",
      "step 4843 loss 0.03405938297510147\n",
      "step 4844 loss 0.034048739820718765\n",
      "step 4845 loss 0.034038037061691284\n",
      "step 4846 loss 0.034027401357889175\n",
      "step 4847 loss 0.03401677682995796\n",
      "step 4848 loss 0.03400616720318794\n",
      "step 4849 loss 0.03399556130170822\n",
      "step 4850 loss 0.03398497402667999\n",
      "step 4851 loss 0.03397440165281296\n",
      "step 4852 loss 0.033963821828365326\n",
      "step 4853 loss 0.03395329788327217\n",
      "step 4854 loss 0.03394276276230812\n",
      "step 4855 loss 0.03393224999308586\n",
      "step 4856 loss 0.03392172232270241\n",
      "step 4857 loss 0.033911239355802536\n",
      "step 4858 loss 0.03390076011419296\n",
      "step 4859 loss 0.03389028459787369\n",
      "step 4860 loss 0.0338798351585865\n",
      "step 4861 loss 0.033869415521621704\n",
      "step 4862 loss 0.03385898098349571\n",
      "step 4863 loss 0.03384856879711151\n",
      "step 4864 loss 0.03383819758892059\n",
      "step 4865 loss 0.033827800303697586\n",
      "step 4866 loss 0.03381744399666786\n",
      "step 4867 loss 0.03380706161260605\n",
      "step 4868 loss 0.03379674255847931\n",
      "step 4869 loss 0.03378641977906227\n",
      "step 4870 loss 0.03377610817551613\n",
      "step 4871 loss 0.03376580402255058\n",
      "step 4872 loss 0.033755525946617126\n",
      "step 4873 loss 0.03374525159597397\n",
      "step 4874 loss 0.033734988421201706\n",
      "step 4875 loss 0.03372476249933243\n",
      "step 4876 loss 0.03371452912688255\n",
      "step 4877 loss 0.033704325556755066\n",
      "step 4878 loss 0.033694133162498474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4879 loss 0.03368392586708069\n",
      "step 4880 loss 0.033673759549856186\n",
      "step 4881 loss 0.03366359323263168\n",
      "step 4882 loss 0.03365344554185867\n",
      "step 4883 loss 0.03364330157637596\n",
      "step 4884 loss 0.03363320231437683\n",
      "step 4885 loss 0.03362308815121651\n",
      "step 4886 loss 0.033612996339797974\n",
      "step 4887 loss 0.03360292315483093\n",
      "step 4888 loss 0.03359285742044449\n",
      "step 4889 loss 0.03358280286192894\n",
      "step 4890 loss 0.03357277065515518\n",
      "step 4891 loss 0.03356274217367172\n",
      "step 4892 loss 0.03355272114276886\n",
      "step 4893 loss 0.03354274109005928\n",
      "step 4894 loss 0.03353278711438179\n",
      "step 4895 loss 0.033522799611091614\n",
      "step 4896 loss 0.03351284936070442\n",
      "step 4897 loss 0.03350289911031723\n",
      "step 4898 loss 0.03349297121167183\n",
      "step 4899 loss 0.03348308056592941\n",
      "step 4900 loss 0.03347314894199371\n",
      "step 4901 loss 0.03346327319741249\n",
      "step 4902 loss 0.03345339372754097\n",
      "step 4903 loss 0.03344355523586273\n",
      "step 4904 loss 0.033433698117733\n",
      "step 4905 loss 0.03342387080192566\n",
      "step 4906 loss 0.03341405466198921\n",
      "step 4907 loss 0.03340424969792366\n",
      "step 4908 loss 0.03339444473385811\n",
      "step 4909 loss 0.033384669572114944\n",
      "step 4910 loss 0.03337489813566208\n",
      "step 4911 loss 0.03336513414978981\n",
      "step 4912 loss 0.03335539996623993\n",
      "step 4913 loss 0.033345695585012436\n",
      "step 4914 loss 0.03333596885204315\n",
      "step 4915 loss 0.03332626819610596\n",
      "step 4916 loss 0.033316560089588165\n",
      "step 4917 loss 0.03330688551068306\n",
      "step 4918 loss 0.03329722583293915\n",
      "step 4919 loss 0.03328755870461464\n",
      "step 4920 loss 0.033277932554483414\n",
      "step 4921 loss 0.03326832503080368\n",
      "step 4922 loss 0.03325870633125305\n",
      "step 4923 loss 0.03324911743402481\n",
      "step 4924 loss 0.03323952853679657\n",
      "step 4925 loss 0.03322996944189072\n",
      "step 4926 loss 0.033220380544662476\n",
      "step 4927 loss 0.033210840076208115\n",
      "step 4928 loss 0.03320131450891495\n",
      "step 4929 loss 0.03319181129336357\n",
      "step 4930 loss 0.0331822969019413\n",
      "step 4931 loss 0.03317278251051903\n",
      "step 4932 loss 0.03316330164670944\n",
      "step 4933 loss 0.03315384313464165\n",
      "step 4934 loss 0.03314441442489624\n",
      "step 4935 loss 0.03313495218753815\n",
      "step 4936 loss 0.03312553092837334\n",
      "step 4937 loss 0.03311608359217644\n",
      "step 4938 loss 0.03310669586062431\n",
      "step 4939 loss 0.03309730812907219\n",
      "step 4940 loss 0.03308791294693947\n",
      "step 4941 loss 0.03307855501770973\n",
      "step 4942 loss 0.03306920826435089\n",
      "step 4943 loss 0.033059850335121155\n",
      "step 4944 loss 0.033050537109375\n",
      "step 4945 loss 0.03304121643304825\n",
      "step 4946 loss 0.03303191065788269\n",
      "step 4947 loss 0.033022619783878326\n",
      "step 4948 loss 0.03301332890987396\n",
      "step 4949 loss 0.033004067838191986\n",
      "step 4950 loss 0.03299479931592941\n",
      "step 4951 loss 0.032985590398311615\n",
      "step 4952 loss 0.03297632932662964\n",
      "step 4953 loss 0.03296714276075363\n",
      "step 4954 loss 0.03295792266726494\n",
      "step 4955 loss 0.032948728650808334\n",
      "step 4956 loss 0.032939568161964417\n",
      "step 4957 loss 0.03293037414550781\n",
      "step 4958 loss 0.032921262085437775\n",
      "step 4959 loss 0.03291210159659386\n",
      "step 4960 loss 0.03290296718478203\n",
      "step 4961 loss 0.03289384767413139\n",
      "step 4962 loss 0.032884735614061356\n",
      "step 4963 loss 0.0328756608068943\n",
      "step 4964 loss 0.032866574823856354\n",
      "step 4965 loss 0.0328575000166893\n",
      "step 4966 loss 0.032848428934812546\n",
      "step 4967 loss 0.032839301973581314\n",
      "step 4968 loss 0.03283006325364113\n",
      "step 4969 loss 0.03282088786363602\n",
      "step 4970 loss 0.032811686396598816\n",
      "step 4971 loss 0.032802458852529526\n",
      "step 4972 loss 0.03279324620962143\n",
      "step 4973 loss 0.03278406336903572\n",
      "step 4974 loss 0.03277484327554703\n",
      "step 4975 loss 0.03276566416025162\n",
      "step 4976 loss 0.032756462693214417\n",
      "step 4977 loss 0.0327472910284996\n",
      "step 4978 loss 0.0327381007373333\n",
      "step 4979 loss 0.03272892162203789\n",
      "step 4980 loss 0.03271975368261337\n",
      "step 4981 loss 0.03271059691905975\n",
      "step 4982 loss 0.03270145505666733\n",
      "step 4983 loss 0.0326923169195652\n",
      "step 4984 loss 0.032683197408914566\n",
      "step 4985 loss 0.03267401084303856\n",
      "step 4986 loss 0.03266485035419464\n",
      "step 4987 loss 0.03265570104122162\n",
      "step 4988 loss 0.03264651447534561\n",
      "step 4989 loss 0.03263735771179199\n",
      "step 4990 loss 0.032628193497657776\n",
      "step 4991 loss 0.03261902928352356\n",
      "step 4992 loss 0.032609861344099045\n",
      "step 4993 loss 0.03260072320699692\n",
      "step 4994 loss 0.0325915552675724\n",
      "step 4995 loss 0.03258245438337326\n",
      "step 4996 loss 0.032573286443948746\n",
      "step 4997 loss 0.03256417438387871\n",
      "step 4998 loss 0.032555073499679565\n",
      "step 4999 loss 0.032545968890190125\n",
      "step 5000 loss 0.032536882907152176\n",
      "step 5001 loss 0.032527804374694824\n",
      "step 5002 loss 0.03251871466636658\n",
      "step 5003 loss 0.032509658485651016\n",
      "step 5004 loss 0.03250061348080635\n",
      "step 5005 loss 0.03249157965183258\n",
      "step 5006 loss 0.0324825681746006\n",
      "step 5007 loss 0.032473526895046234\n",
      "step 5008 loss 0.032464541494846344\n",
      "step 5009 loss 0.03245551884174347\n",
      "step 5010 loss 0.03244655206799507\n",
      "step 5011 loss 0.032437413930892944\n",
      "step 5012 loss 0.0324283093214035\n",
      "step 5013 loss 0.03241918236017227\n",
      "step 5014 loss 0.032410070300102234\n",
      "step 5015 loss 0.0324009470641613\n",
      "step 5016 loss 0.032391831278800964\n",
      "step 5017 loss 0.03238273039460182\n",
      "step 5018 loss 0.032373614609241486\n",
      "step 5019 loss 0.03236454352736473\n",
      "step 5020 loss 0.032355450093746185\n",
      "step 5021 loss 0.03234637901186943\n",
      "step 5022 loss 0.03233732283115387\n",
      "step 5023 loss 0.03232826292514801\n",
      "step 5024 loss 0.03231920301914215\n",
      "step 5025 loss 0.03231015428900719\n",
      "step 5026 loss 0.03230113908648491\n",
      "step 5027 loss 0.03229213133454323\n",
      "step 5028 loss 0.032283101230859756\n",
      "step 5029 loss 0.032274115830659866\n",
      "step 5030 loss 0.032265134155750275\n",
      "step 5031 loss 0.03225616365671158\n",
      "step 5032 loss 0.032247208058834076\n",
      "step 5033 loss 0.03223825618624687\n",
      "step 5034 loss 0.032229308038949966\n",
      "step 5035 loss 0.03222037851810455\n",
      "step 5036 loss 0.03221148997545242\n",
      "step 5037 loss 0.0322025902569294\n",
      "step 5038 loss 0.032193705439567566\n",
      "step 5039 loss 0.03218483552336693\n",
      "step 5040 loss 0.03217596560716629\n",
      "step 5041 loss 0.03216712549328804\n",
      "step 5042 loss 0.032158274203538895\n",
      "step 5043 loss 0.032149482518434525\n",
      "step 5044 loss 0.03214064985513687\n",
      "step 5045 loss 0.03213182091712952\n",
      "step 5046 loss 0.032123006880283356\n",
      "step 5047 loss 0.03211420029401779\n",
      "step 5048 loss 0.032105401158332825\n",
      "step 5049 loss 0.03209661319851875\n",
      "step 5050 loss 0.032087843865156174\n",
      "step 5051 loss 0.03207908198237419\n",
      "step 5052 loss 0.0320703350007534\n",
      "step 5053 loss 0.03206159919500351\n",
      "step 5054 loss 0.03205287829041481\n",
      "step 5055 loss 0.03204416111111641\n",
      "step 5056 loss 0.032035451382398605\n",
      "step 5057 loss 0.03202677145600319\n",
      "step 5058 loss 0.03201807290315628\n",
      "step 5059 loss 0.03200941160321236\n",
      "step 5060 loss 0.03200075402855873\n",
      "step 5061 loss 0.031992096453905106\n",
      "step 5062 loss 0.03198349103331566\n",
      "step 5063 loss 0.03197488933801651\n",
      "step 5064 loss 0.031966254115104675\n",
      "step 5065 loss 0.03195767477154732\n",
      "step 5066 loss 0.031949110329151154\n",
      "step 5067 loss 0.03194054216146469\n",
      "step 5068 loss 0.03193197771906853\n",
      "step 5069 loss 0.031923435628414154\n",
      "step 5070 loss 0.03191491216421127\n",
      "step 5071 loss 0.0319063775241375\n",
      "step 5072 loss 0.031897857785224915\n",
      "step 5073 loss 0.031889401376247406\n",
      "step 5074 loss 0.03188092261552811\n",
      "step 5075 loss 0.031872451305389404\n",
      "step 5076 loss 0.031863998621702194\n",
      "step 5077 loss 0.031855545938014984\n",
      "step 5078 loss 0.03184713050723076\n",
      "step 5079 loss 0.03183871880173683\n",
      "step 5080 loss 0.03183028846979141\n",
      "step 5081 loss 0.03182191774249077\n",
      "step 5082 loss 0.03181353956460953\n",
      "step 5083 loss 0.03180517256259918\n",
      "step 5084 loss 0.03179681673645973\n",
      "step 5085 loss 0.03178849071264267\n",
      "step 5086 loss 0.03178013488650322\n",
      "step 5087 loss 0.03177182748913765\n",
      "step 5088 loss 0.031763531267642975\n",
      "step 5089 loss 0.031755246222019196\n",
      "step 5090 loss 0.03174693137407303\n",
      "step 5091 loss 0.031738683581352234\n",
      "step 5092 loss 0.031730424612760544\n",
      "step 5093 loss 0.03172217309474945\n",
      "step 5094 loss 0.03171395882964134\n",
      "step 5095 loss 0.031705714762210846\n",
      "step 5096 loss 0.03169751912355423\n",
      "step 5097 loss 0.031689342111349106\n",
      "step 5098 loss 0.03168115392327309\n",
      "step 5099 loss 0.03167296573519707\n",
      "step 5100 loss 0.031664811074733734\n",
      "step 5101 loss 0.0316566601395607\n",
      "step 5102 loss 0.03164854645729065\n",
      "step 5103 loss 0.03164041042327881\n",
      "step 5104 loss 0.031632307916879654\n",
      "step 5105 loss 0.03162418678402901\n",
      "step 5106 loss 0.031616099178791046\n",
      "step 5107 loss 0.03160802274942398\n",
      "step 5108 loss 0.03159995749592781\n",
      "step 5109 loss 0.03159191459417343\n",
      "step 5110 loss 0.03158386051654816\n",
      "step 5111 loss 0.03157583624124527\n",
      "step 5112 loss 0.03156781941652298\n",
      "step 5113 loss 0.031559813767671585\n",
      "step 5114 loss 0.03155183792114258\n",
      "step 5115 loss 0.03154384344816208\n",
      "step 5116 loss 0.03153586760163307\n",
      "step 5117 loss 0.03152793273329735\n",
      "step 5118 loss 0.03151997923851013\n",
      "step 5119 loss 0.031512025743722916\n",
      "step 5120 loss 0.03150409832596779\n",
      "step 5121 loss 0.03149619698524475\n",
      "step 5122 loss 0.03148829564452171\n",
      "step 5123 loss 0.03148040920495987\n",
      "step 5124 loss 0.03147248923778534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5125 loss 0.0314643532037735\n",
      "step 5126 loss 0.03145623579621315\n",
      "step 5127 loss 0.03144807368516922\n",
      "step 5128 loss 0.03143991529941559\n",
      "step 5129 loss 0.03143177181482315\n",
      "step 5130 loss 0.03142361342906952\n",
      "step 5131 loss 0.0314154326915741\n",
      "step 5132 loss 0.031407274305820465\n",
      "step 5133 loss 0.031399112194776535\n",
      "step 5134 loss 0.0313909538090229\n",
      "step 5135 loss 0.03138280287384987\n",
      "step 5136 loss 0.03137463331222534\n",
      "step 5137 loss 0.031366489827632904\n",
      "step 5138 loss 0.031358346343040466\n",
      "step 5139 loss 0.031350210309028625\n",
      "step 5140 loss 0.03134206682443619\n",
      "step 5141 loss 0.03133394569158554\n",
      "step 5142 loss 0.03132582828402519\n",
      "step 5143 loss 0.031317707151174545\n",
      "step 5144 loss 0.03130960091948509\n",
      "step 5145 loss 0.03130153566598892\n",
      "step 5146 loss 0.03129344433546066\n",
      "step 5147 loss 0.0312853679060936\n",
      "step 5148 loss 0.031277310103178024\n",
      "step 5149 loss 0.031269241124391556\n",
      "step 5150 loss 0.03126120567321777\n",
      "step 5151 loss 0.0312531478703022\n",
      "step 5152 loss 0.031245140358805656\n",
      "step 5153 loss 0.031237132847309113\n",
      "step 5154 loss 0.031229129061102867\n",
      "step 5155 loss 0.03122112713754177\n",
      "step 5156 loss 0.031213155016303062\n",
      "step 5157 loss 0.031205182895064354\n",
      "step 5158 loss 0.03119722381234169\n",
      "step 5159 loss 0.031189285218715668\n",
      "step 5160 loss 0.031181320548057556\n",
      "step 5161 loss 0.031173400580883026\n",
      "step 5162 loss 0.03116549551486969\n",
      "step 5163 loss 0.031157614663243294\n",
      "step 5164 loss 0.031149696558713913\n",
      "step 5165 loss 0.031141825020313263\n",
      "step 5166 loss 0.03113395906984806\n",
      "step 5167 loss 0.03112608939409256\n",
      "step 5168 loss 0.031118230894207954\n",
      "step 5169 loss 0.031110424548387527\n",
      "step 5170 loss 0.03110259585082531\n",
      "step 5171 loss 0.03109477460384369\n",
      "step 5172 loss 0.031086964532732964\n",
      "step 5173 loss 0.031079178676009178\n",
      "step 5174 loss 0.031071390956640244\n",
      "step 5175 loss 0.031063614413142204\n",
      "step 5176 loss 0.031055882573127747\n",
      "step 5177 loss 0.031048141419887543\n",
      "step 5178 loss 0.031040402129292488\n",
      "step 5179 loss 0.031032681465148926\n",
      "step 5180 loss 0.031024958938360214\n",
      "step 5181 loss 0.03101726621389389\n",
      "step 5182 loss 0.031009580940008163\n",
      "step 5183 loss 0.031001925468444824\n",
      "step 5184 loss 0.03099423460662365\n",
      "step 5185 loss 0.030986590310931206\n",
      "step 5186 loss 0.03097894974052906\n",
      "step 5187 loss 0.030971305444836617\n",
      "step 5188 loss 0.030963702127337456\n",
      "step 5189 loss 0.030956095084547997\n",
      "step 5190 loss 0.03094848245382309\n",
      "step 5191 loss 0.030940908938646317\n",
      "step 5192 loss 0.0309333186596632\n",
      "step 5193 loss 0.030925758183002472\n",
      "step 5194 loss 0.030918188393115997\n",
      "step 5195 loss 0.030910659581422806\n",
      "step 5196 loss 0.03090314380824566\n",
      "step 5197 loss 0.030895596370100975\n",
      "step 5198 loss 0.030888095498085022\n",
      "step 5199 loss 0.030880581587553024\n",
      "step 5200 loss 0.03087311051785946\n",
      "step 5201 loss 0.0308656208217144\n",
      "step 5202 loss 0.030858183279633522\n",
      "step 5203 loss 0.030850717797875404\n",
      "step 5204 loss 0.030843280255794525\n",
      "step 5205 loss 0.030835840851068497\n",
      "step 5206 loss 0.03082842193543911\n",
      "step 5207 loss 0.030821017920970917\n",
      "step 5208 loss 0.03081360086798668\n",
      "step 5209 loss 0.030806217342615128\n",
      "step 5210 loss 0.030798830091953278\n",
      "step 5211 loss 0.030791476368904114\n",
      "step 5212 loss 0.03078412264585495\n",
      "step 5213 loss 0.03077678754925728\n",
      "step 5214 loss 0.030769435688853264\n",
      "step 5215 loss 0.030762100592255592\n",
      "step 5216 loss 0.03075481206178665\n",
      "step 5217 loss 0.03074749931693077\n",
      "step 5218 loss 0.03074021264910698\n",
      "step 5219 loss 0.030732912942767143\n",
      "step 5220 loss 0.030725650489330292\n",
      "step 5221 loss 0.030718374997377396\n",
      "step 5222 loss 0.03071112185716629\n",
      "step 5223 loss 0.030703887343406677\n",
      "step 5224 loss 0.030696658417582512\n",
      "step 5225 loss 0.030689453706145287\n",
      "step 5226 loss 0.030682239681482315\n",
      "step 5227 loss 0.03067503497004509\n",
      "step 5228 loss 0.030667848885059357\n",
      "step 5229 loss 0.030660677701234818\n",
      "step 5230 loss 0.030653517693281174\n",
      "step 5231 loss 0.030646324157714844\n",
      "step 5232 loss 0.030639195814728737\n",
      "step 5233 loss 0.03063206747174263\n",
      "step 5234 loss 0.03062494285404682\n",
      "step 5235 loss 0.030617818236351013\n",
      "step 5236 loss 0.030610723420977592\n",
      "step 5237 loss 0.030603613704442978\n",
      "step 5238 loss 0.030596531927585602\n",
      "step 5239 loss 0.030589452013373375\n",
      "step 5240 loss 0.030582351610064507\n",
      "step 5241 loss 0.030575208365917206\n",
      "step 5242 loss 0.030568061396479607\n",
      "step 5243 loss 0.03056090511381626\n",
      "step 5244 loss 0.030553780496120453\n",
      "step 5245 loss 0.0305466391146183\n",
      "step 5246 loss 0.030539488419890404\n",
      "step 5247 loss 0.030532388016581535\n",
      "step 5248 loss 0.03052525594830513\n",
      "step 5249 loss 0.030518146231770515\n",
      "step 5250 loss 0.030511021614074707\n",
      "step 5251 loss 0.030503930523991585\n",
      "step 5252 loss 0.03049684688448906\n",
      "step 5253 loss 0.030489735305309296\n",
      "step 5254 loss 0.030482668429613113\n",
      "step 5255 loss 0.030475588515400887\n",
      "step 5256 loss 0.030468551442027092\n",
      "step 5257 loss 0.030461475253105164\n",
      "step 5258 loss 0.030454445630311966\n",
      "step 5259 loss 0.03044741228222847\n",
      "step 5260 loss 0.03044036589562893\n",
      "step 5261 loss 0.030433379113674164\n",
      "step 5262 loss 0.030426358804106712\n",
      "step 5263 loss 0.030419357120990753\n",
      "step 5264 loss 0.030412377789616585\n",
      "step 5265 loss 0.030405402183532715\n",
      "step 5266 loss 0.0303984172642231\n",
      "step 5267 loss 0.03039146400988102\n",
      "step 5268 loss 0.03038450889289379\n",
      "step 5269 loss 0.030377577990293503\n",
      "step 5270 loss 0.030370647087693214\n",
      "step 5271 loss 0.030363745987415314\n",
      "step 5272 loss 0.030356813222169876\n",
      "step 5273 loss 0.030349917709827423\n",
      "step 5274 loss 0.030343031510710716\n",
      "step 5275 loss 0.03033614158630371\n",
      "step 5276 loss 0.030329275876283646\n",
      "step 5277 loss 0.030322421342134476\n",
      "step 5278 loss 0.030315563082695007\n",
      "step 5279 loss 0.030308740213513374\n",
      "step 5280 loss 0.030301883816719055\n",
      "step 5281 loss 0.030295075848698616\n",
      "step 5282 loss 0.030288277193903923\n",
      "step 5283 loss 0.03028145805001259\n",
      "step 5284 loss 0.030274655669927597\n",
      "step 5285 loss 0.030267871916294098\n",
      "step 5286 loss 0.03026111051440239\n",
      "step 5287 loss 0.030254339799284935\n",
      "step 5288 loss 0.030247611925005913\n",
      "step 5289 loss 0.030240867286920547\n",
      "step 5290 loss 0.030234133824706078\n",
      "step 5291 loss 0.03022739849984646\n",
      "step 5292 loss 0.03022068925201893\n",
      "step 5293 loss 0.030213985592126846\n",
      "step 5294 loss 0.030207276344299316\n",
      "step 5295 loss 0.030200600624084473\n",
      "step 5296 loss 0.030193930491805077\n",
      "step 5297 loss 0.03018726408481598\n",
      "step 5298 loss 0.03018062561750412\n",
      "step 5299 loss 0.030173959210515022\n",
      "step 5300 loss 0.030167317017912865\n",
      "step 5301 loss 0.0301606934517622\n",
      "step 5302 loss 0.03015408292412758\n",
      "step 5303 loss 0.030147481709718704\n",
      "step 5304 loss 0.03014088235795498\n",
      "step 5305 loss 0.030134301632642746\n",
      "step 5306 loss 0.03012770600616932\n",
      "step 5307 loss 0.030121149495244026\n",
      "step 5308 loss 0.03011457249522209\n",
      "step 5309 loss 0.030108045786619186\n",
      "step 5310 loss 0.030101504176855087\n",
      "step 5311 loss 0.03009497933089733\n",
      "step 5312 loss 0.03008843958377838\n",
      "step 5313 loss 0.03008192591369152\n",
      "step 5314 loss 0.030075430870056152\n",
      "step 5315 loss 0.03006894141435623\n",
      "step 5316 loss 0.03006245382130146\n",
      "step 5317 loss 0.03005598485469818\n",
      "step 5318 loss 0.030049504712224007\n",
      "step 5319 loss 0.030043046921491623\n",
      "step 5320 loss 0.030036626383662224\n",
      "step 5321 loss 0.030030157417058945\n",
      "step 5322 loss 0.030023746192455292\n",
      "step 5323 loss 0.030017342418432236\n",
      "step 5324 loss 0.030010920017957687\n",
      "step 5325 loss 0.030004525557160378\n",
      "step 5326 loss 0.02999814972281456\n",
      "step 5327 loss 0.0299917533993721\n",
      "step 5328 loss 0.02998538501560688\n",
      "step 5329 loss 0.029979009181261063\n",
      "step 5330 loss 0.029972657561302185\n",
      "step 5331 loss 0.029966318979859352\n",
      "step 5332 loss 0.02995997853577137\n",
      "step 5333 loss 0.02995365299284458\n",
      "step 5334 loss 0.029947342351078987\n",
      "step 5335 loss 0.029941033571958542\n",
      "step 5336 loss 0.029934730380773544\n",
      "step 5337 loss 0.029928455129265785\n",
      "step 5338 loss 0.02992217056453228\n",
      "step 5339 loss 0.02991590090095997\n",
      "step 5340 loss 0.02990962564945221\n",
      "step 5341 loss 0.029903393238782883\n",
      "step 5342 loss 0.02989712730050087\n",
      "step 5343 loss 0.02989090234041214\n",
      "step 5344 loss 0.029884690418839455\n",
      "step 5345 loss 0.02987847290933132\n",
      "step 5346 loss 0.02987224981188774\n",
      "step 5347 loss 0.029866041615605354\n",
      "step 5348 loss 0.02985985204577446\n",
      "step 5349 loss 0.02985367737710476\n",
      "step 5350 loss 0.02984750270843506\n",
      "step 5351 loss 0.029841348528862\n",
      "step 5352 loss 0.029835186898708344\n",
      "step 5353 loss 0.029829036444425583\n",
      "step 5354 loss 0.029822902753949165\n",
      "step 5355 loss 0.02981676161289215\n",
      "step 5356 loss 0.02981063909828663\n",
      "step 5357 loss 0.029804520308971405\n",
      "step 5358 loss 0.029798423871397972\n",
      "step 5359 loss 0.02979235351085663\n",
      "step 5360 loss 0.029786257073283195\n",
      "step 5361 loss 0.029780181124806404\n",
      "step 5362 loss 0.029774103313684464\n",
      "step 5363 loss 0.029768047854304314\n",
      "step 5364 loss 0.029761988669633865\n",
      "step 5365 loss 0.029755964875221252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5366 loss 0.029749931767582893\n",
      "step 5367 loss 0.029743898659944534\n",
      "step 5368 loss 0.02973787859082222\n",
      "step 5369 loss 0.0297318696975708\n",
      "step 5370 loss 0.02972586639225483\n",
      "step 5371 loss 0.02971988171339035\n",
      "step 5372 loss 0.029713893309235573\n",
      "step 5373 loss 0.029707923531532288\n",
      "step 5374 loss 0.02970195561647415\n",
      "step 5375 loss 0.02969600260257721\n",
      "step 5376 loss 0.029690051451325417\n",
      "step 5377 loss 0.029684115201234818\n",
      "step 5378 loss 0.029678182676434517\n",
      "step 5379 loss 0.029672250151634216\n",
      "step 5380 loss 0.02966632880270481\n",
      "step 5381 loss 0.02966042049229145\n",
      "step 5382 loss 0.029654527083039284\n",
      "step 5383 loss 0.029648635536432266\n",
      "step 5384 loss 0.029642755165696144\n",
      "step 5385 loss 0.029636887833476067\n",
      "step 5386 loss 0.029631027951836586\n",
      "step 5387 loss 0.02962515875697136\n",
      "step 5388 loss 0.029619311913847923\n",
      "step 5389 loss 0.02961348369717598\n",
      "step 5390 loss 0.02960764244198799\n",
      "step 5391 loss 0.02960183657705784\n",
      "step 5392 loss 0.02959601581096649\n",
      "step 5393 loss 0.02959020435810089\n",
      "step 5394 loss 0.029584413394331932\n",
      "step 5395 loss 0.029578611254692078\n",
      "step 5396 loss 0.02957283891737461\n",
      "step 5397 loss 0.029567070305347443\n",
      "step 5398 loss 0.029561296105384827\n",
      "step 5399 loss 0.02955554984509945\n",
      "step 5400 loss 0.029549788683652878\n",
      "step 5401 loss 0.02954404428601265\n",
      "step 5402 loss 0.029538316652178764\n",
      "step 5403 loss 0.029532594606280327\n",
      "step 5404 loss 0.029526878148317337\n",
      "step 5405 loss 0.029521169140934944\n",
      "step 5406 loss 0.02951546385884285\n",
      "step 5407 loss 0.02950977347791195\n",
      "step 5408 loss 0.029504094272851944\n",
      "step 5409 loss 0.02949841320514679\n",
      "step 5410 loss 0.02949262596666813\n",
      "step 5411 loss 0.029486794024705887\n",
      "step 5412 loss 0.02948092482984066\n",
      "step 5413 loss 0.029475068673491478\n",
      "step 5414 loss 0.029469208791851997\n",
      "step 5415 loss 0.029463324695825577\n",
      "step 5416 loss 0.029457448050379753\n",
      "step 5417 loss 0.029451554641127586\n",
      "step 5418 loss 0.029445696622133255\n",
      "step 5419 loss 0.02943982370197773\n",
      "step 5420 loss 0.029433928430080414\n",
      "step 5421 loss 0.029428057372570038\n",
      "step 5422 loss 0.029422182589769363\n",
      "step 5423 loss 0.029416339471936226\n",
      "step 5424 loss 0.02941046468913555\n",
      "step 5425 loss 0.029404621571302414\n",
      "step 5426 loss 0.029398761689662933\n",
      "step 5427 loss 0.029392898082733154\n",
      "step 5428 loss 0.029387034475803375\n",
      "step 5429 loss 0.029381221160292625\n",
      "step 5430 loss 0.029375381767749786\n",
      "step 5431 loss 0.029369542375206947\n",
      "step 5432 loss 0.029363742098212242\n",
      "step 5433 loss 0.02935793250799179\n",
      "step 5434 loss 0.02935211919248104\n",
      "step 5435 loss 0.029346313327550888\n",
      "step 5436 loss 0.029340514913201332\n",
      "step 5437 loss 0.029334750026464462\n",
      "step 5438 loss 0.0293289665132761\n",
      "step 5439 loss 0.029323196038603783\n",
      "step 5440 loss 0.029317433014512062\n",
      "step 5441 loss 0.02931167371571064\n",
      "step 5442 loss 0.029305927455425262\n",
      "step 5443 loss 0.029300181195139885\n",
      "step 5444 loss 0.029294447973370552\n",
      "step 5445 loss 0.02928873337805271\n",
      "step 5446 loss 0.029283013194799423\n",
      "step 5447 loss 0.02927730791270733\n",
      "step 5448 loss 0.029271593317389488\n",
      "step 5449 loss 0.02926592342555523\n",
      "step 5450 loss 0.029260240495204926\n",
      "step 5451 loss 0.02925456315279007\n",
      "step 5452 loss 0.02924889326095581\n",
      "step 5453 loss 0.02924323081970215\n",
      "step 5454 loss 0.029237575829029083\n",
      "step 5455 loss 0.029231930151581764\n",
      "step 5456 loss 0.029226303100585938\n",
      "step 5457 loss 0.02922068163752556\n",
      "step 5458 loss 0.02921506017446518\n",
      "step 5459 loss 0.02920944057404995\n",
      "step 5460 loss 0.029203834012150764\n",
      "step 5461 loss 0.02919824607670307\n",
      "step 5462 loss 0.029192643240094185\n",
      "step 5463 loss 0.029187075793743134\n",
      "step 5464 loss 0.029181499034166336\n",
      "step 5465 loss 0.029175955802202225\n",
      "step 5466 loss 0.029170379042625427\n",
      "step 5467 loss 0.029164833948016167\n",
      "step 5468 loss 0.029159309342503548\n",
      "step 5469 loss 0.029153762385249138\n",
      "step 5470 loss 0.029148254543542862\n",
      "step 5471 loss 0.029142742976546288\n",
      "step 5472 loss 0.029137229546904564\n",
      "step 5473 loss 0.029131736606359482\n",
      "step 5474 loss 0.029126234352588654\n",
      "step 5475 loss 0.02912076562643051\n",
      "step 5476 loss 0.029115281999111176\n",
      "step 5477 loss 0.029109816998243332\n",
      "step 5478 loss 0.029104363173246384\n",
      "step 5479 loss 0.029098905622959137\n",
      "step 5480 loss 0.029093462973833084\n",
      "step 5481 loss 0.029088038951158524\n",
      "step 5482 loss 0.029082613065838814\n",
      "step 5483 loss 0.02907719649374485\n",
      "step 5484 loss 0.029071779921650887\n",
      "step 5485 loss 0.029066376388072968\n",
      "step 5486 loss 0.02906099520623684\n",
      "step 5487 loss 0.029055576771497726\n",
      "step 5488 loss 0.029050197452306747\n",
      "step 5489 loss 0.029044823721051216\n",
      "step 5490 loss 0.02903946489095688\n",
      "step 5491 loss 0.029034096747636795\n",
      "step 5492 loss 0.0290287546813488\n",
      "step 5493 loss 0.02902340702712536\n",
      "step 5494 loss 0.029018079861998558\n",
      "step 5495 loss 0.02901275083422661\n",
      "step 5496 loss 0.029007449746131897\n",
      "step 5497 loss 0.029002118855714798\n",
      "step 5498 loss 0.02899683266878128\n",
      "step 5499 loss 0.02899153158068657\n",
      "step 5500 loss 0.028986243531107903\n",
      "step 5501 loss 0.028980964794754982\n",
      "step 5502 loss 0.028975697234272957\n",
      "step 5503 loss 0.02897043153643608\n",
      "step 5504 loss 0.028965169563889503\n",
      "step 5505 loss 0.028959937393665314\n",
      "step 5506 loss 0.02895466983318329\n",
      "step 5507 loss 0.02894946187734604\n",
      "step 5508 loss 0.028944196179509163\n",
      "step 5509 loss 0.028939006850123405\n",
      "step 5510 loss 0.028933804482221603\n",
      "step 5511 loss 0.028928589075803757\n",
      "step 5512 loss 0.02892340160906315\n",
      "step 5513 loss 0.028918199241161346\n",
      "step 5514 loss 0.028913021087646484\n",
      "step 5515 loss 0.028907842934131622\n",
      "step 5516 loss 0.02890261635184288\n",
      "step 5517 loss 0.028897209092974663\n",
      "step 5518 loss 0.028891801834106445\n",
      "step 5519 loss 0.028886379674077034\n",
      "step 5520 loss 0.028880935162305832\n",
      "step 5521 loss 0.028875479474663734\n",
      "step 5522 loss 0.028870046138763428\n",
      "step 5523 loss 0.02886456437408924\n",
      "step 5524 loss 0.028859132900834084\n",
      "step 5525 loss 0.02885369211435318\n",
      "step 5526 loss 0.028848204761743546\n",
      "step 5527 loss 0.028842730447649956\n",
      "step 5528 loss 0.028837282210588455\n",
      "step 5529 loss 0.02883182466030121\n",
      "step 5530 loss 0.02882635034620762\n",
      "step 5531 loss 0.028820907697081566\n",
      "step 5532 loss 0.02881545014679432\n",
      "step 5533 loss 0.02881000004708767\n",
      "step 5534 loss 0.028804564848542213\n",
      "step 5535 loss 0.02879909798502922\n",
      "step 5536 loss 0.028793666511774063\n",
      "step 5537 loss 0.028788231313228607\n",
      "step 5538 loss 0.02878282032907009\n",
      "step 5539 loss 0.028777392581105232\n",
      "step 5540 loss 0.028771990910172462\n",
      "step 5541 loss 0.02876659668982029\n",
      "step 5542 loss 0.028761185705661774\n",
      "step 5543 loss 0.02875576727092266\n",
      "step 5544 loss 0.02875039540231228\n",
      "step 5545 loss 0.028745010495185852\n",
      "step 5546 loss 0.028739649802446365\n",
      "step 5547 loss 0.028734276071190834\n",
      "step 5548 loss 0.02872890792787075\n",
      "step 5549 loss 0.028723571449518204\n",
      "step 5550 loss 0.028718214482069016\n",
      "step 5551 loss 0.028712870553135872\n",
      "step 5552 loss 0.028707552701234818\n",
      "step 5553 loss 0.028702234849333763\n",
      "step 5554 loss 0.028696902096271515\n",
      "step 5555 loss 0.0286916084587574\n",
      "step 5556 loss 0.028686312958598137\n",
      "step 5557 loss 0.028681015595793724\n",
      "step 5558 loss 0.028675733134150505\n",
      "step 5559 loss 0.028670473024249077\n",
      "step 5560 loss 0.02866518497467041\n",
      "step 5561 loss 0.028659939765930176\n",
      "step 5562 loss 0.0286546777933836\n",
      "step 5563 loss 0.028649425134062767\n",
      "step 5564 loss 0.028644206002354622\n",
      "step 5565 loss 0.028638966381549835\n",
      "step 5566 loss 0.028633737936615944\n",
      "step 5567 loss 0.028628520667552948\n",
      "step 5568 loss 0.028623348101973534\n",
      "step 5569 loss 0.028618108481168747\n",
      "step 5570 loss 0.02861294336616993\n",
      "step 5571 loss 0.02860776148736477\n",
      "step 5572 loss 0.02860257215797901\n",
      "step 5573 loss 0.028597392141819\n",
      "step 5574 loss 0.02859225682914257\n",
      "step 5575 loss 0.028587087988853455\n",
      "step 5576 loss 0.028581950813531876\n",
      "step 5577 loss 0.02857682667672634\n",
      "step 5578 loss 0.028571680188179016\n",
      "step 5579 loss 0.028566556051373482\n",
      "step 5580 loss 0.028561454266309738\n",
      "step 5581 loss 0.028556331992149353\n",
      "step 5582 loss 0.028551248833537102\n",
      "step 5583 loss 0.028546154499053955\n",
      "step 5584 loss 0.028541073203086853\n",
      "step 5585 loss 0.028535988181829453\n",
      "step 5586 loss 0.028530923649668694\n",
      "step 5587 loss 0.02852586656808853\n",
      "step 5588 loss 0.02852080762386322\n",
      "step 5589 loss 0.028515754267573357\n",
      "step 5590 loss 0.028510717675089836\n",
      "step 5591 loss 0.028505705296993256\n",
      "step 5592 loss 0.028500661253929138\n",
      "step 5593 loss 0.028495660051703453\n",
      "step 5594 loss 0.028490647673606873\n",
      "step 5595 loss 0.028485657647252083\n",
      "step 5596 loss 0.02848064713180065\n",
      "step 5597 loss 0.028475677594542503\n",
      "step 5598 loss 0.02847069315612316\n",
      "step 5599 loss 0.028465719893574715\n",
      "step 5600 loss 0.028460772708058357\n",
      "step 5601 loss 0.02845580503344536\n",
      "step 5602 loss 0.02845086343586445\n",
      "step 5603 loss 0.028445906937122345\n",
      "step 5604 loss 0.028440970927476883\n",
      "step 5605 loss 0.028436049818992615\n",
      "step 5606 loss 0.0284311231225729\n",
      "step 5607 loss 0.02842622436583042\n",
      "step 5608 loss 0.02842133119702339\n",
      "step 5609 loss 0.02841641940176487\n",
      "step 5610 loss 0.02841152623295784\n",
      "step 5611 loss 0.028406653553247452\n",
      "step 5612 loss 0.028401780873537064\n",
      "step 5613 loss 0.02839692123234272\n",
      "step 5614 loss 0.028392039239406586\n",
      "step 5615 loss 0.028387179598212242\n",
      "step 5616 loss 0.02838234417140484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5617 loss 0.02837749943137169\n",
      "step 5618 loss 0.028372669592499733\n",
      "step 5619 loss 0.028367843478918076\n",
      "step 5620 loss 0.028363026678562164\n",
      "step 5621 loss 0.028358198702335358\n",
      "step 5622 loss 0.028353409841656685\n",
      "step 5623 loss 0.02834862656891346\n",
      "step 5624 loss 0.028343824669718742\n",
      "step 5625 loss 0.02833903767168522\n",
      "step 5626 loss 0.028334269300103188\n",
      "step 5627 loss 0.028329508379101753\n",
      "step 5628 loss 0.02832474745810032\n",
      "step 5629 loss 0.02831997722387314\n",
      "step 5630 loss 0.02831525355577469\n",
      "step 5631 loss 0.02831050008535385\n",
      "step 5632 loss 0.028305763378739357\n",
      "step 5633 loss 0.028301037847995758\n",
      "step 5634 loss 0.0282963328063488\n",
      "step 5635 loss 0.028291599825024605\n",
      "step 5636 loss 0.028286907821893692\n",
      "step 5637 loss 0.028282208368182182\n",
      "step 5638 loss 0.02827751450240612\n",
      "step 5639 loss 0.028272822499275208\n",
      "step 5640 loss 0.028268150985240936\n",
      "step 5641 loss 0.028263485059142113\n",
      "step 5642 loss 0.02825881913304329\n",
      "step 5643 loss 0.028254160657525063\n",
      "step 5644 loss 0.028249507769942284\n",
      "step 5645 loss 0.0282448623329401\n",
      "step 5646 loss 0.028240226209163666\n",
      "step 5647 loss 0.02823559194803238\n",
      "step 5648 loss 0.028230972588062286\n",
      "step 5649 loss 0.028226351365447044\n",
      "step 5650 loss 0.028221741318702698\n",
      "step 5651 loss 0.028217148035764694\n",
      "step 5652 loss 0.028212549164891243\n",
      "step 5653 loss 0.02820795588195324\n",
      "step 5654 loss 0.028203368186950684\n",
      "step 5655 loss 0.028198784217238426\n",
      "step 5656 loss 0.028194215148687363\n",
      "step 5657 loss 0.028189649805426598\n",
      "step 5658 loss 0.028185086324810982\n",
      "step 5659 loss 0.0281805582344532\n",
      "step 5660 loss 0.02817600779235363\n",
      "step 5661 loss 0.028171472251415253\n",
      "step 5662 loss 0.02816692739725113\n",
      "step 5663 loss 0.028162408620119095\n",
      "step 5664 loss 0.02815789170563221\n",
      "step 5665 loss 0.02815338410437107\n",
      "step 5666 loss 0.028148889541625977\n",
      "step 5667 loss 0.028144387528300285\n",
      "step 5668 loss 0.028139891102910042\n",
      "step 5669 loss 0.02813541889190674\n",
      "step 5670 loss 0.02813093364238739\n",
      "step 5671 loss 0.02812645584344864\n",
      "step 5672 loss 0.02812199667096138\n",
      "step 5673 loss 0.028117544949054718\n",
      "step 5674 loss 0.028113093227148056\n",
      "step 5675 loss 0.028108647093176842\n",
      "step 5676 loss 0.028104213997721672\n",
      "step 5677 loss 0.028099780902266502\n",
      "step 5678 loss 0.028095349669456482\n",
      "step 5679 loss 0.02809092029929161\n",
      "step 5680 loss 0.02808651141822338\n",
      "step 5681 loss 0.028082117438316345\n",
      "step 5682 loss 0.02807769924402237\n",
      "step 5683 loss 0.028073305264115334\n",
      "step 5684 loss 0.028068922460079193\n",
      "step 5685 loss 0.028064507991075516\n",
      "step 5686 loss 0.028059953823685646\n",
      "step 5687 loss 0.028055420145392418\n",
      "step 5688 loss 0.028050865978002548\n",
      "step 5689 loss 0.028046319261193275\n",
      "step 5690 loss 0.02804175391793251\n",
      "step 5691 loss 0.028037190437316895\n",
      "step 5692 loss 0.028032628819346428\n",
      "step 5693 loss 0.02802807278931141\n",
      "step 5694 loss 0.02802351862192154\n",
      "step 5695 loss 0.02801894210278988\n",
      "step 5696 loss 0.02801436372101307\n",
      "step 5697 loss 0.028009843081235886\n",
      "step 5698 loss 0.02800527960062027\n",
      "step 5699 loss 0.028000712394714355\n",
      "step 5700 loss 0.027996163815259933\n",
      "step 5701 loss 0.027991633862257004\n",
      "step 5702 loss 0.027987079694867134\n",
      "step 5703 loss 0.02798253484070301\n",
      "step 5704 loss 0.027978001162409782\n",
      "step 5705 loss 0.027973482385277748\n",
      "step 5706 loss 0.027968954294919968\n",
      "step 5707 loss 0.027964435517787933\n",
      "step 5708 loss 0.02795991487801075\n",
      "step 5709 loss 0.02795540727674961\n",
      "step 5710 loss 0.02795090526342392\n",
      "step 5711 loss 0.027946418151259422\n",
      "step 5712 loss 0.027941908687353134\n",
      "step 5713 loss 0.027937421575188637\n",
      "step 5714 loss 0.027932940050959587\n",
      "step 5715 loss 0.027928443625569344\n",
      "step 5716 loss 0.027923978865146637\n",
      "step 5717 loss 0.027919521555304527\n",
      "step 5718 loss 0.027915069833397865\n",
      "step 5719 loss 0.027910616248846054\n",
      "step 5720 loss 0.02790617011487484\n",
      "step 5721 loss 0.02790173515677452\n",
      "step 5722 loss 0.027897300198674202\n",
      "step 5723 loss 0.027892880141735077\n",
      "step 5724 loss 0.027888452634215355\n",
      "step 5725 loss 0.027884040027856827\n",
      "step 5726 loss 0.0278796199709177\n",
      "step 5727 loss 0.027875246480107307\n",
      "step 5728 loss 0.027870850637555122\n",
      "step 5729 loss 0.027866441756486893\n",
      "step 5730 loss 0.027862070128321648\n",
      "step 5731 loss 0.027857687324285507\n",
      "step 5732 loss 0.02785331942141056\n",
      "step 5733 loss 0.02784895710647106\n",
      "step 5734 loss 0.02784460037946701\n",
      "step 5735 loss 0.027840247377753258\n",
      "step 5736 loss 0.02783590741455555\n",
      "step 5737 loss 0.02783156931400299\n",
      "step 5738 loss 0.027827244251966476\n",
      "step 5739 loss 0.027822919189929962\n",
      "step 5740 loss 0.027818597853183746\n",
      "step 5741 loss 0.02781428024172783\n",
      "step 5742 loss 0.027809971943497658\n",
      "step 5743 loss 0.02780567668378353\n",
      "step 5744 loss 0.02780137024819851\n",
      "step 5745 loss 0.027797093614935875\n",
      "step 5746 loss 0.027792811393737793\n",
      "step 5747 loss 0.027788547798991203\n",
      "step 5748 loss 0.027784261852502823\n",
      "step 5749 loss 0.02778000198304653\n",
      "step 5750 loss 0.02777574583888054\n",
      "step 5751 loss 0.027771510183811188\n",
      "step 5752 loss 0.02776721492409706\n",
      "step 5753 loss 0.027762873098254204\n",
      "step 5754 loss 0.027758512645959854\n",
      "step 5755 loss 0.027754120528697968\n",
      "step 5756 loss 0.027749700471758842\n",
      "step 5757 loss 0.027745278552174568\n",
      "step 5758 loss 0.02774084359407425\n",
      "step 5759 loss 0.02773638814687729\n",
      "step 5760 loss 0.02773193269968033\n",
      "step 5761 loss 0.027727462351322174\n",
      "step 5762 loss 0.027722982689738274\n",
      "step 5763 loss 0.027718521654605865\n",
      "step 5764 loss 0.027714041993021965\n",
      "step 5765 loss 0.02770957164466381\n",
      "step 5766 loss 0.027705078944563866\n",
      "step 5767 loss 0.027700603008270264\n",
      "step 5768 loss 0.027696147561073303\n",
      "step 5769 loss 0.02769169583916664\n",
      "step 5770 loss 0.027687206864356995\n",
      "step 5771 loss 0.027682757005095482\n",
      "step 5772 loss 0.027678299695253372\n",
      "step 5773 loss 0.027673842385411263\n",
      "step 5774 loss 0.027669422328472137\n",
      "step 5775 loss 0.02766498550772667\n",
      "step 5776 loss 0.02766055054962635\n",
      "step 5777 loss 0.027656136080622673\n",
      "step 5778 loss 0.027651729062199593\n",
      "step 5779 loss 0.02764730527997017\n",
      "step 5780 loss 0.02764291688799858\n",
      "step 5781 loss 0.027638528496026993\n",
      "step 5782 loss 0.027634158730506897\n",
      "step 5783 loss 0.027629785239696503\n",
      "step 5784 loss 0.027625413611531258\n",
      "step 5785 loss 0.02762107364833355\n",
      "step 5786 loss 0.027616729959845543\n",
      "step 5787 loss 0.027612395584583282\n",
      "step 5788 loss 0.027608061209321022\n",
      "step 5789 loss 0.027603760361671448\n",
      "step 5790 loss 0.027599439024925232\n",
      "step 5791 loss 0.027595127001404762\n",
      "step 5792 loss 0.027590852230787277\n",
      "step 5793 loss 0.02758657932281494\n",
      "step 5794 loss 0.027582302689552307\n",
      "step 5795 loss 0.027578039094805717\n",
      "step 5796 loss 0.027573786675930023\n",
      "step 5797 loss 0.027569547295570374\n",
      "step 5798 loss 0.027565311640501022\n",
      "step 5799 loss 0.027561083436012268\n",
      "step 5800 loss 0.02755686081945896\n",
      "step 5801 loss 0.02755265310406685\n",
      "step 5802 loss 0.027548443526029587\n",
      "step 5803 loss 0.027544261887669563\n",
      "step 5804 loss 0.027540063485503197\n",
      "step 5805 loss 0.02753588743507862\n",
      "step 5806 loss 0.027531728148460388\n",
      "step 5807 loss 0.027527572587132454\n",
      "step 5808 loss 0.027523422613739967\n",
      "step 5809 loss 0.027519291266798973\n",
      "step 5810 loss 0.027515139430761337\n",
      "step 5811 loss 0.02751103788614273\n",
      "step 5812 loss 0.02750689722597599\n",
      "step 5813 loss 0.027502797544002533\n",
      "step 5814 loss 0.02749868854880333\n",
      "step 5815 loss 0.027494626119732857\n",
      "step 5816 loss 0.02749052084982395\n",
      "step 5817 loss 0.02748642861843109\n",
      "step 5818 loss 0.027482379227876663\n",
      "step 5819 loss 0.02747831866145134\n",
      "step 5820 loss 0.027474256232380867\n",
      "step 5821 loss 0.027470216155052185\n",
      "step 5822 loss 0.0274661872535944\n",
      "step 5823 loss 0.027462158352136612\n",
      "step 5824 loss 0.027458129450678825\n",
      "step 5825 loss 0.027454109862446785\n",
      "step 5826 loss 0.02745009958744049\n",
      "step 5827 loss 0.02744610793888569\n",
      "step 5828 loss 0.027442123740911484\n",
      "step 5829 loss 0.02743813954293728\n",
      "step 5830 loss 0.02743414230644703\n",
      "step 5831 loss 0.02743019163608551\n",
      "step 5832 loss 0.02742621675133705\n",
      "step 5833 loss 0.02742227539420128\n",
      "step 5834 loss 0.02741832658648491\n",
      "step 5835 loss 0.027414370328187943\n",
      "step 5836 loss 0.02741045318543911\n",
      "step 5837 loss 0.02740650810301304\n",
      "step 5838 loss 0.027402594685554504\n",
      "step 5839 loss 0.027398694306612015\n",
      "step 5840 loss 0.027394771575927734\n",
      "step 5841 loss 0.0273908618837595\n",
      "step 5842 loss 0.027386978268623352\n",
      "step 5843 loss 0.027383098378777504\n",
      "step 5844 loss 0.027379220351576805\n",
      "step 5845 loss 0.027375349774956703\n",
      "step 5846 loss 0.0273714829236269\n",
      "step 5847 loss 0.027367601171135902\n",
      "step 5848 loss 0.027363749220967293\n",
      "step 5849 loss 0.027359910309314728\n",
      "step 5850 loss 0.027356062084436417\n",
      "step 5851 loss 0.02735223062336445\n",
      "step 5852 loss 0.02734840102493763\n",
      "step 5853 loss 0.02734459936618805\n",
      "step 5854 loss 0.02734077163040638\n",
      "step 5855 loss 0.02733696438372135\n",
      "step 5856 loss 0.02733316645026207\n",
      "step 5857 loss 0.02732936292886734\n",
      "step 5858 loss 0.0273255817592144\n",
      "step 5859 loss 0.02732177823781967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5860 loss 0.02731800638139248\n",
      "step 5861 loss 0.027314234524965286\n",
      "step 5862 loss 0.027310475707054138\n",
      "step 5863 loss 0.02730671875178814\n",
      "step 5864 loss 0.027302980422973633\n",
      "step 5865 loss 0.02729920856654644\n",
      "step 5866 loss 0.027295494452118874\n",
      "step 5867 loss 0.02729174681007862\n",
      "step 5868 loss 0.02728802151978016\n",
      "step 5869 loss 0.0272842887789011\n",
      "step 5870 loss 0.027280569076538086\n",
      "step 5871 loss 0.027276858687400818\n",
      "step 5872 loss 0.027273159474134445\n",
      "step 5873 loss 0.02726946398615837\n",
      "step 5874 loss 0.0272657610476017\n",
      "step 5875 loss 0.027262063696980476\n",
      "step 5876 loss 0.027258386835455894\n",
      "step 5877 loss 0.02725471928715706\n",
      "step 5878 loss 0.02725105732679367\n",
      "step 5879 loss 0.027247386053204536\n",
      "step 5880 loss 0.027243731543421745\n",
      "step 5881 loss 0.027240075170993805\n",
      "step 5882 loss 0.027236437425017357\n",
      "step 5883 loss 0.02723279967904091\n",
      "step 5884 loss 0.027229158207774162\n",
      "step 5885 loss 0.027225535362958908\n",
      "step 5886 loss 0.027221906930208206\n",
      "step 5887 loss 0.027218280360102654\n",
      "step 5888 loss 0.02721465565264225\n",
      "step 5889 loss 0.027211064472794533\n",
      "step 5890 loss 0.02720746584236622\n",
      "step 5891 loss 0.027203859761357307\n",
      "step 5892 loss 0.02720027044415474\n",
      "step 5893 loss 0.027196694165468216\n",
      "step 5894 loss 0.027193106710910797\n",
      "step 5895 loss 0.027189528569579124\n",
      "step 5896 loss 0.027185970917344093\n",
      "step 5897 loss 0.02718239836394787\n",
      "step 5898 loss 0.02717885933816433\n",
      "step 5899 loss 0.027175297960639\n",
      "step 5900 loss 0.027171755209565163\n",
      "step 5901 loss 0.02716820314526558\n",
      "step 5902 loss 0.027164654806256294\n",
      "step 5903 loss 0.02716114930808544\n",
      "step 5904 loss 0.0271576140075922\n",
      "step 5905 loss 0.02715408243238926\n",
      "step 5906 loss 0.027150562033057213\n",
      "step 5907 loss 0.02714705280959606\n",
      "step 5908 loss 0.027143560349941254\n",
      "step 5909 loss 0.027140052989125252\n",
      "step 5910 loss 0.027136554941534996\n",
      "step 5911 loss 0.027133045718073845\n",
      "step 5912 loss 0.02712956815958023\n",
      "step 5913 loss 0.027126096189022064\n",
      "step 5914 loss 0.027122633531689644\n",
      "step 5915 loss 0.02711915783584118\n",
      "step 5916 loss 0.02711568772792816\n",
      "step 5917 loss 0.027112239971756935\n",
      "step 5918 loss 0.02710878849029541\n",
      "step 5919 loss 0.027105331420898438\n",
      "step 5920 loss 0.027101891115307808\n",
      "step 5921 loss 0.027098454535007477\n",
      "step 5922 loss 0.027095003053545952\n",
      "step 5923 loss 0.027091585099697113\n",
      "step 5924 loss 0.02708818018436432\n",
      "step 5925 loss 0.027084749191999435\n",
      "step 5926 loss 0.02708134613931179\n",
      "step 5927 loss 0.027077915146946907\n",
      "step 5928 loss 0.027074525132775307\n",
      "step 5929 loss 0.027071112766861916\n",
      "step 5930 loss 0.027067726477980614\n",
      "step 5931 loss 0.027064332738518715\n",
      "step 5932 loss 0.027060937136411667\n",
      "step 5933 loss 0.02705756016075611\n",
      "step 5934 loss 0.0270541962236166\n",
      "step 5935 loss 0.02705082856118679\n",
      "step 5936 loss 0.027047447860240936\n",
      "step 5937 loss 0.027044091373682022\n",
      "step 5938 loss 0.0270407572388649\n",
      "step 5939 loss 0.027037376537919044\n",
      "step 5940 loss 0.027034059166908264\n",
      "step 5941 loss 0.027030715718865395\n",
      "step 5942 loss 0.027027392759919167\n",
      "step 5943 loss 0.027024049311876297\n",
      "step 5944 loss 0.02702072076499462\n",
      "step 5945 loss 0.02701740711927414\n",
      "step 5946 loss 0.027014093473553658\n",
      "step 5947 loss 0.02701079286634922\n",
      "step 5948 loss 0.0270074512809515\n",
      "step 5949 loss 0.027004165574908257\n",
      "step 5950 loss 0.027000876143574715\n",
      "step 5951 loss 0.02699757181107998\n",
      "step 5952 loss 0.026994286105036736\n",
      "step 5953 loss 0.02699100412428379\n",
      "step 5954 loss 0.026987725868821144\n",
      "step 5955 loss 0.02698443830013275\n",
      "step 5956 loss 0.02698119729757309\n",
      "step 5957 loss 0.026977915316820145\n",
      "step 5958 loss 0.026974646374583244\n",
      "step 5959 loss 0.02697138860821724\n",
      "step 5960 loss 0.02696813829243183\n",
      "step 5961 loss 0.026964884251356125\n",
      "step 5962 loss 0.02696164697408676\n",
      "step 5963 loss 0.026958415284752846\n",
      "step 5964 loss 0.02695518173277378\n",
      "step 5965 loss 0.026951966807246208\n",
      "step 5966 loss 0.02694874256849289\n",
      "step 5967 loss 0.02694549970328808\n",
      "step 5968 loss 0.026942305266857147\n",
      "step 5969 loss 0.026939092203974724\n",
      "step 5970 loss 0.026935892179608345\n",
      "step 5971 loss 0.026932675391435623\n",
      "step 5972 loss 0.026929490268230438\n",
      "step 5973 loss 0.026926303282380104\n",
      "step 5974 loss 0.026923108845949173\n",
      "step 5975 loss 0.026919927448034286\n",
      "step 5976 loss 0.026916736736893654\n",
      "step 5977 loss 0.02691355161368847\n",
      "step 5978 loss 0.02691040001809597\n",
      "step 5979 loss 0.026907239109277725\n",
      "step 5980 loss 0.02690407820045948\n",
      "step 5981 loss 0.02690090425312519\n",
      "step 5982 loss 0.026897747069597244\n",
      "step 5983 loss 0.02689461037516594\n",
      "step 5984 loss 0.026891455054283142\n",
      "step 5985 loss 0.026888325810432434\n",
      "step 5986 loss 0.026885204017162323\n",
      "step 5987 loss 0.026882071048021317\n",
      "step 5988 loss 0.026878943666815758\n",
      "step 5989 loss 0.0268758162856102\n",
      "step 5990 loss 0.026872694492340088\n",
      "step 5991 loss 0.026869572699069977\n",
      "step 5992 loss 0.026866478845477104\n",
      "step 5993 loss 0.026863373816013336\n",
      "step 5994 loss 0.026860274374485016\n",
      "step 5995 loss 0.026857156306505203\n",
      "step 5996 loss 0.026854081079363823\n",
      "step 5997 loss 0.026850981637835503\n",
      "step 5998 loss 0.02684791013598442\n",
      "step 5999 loss 0.026844821870326996\n",
      "step 6000 loss 0.026841750368475914\n",
      "step 6001 loss 0.026838675141334534\n",
      "step 6002 loss 0.0268356055021286\n",
      "step 6003 loss 0.026832543313503265\n",
      "step 6004 loss 0.026829490438103676\n",
      "step 6005 loss 0.026826433837413788\n",
      "step 6006 loss 0.02682337909936905\n",
      "step 6007 loss 0.026820329949259758\n",
      "step 6008 loss 0.026817303150892258\n",
      "step 6009 loss 0.026814259588718414\n",
      "step 6010 loss 0.02681124210357666\n",
      "step 6011 loss 0.026808200404047966\n",
      "step 6012 loss 0.026805156841874123\n",
      "step 6013 loss 0.026802148669958115\n",
      "step 6014 loss 0.02679913491010666\n",
      "step 6015 loss 0.026796111837029457\n",
      "step 6016 loss 0.0267931055277586\n",
      "step 6017 loss 0.02679009921848774\n",
      "step 6018 loss 0.026787109673023224\n",
      "step 6019 loss 0.026784110814332962\n",
      "step 6020 loss 0.026781119406223297\n",
      "step 6021 loss 0.026778120547533035\n",
      "step 6022 loss 0.026775136590003967\n",
      "step 6023 loss 0.026772160083055496\n",
      "step 6024 loss 0.026769189164042473\n",
      "step 6025 loss 0.026766197755932808\n",
      "step 6026 loss 0.026763254776597023\n",
      "step 6027 loss 0.026760289445519447\n",
      "step 6028 loss 0.02675730735063553\n",
      "step 6029 loss 0.026754355058073997\n",
      "step 6030 loss 0.026751399040222168\n",
      "step 6031 loss 0.02674846723675728\n",
      "step 6032 loss 0.02674553170800209\n",
      "step 6033 loss 0.026742568239569664\n",
      "step 6034 loss 0.026739640161395073\n",
      "step 6035 loss 0.02673671767115593\n",
      "step 6036 loss 0.026733778417110443\n",
      "step 6037 loss 0.026730863377451897\n",
      "step 6038 loss 0.026727933436632156\n",
      "step 6039 loss 0.02672502025961876\n",
      "step 6040 loss 0.026722097769379616\n",
      "step 6041 loss 0.026719193905591965\n",
      "step 6042 loss 0.026716290041804314\n",
      "step 6043 loss 0.0267134141176939\n",
      "step 6044 loss 0.026710495352745056\n",
      "step 6045 loss 0.026707615703344345\n",
      "step 6046 loss 0.02670472115278244\n",
      "step 6047 loss 0.026701822876930237\n",
      "step 6048 loss 0.02669895626604557\n",
      "step 6049 loss 0.026696067303419113\n",
      "step 6050 loss 0.0266931913793087\n",
      "step 6051 loss 0.026690326631069183\n",
      "step 6052 loss 0.026687467470765114\n",
      "step 6053 loss 0.026684610173106194\n",
      "step 6054 loss 0.026681749150156975\n",
      "step 6055 loss 0.026678884401917458\n",
      "step 6056 loss 0.02667604759335518\n",
      "step 6057 loss 0.026673197746276855\n",
      "step 6058 loss 0.026670347899198532\n",
      "step 6059 loss 0.026667499914765358\n",
      "step 6060 loss 0.026664666831493378\n",
      "step 6061 loss 0.026661856099963188\n",
      "step 6062 loss 0.02665901742875576\n",
      "step 6063 loss 0.02665618248283863\n",
      "step 6064 loss 0.026653366163372993\n",
      "step 6065 loss 0.026650553569197655\n",
      "step 6066 loss 0.026647748425602913\n",
      "step 6067 loss 0.026644930243492126\n",
      "step 6068 loss 0.026642121374607086\n",
      "step 6069 loss 0.02663932740688324\n",
      "step 6070 loss 0.026636533439159393\n",
      "step 6071 loss 0.026633739471435547\n",
      "step 6072 loss 0.026630939915776253\n",
      "step 6073 loss 0.0266281608492136\n",
      "step 6074 loss 0.026625370606780052\n",
      "step 6075 loss 0.026622585952281952\n",
      "step 6076 loss 0.026619810611009598\n",
      "step 6077 loss 0.026617038995027542\n",
      "step 6078 loss 0.02661428414285183\n",
      "step 6079 loss 0.02661151811480522\n",
      "step 6080 loss 0.026608748361468315\n",
      "step 6081 loss 0.026605989784002304\n",
      "step 6082 loss 0.02660324051976204\n",
      "step 6083 loss 0.026600494980812073\n",
      "step 6084 loss 0.026597756892442703\n",
      "step 6085 loss 0.026594987139105797\n",
      "step 6086 loss 0.026592249050736427\n",
      "step 6087 loss 0.026589518412947655\n",
      "step 6088 loss 0.02658678963780403\n",
      "step 6089 loss 0.026584062725305557\n",
      "step 6090 loss 0.026581335812807083\n",
      "step 6091 loss 0.0265786275267601\n",
      "step 6092 loss 0.02657589688897133\n",
      "step 6093 loss 0.026573190465569496\n",
      "step 6094 loss 0.026570456102490425\n",
      "step 6095 loss 0.026567768305540085\n",
      "step 6096 loss 0.026565060019493103\n",
      "step 6097 loss 0.026562366634607315\n",
      "step 6098 loss 0.02655966579914093\n",
      "step 6099 loss 0.02655697613954544\n",
      "step 6100 loss 0.026554279029369354\n",
      "step 6101 loss 0.02655159868299961\n",
      "step 6102 loss 0.02654891088604927\n",
      "step 6103 loss 0.026546234264969826\n",
      "step 6104 loss 0.02654355950653553\n",
      "step 6105 loss 0.026540890336036682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 6106 loss 0.026538219302892685\n",
      "step 6107 loss 0.026535553857684135\n",
      "step 6108 loss 0.026532894000411034\n",
      "step 6109 loss 0.026530232280492783\n",
      "step 6110 loss 0.026527591049671173\n",
      "step 6111 loss 0.026524925604462624\n",
      "step 6112 loss 0.026522288098931313\n",
      "step 6113 loss 0.026519637554883957\n",
      "step 6114 loss 0.026516996324062347\n",
      "step 6115 loss 0.026514345780014992\n",
      "step 6116 loss 0.026511726900935173\n",
      "step 6117 loss 0.026509111747145653\n",
      "step 6118 loss 0.026506466791033745\n",
      "step 6119 loss 0.026503844186663628\n",
      "step 6120 loss 0.026501206681132317\n",
      "step 6121 loss 0.02649860642850399\n",
      "step 6122 loss 0.026495981961488724\n",
      "step 6123 loss 0.026493370532989502\n",
      "step 6124 loss 0.02649078145623207\n",
      "step 6125 loss 0.026488153263926506\n",
      "step 6126 loss 0.026485556736588478\n",
      "step 6127 loss 0.02648298256099224\n",
      "step 6128 loss 0.026480378583073616\n",
      "step 6129 loss 0.026477791368961334\n",
      "step 6130 loss 0.026475191116333008\n",
      "step 6131 loss 0.026472611352801323\n",
      "step 6132 loss 0.026470037177205086\n",
      "step 6133 loss 0.026467470452189445\n",
      "step 6134 loss 0.026464877650141716\n",
      "step 6135 loss 0.02646232582628727\n",
      "step 6136 loss 0.026459742337465286\n",
      "step 6137 loss 0.02645719051361084\n",
      "step 6138 loss 0.026454629376530647\n",
      "step 6139 loss 0.026452070102095604\n",
      "step 6140 loss 0.026449518278241158\n",
      "step 6141 loss 0.026446960866451263\n",
      "step 6142 loss 0.026444420218467712\n",
      "step 6143 loss 0.026441866531968117\n",
      "step 6144 loss 0.026439327746629715\n",
      "step 6145 loss 0.026436805725097656\n",
      "step 6146 loss 0.026434265077114105\n",
      "step 6147 loss 0.0264317374676466\n",
      "step 6148 loss 0.02642921917140484\n",
      "step 6149 loss 0.026426684111356735\n",
      "step 6150 loss 0.02642415463924408\n",
      "step 6151 loss 0.02642165496945381\n",
      "step 6152 loss 0.026419129222631454\n",
      "step 6153 loss 0.026416607201099396\n",
      "step 6154 loss 0.026414107531309128\n",
      "step 6155 loss 0.02641160413622856\n",
      "step 6156 loss 0.026409100741147995\n",
      "step 6157 loss 0.026406602934002876\n",
      "step 6158 loss 0.026404118165373802\n",
      "step 6159 loss 0.02640163153409958\n",
      "step 6160 loss 0.026399126276373863\n",
      "step 6161 loss 0.026396647095680237\n",
      "step 6162 loss 0.026394156739115715\n",
      "step 6163 loss 0.02639167755842209\n",
      "step 6164 loss 0.02638920396566391\n",
      "step 6165 loss 0.026386713609099388\n",
      "step 6166 loss 0.026384256780147552\n",
      "step 6167 loss 0.026381783187389374\n",
      "step 6168 loss 0.02637932077050209\n",
      "step 6169 loss 0.026376867666840553\n",
      "step 6170 loss 0.026374416425824165\n",
      "step 6171 loss 0.026371978223323822\n",
      "step 6172 loss 0.02636951021850109\n",
      "step 6173 loss 0.026367053389549255\n",
      "step 6174 loss 0.026364607736468315\n",
      "step 6175 loss 0.026362190023064613\n",
      "step 6176 loss 0.02635972946882248\n",
      "step 6177 loss 0.026357300579547882\n",
      "step 6178 loss 0.026354866102337837\n",
      "step 6179 loss 0.026352429762482643\n",
      "step 6180 loss 0.026350002735853195\n",
      "step 6181 loss 0.02634759433567524\n",
      "step 6182 loss 0.026345165446400642\n",
      "step 6183 loss 0.026342742145061493\n",
      "step 6184 loss 0.026340337470173836\n",
      "step 6185 loss 0.026337919756770134\n",
      "step 6186 loss 0.02633551135659218\n",
      "step 6187 loss 0.026333102956414223\n",
      "step 6188 loss 0.02633071132004261\n",
      "step 6189 loss 0.026328302919864655\n",
      "step 6190 loss 0.026325903832912445\n",
      "step 6191 loss 0.026323523372411728\n",
      "step 6192 loss 0.026321139186620712\n",
      "step 6193 loss 0.026318736374378204\n",
      "step 6194 loss 0.026316355913877487\n",
      "step 6195 loss 0.026313969865441322\n",
      "step 6196 loss 0.026311587542295456\n",
      "step 6197 loss 0.026309233158826828\n",
      "step 6198 loss 0.026306848973035812\n",
      "step 6199 loss 0.02630448527634144\n",
      "step 6200 loss 0.02630210481584072\n",
      "step 6201 loss 0.026299752295017242\n",
      "step 6202 loss 0.026297390460968018\n",
      "step 6203 loss 0.026295023038983345\n",
      "step 6204 loss 0.026292674243450165\n",
      "step 6205 loss 0.026290321722626686\n",
      "step 6206 loss 0.026287974789738655\n",
      "step 6207 loss 0.026285629719495773\n",
      "step 6208 loss 0.026283282786607742\n",
      "step 6209 loss 0.026280950754880905\n",
      "step 6210 loss 0.02627861313521862\n",
      "step 6211 loss 0.02627628482878208\n",
      "step 6212 loss 0.026273950934410095\n",
      "step 6213 loss 0.026271618902683258\n",
      "step 6214 loss 0.02626929059624672\n",
      "step 6215 loss 0.02626696601510048\n",
      "step 6216 loss 0.02626466192305088\n",
      "step 6217 loss 0.026262328028678894\n",
      "step 6218 loss 0.02626003511250019\n",
      "step 6219 loss 0.026257716119289398\n",
      "step 6220 loss 0.02625541016459465\n",
      "step 6221 loss 0.02625310979783535\n",
      "step 6222 loss 0.026250796392560005\n",
      "step 6223 loss 0.026248503476381302\n",
      "step 6224 loss 0.026246199384331703\n",
      "step 6225 loss 0.026243915781378746\n",
      "step 6226 loss 0.026241637766361237\n",
      "step 6227 loss 0.026239333674311638\n",
      "step 6228 loss 0.026237063109874725\n",
      "step 6229 loss 0.026234764605760574\n",
      "step 6230 loss 0.026232490316033363\n",
      "step 6231 loss 0.02623012103140354\n",
      "step 6232 loss 0.026227761059999466\n",
      "step 6233 loss 0.02622538059949875\n",
      "step 6234 loss 0.026223013177514076\n",
      "step 6235 loss 0.02622060850262642\n",
      "step 6236 loss 0.026218228042125702\n",
      "step 6237 loss 0.02621583640575409\n",
      "step 6238 loss 0.026213426142930984\n",
      "step 6239 loss 0.026211032643914223\n",
      "step 6240 loss 0.026208654046058655\n",
      "step 6241 loss 0.026206254959106445\n",
      "step 6242 loss 0.026203863322734833\n",
      "step 6243 loss 0.026201467961072922\n",
      "step 6244 loss 0.02619905211031437\n",
      "step 6245 loss 0.026196666061878204\n",
      "step 6246 loss 0.026194266974925995\n",
      "step 6247 loss 0.026191867887973785\n",
      "step 6248 loss 0.026189489290118217\n",
      "step 6249 loss 0.026187093928456306\n",
      "step 6250 loss 0.026184696704149246\n",
      "step 6251 loss 0.026182325556874275\n",
      "step 6252 loss 0.026179924607276917\n",
      "step 6253 loss 0.026177549734711647\n",
      "step 6254 loss 0.026175163686275482\n",
      "step 6255 loss 0.026172775775194168\n",
      "step 6256 loss 0.0261703971773386\n",
      "step 6257 loss 0.026168039068579674\n",
      "step 6258 loss 0.02616565302014351\n",
      "step 6259 loss 0.026163289323449135\n",
      "step 6260 loss 0.026160912588238716\n",
      "step 6261 loss 0.02615853026509285\n",
      "step 6262 loss 0.026156188920140266\n",
      "step 6263 loss 0.026153817772865295\n",
      "step 6264 loss 0.026151476427912712\n",
      "step 6265 loss 0.026149116456508636\n",
      "step 6266 loss 0.02614676021039486\n",
      "step 6267 loss 0.02614441327750683\n",
      "step 6268 loss 0.02614206075668335\n",
      "step 6269 loss 0.026139726862311363\n",
      "step 6270 loss 0.026137379929423332\n",
      "step 6271 loss 0.02613503485918045\n",
      "step 6272 loss 0.026132704690098763\n",
      "step 6273 loss 0.026130370795726776\n",
      "step 6274 loss 0.026128048077225685\n",
      "step 6275 loss 0.026125721633434296\n",
      "step 6276 loss 0.026123378425836563\n",
      "step 6277 loss 0.02612105756998062\n",
      "step 6278 loss 0.026118747889995575\n",
      "step 6279 loss 0.02611643075942993\n",
      "step 6280 loss 0.026114128530025482\n",
      "step 6281 loss 0.02611180767416954\n",
      "step 6282 loss 0.02610950544476509\n",
      "step 6283 loss 0.026107197627425194\n",
      "step 6284 loss 0.02610490843653679\n",
      "step 6285 loss 0.026102609932422638\n",
      "step 6286 loss 0.02610030584037304\n",
      "step 6287 loss 0.026098022237420082\n",
      "step 6288 loss 0.02609572932124138\n",
      "step 6289 loss 0.026093434542417526\n",
      "step 6290 loss 0.026091160252690315\n",
      "step 6291 loss 0.02608889900147915\n",
      "step 6292 loss 0.026086606085300446\n",
      "step 6293 loss 0.026084329932928085\n",
      "step 6294 loss 0.026082061231136322\n",
      "step 6295 loss 0.026079779490828514\n",
      "step 6296 loss 0.026077518239617348\n",
      "step 6297 loss 0.026075253263115883\n",
      "step 6298 loss 0.026072995737195015\n",
      "step 6299 loss 0.0260707326233387\n",
      "step 6300 loss 0.02606848254799843\n",
      "step 6301 loss 0.026066234335303307\n",
      "step 6302 loss 0.026063986122608185\n",
      "step 6303 loss 0.02606174536049366\n",
      "step 6304 loss 0.026059499010443687\n",
      "step 6305 loss 0.026057258248329163\n",
      "step 6306 loss 0.02605501189827919\n",
      "step 6307 loss 0.026052774861454964\n",
      "step 6308 loss 0.026050549000501633\n",
      "step 6309 loss 0.026048339903354645\n",
      "step 6310 loss 0.026046080514788628\n",
      "step 6311 loss 0.02604386769235134\n",
      "step 6312 loss 0.02604164369404316\n",
      "step 6313 loss 0.026039432734251022\n",
      "step 6314 loss 0.026037221774458885\n",
      "step 6315 loss 0.026034999638795853\n",
      "step 6316 loss 0.026032790541648865\n",
      "step 6317 loss 0.026030588895082474\n",
      "step 6318 loss 0.026028385385870934\n",
      "step 6319 loss 0.026026206091046333\n",
      "step 6320 loss 0.02602398209273815\n",
      "step 6321 loss 0.026021786034107208\n",
      "step 6322 loss 0.02601959928870201\n",
      "step 6323 loss 0.026017403230071068\n",
      "step 6324 loss 0.026015207171440125\n",
      "step 6325 loss 0.026013050228357315\n",
      "step 6326 loss 0.02601083740592003\n",
      "step 6327 loss 0.02600867673754692\n",
      "step 6328 loss 0.026006503030657768\n",
      "step 6329 loss 0.026004308834671974\n",
      "step 6330 loss 0.02600215934216976\n",
      "step 6331 loss 0.025999970734119415\n",
      "step 6332 loss 0.025997815653681755\n",
      "step 6333 loss 0.02599564753472805\n",
      "step 6334 loss 0.025993485003709793\n",
      "step 6335 loss 0.02599131502211094\n",
      "step 6336 loss 0.02598918043076992\n",
      "step 6337 loss 0.02598702721297741\n",
      "step 6338 loss 0.0259848739951849\n",
      "step 6339 loss 0.02598273567855358\n",
      "step 6340 loss 0.025980569422245026\n",
      "step 6341 loss 0.025978434830904007\n",
      "step 6342 loss 0.025976305827498436\n",
      "step 6343 loss 0.02597416378557682\n",
      "step 6344 loss 0.025972027331590652\n",
      "step 6345 loss 0.025969890877604485\n",
      "step 6346 loss 0.025967774912714958\n",
      "step 6347 loss 0.025965632870793343\n",
      "step 6348 loss 0.025963518768548965\n",
      "step 6349 loss 0.025961387902498245\n",
      "step 6350 loss 0.025959288701415062\n",
      "step 6351 loss 0.02595715969800949\n",
      "step 6352 loss 0.025955047458410263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 6353 loss 0.025952953845262527\n",
      "step 6354 loss 0.025950824841856956\n",
      "step 6355 loss 0.02594873681664467\n",
      "step 6356 loss 0.02594662271440029\n",
      "step 6357 loss 0.025944538414478302\n",
      "step 6358 loss 0.02594243548810482\n",
      "step 6359 loss 0.025940347462892532\n",
      "step 6360 loss 0.025938238948583603\n",
      "step 6361 loss 0.02593616023659706\n",
      "step 6362 loss 0.02593408152461052\n",
      "step 6363 loss 0.025931986048817635\n",
      "step 6364 loss 0.025929909199476242\n",
      "step 6365 loss 0.025927817448973656\n",
      "step 6366 loss 0.02592574618756771\n",
      "step 6367 loss 0.025923671200871468\n",
      "step 6368 loss 0.025921601802110672\n",
      "step 6369 loss 0.025919537991285324\n",
      "step 6370 loss 0.02591746859252453\n",
      "step 6371 loss 0.02591540850698948\n",
      "step 6372 loss 0.025913339108228683\n",
      "step 6373 loss 0.025911293923854828\n",
      "step 6374 loss 0.025909218937158585\n",
      "step 6375 loss 0.02590717002749443\n",
      "step 6376 loss 0.025905117392539978\n",
      "step 6377 loss 0.025903087109327316\n",
      "step 6378 loss 0.025901013985276222\n",
      "step 6379 loss 0.02589898183941841\n",
      "step 6380 loss 0.025896938517689705\n",
      "step 6381 loss 0.02589488960802555\n",
      "step 6382 loss 0.02589286118745804\n",
      "step 6383 loss 0.025890836492180824\n",
      "step 6384 loss 0.025888800621032715\n",
      "step 6385 loss 0.025886764749884605\n",
      "step 6386 loss 0.02588474005460739\n",
      "step 6387 loss 0.025882720947265625\n",
      "step 6388 loss 0.025880688801407814\n",
      "step 6389 loss 0.02587866596877575\n",
      "step 6390 loss 0.025876659899950027\n",
      "step 6391 loss 0.025874655693769455\n",
      "step 6392 loss 0.02587262913584709\n",
      "step 6393 loss 0.025870628654956818\n",
      "step 6394 loss 0.02586861327290535\n",
      "step 6395 loss 0.02586660347878933\n",
      "step 6396 loss 0.025864623486995697\n",
      "step 6397 loss 0.025862611830234528\n",
      "step 6398 loss 0.025860624387860298\n",
      "step 6399 loss 0.02585863135755062\n",
      "step 6400 loss 0.0258566252887249\n",
      "step 6401 loss 0.025854645296931267\n",
      "step 6402 loss 0.025852670893073082\n",
      "step 6403 loss 0.025850672274827957\n",
      "step 6404 loss 0.025848694145679474\n",
      "step 6405 loss 0.02584671415388584\n",
      "step 6406 loss 0.025844726711511612\n",
      "step 6407 loss 0.025842741131782532\n",
      "step 6408 loss 0.025840774178504944\n",
      "step 6409 loss 0.0258388239890337\n",
      "step 6410 loss 0.025836843997240067\n",
      "step 6411 loss 0.025834886357188225\n",
      "step 6412 loss 0.02583291381597519\n",
      "step 6413 loss 0.025830965489149094\n",
      "step 6414 loss 0.02582898549735546\n",
      "step 6415 loss 0.025827044621109962\n",
      "step 6416 loss 0.02582508511841297\n",
      "step 6417 loss 0.025823134928941727\n",
      "step 6418 loss 0.025821184739470482\n",
      "step 6419 loss 0.025819243863224983\n",
      "step 6420 loss 0.025817301124334335\n",
      "step 6421 loss 0.025815345346927643\n",
      "step 6422 loss 0.025813398882746696\n",
      "step 6423 loss 0.02581147663295269\n",
      "step 6424 loss 0.025809545069932938\n",
      "step 6425 loss 0.02580760046839714\n",
      "step 6426 loss 0.025805668905377388\n",
      "step 6427 loss 0.025803742930293083\n",
      "step 6428 loss 0.02580181322991848\n",
      "step 6429 loss 0.025799879804253578\n",
      "step 6430 loss 0.02579796500504017\n",
      "step 6431 loss 0.02579604834318161\n",
      "step 6432 loss 0.025794142857193947\n",
      "step 6433 loss 0.025792215019464493\n",
      "step 6434 loss 0.02579030580818653\n",
      "step 6435 loss 0.025788402184844017\n",
      "step 6436 loss 0.025786474347114563\n",
      "step 6437 loss 0.025784583762288094\n",
      "step 6438 loss 0.02578268013894558\n",
      "step 6439 loss 0.025780782103538513\n",
      "step 6440 loss 0.025778889656066895\n",
      "step 6441 loss 0.02577698789536953\n",
      "step 6442 loss 0.025775102898478508\n",
      "step 6443 loss 0.025773193687200546\n",
      "step 6444 loss 0.025771304965019226\n",
      "step 6445 loss 0.0257694274187088\n",
      "step 6446 loss 0.02576753869652748\n",
      "step 6447 loss 0.025765638798475266\n",
      "step 6448 loss 0.025763778015971184\n",
      "step 6449 loss 0.02576187625527382\n",
      "step 6450 loss 0.025760017335414886\n",
      "step 6451 loss 0.025758152827620506\n",
      "step 6452 loss 0.02575628273189068\n",
      "step 6453 loss 0.025754408910870552\n",
      "step 6454 loss 0.025752538815140724\n",
      "step 6455 loss 0.0257506612688303\n",
      "step 6456 loss 0.025748806074261665\n",
      "step 6457 loss 0.02574695646762848\n",
      "step 6458 loss 0.02574508637189865\n",
      "step 6459 loss 0.025743218138813972\n",
      "step 6460 loss 0.02574138157069683\n",
      "step 6461 loss 0.025739531964063644\n",
      "step 6462 loss 0.025737671181559563\n",
      "step 6463 loss 0.025735821574926376\n",
      "step 6464 loss 0.025733981281518936\n",
      "step 6465 loss 0.025732126086950302\n",
      "step 6466 loss 0.025730302557349205\n",
      "step 6467 loss 0.025728445500135422\n",
      "step 6468 loss 0.02572663128376007\n",
      "step 6469 loss 0.02572479099035263\n",
      "step 6470 loss 0.02572295442223549\n",
      "step 6471 loss 0.025721121579408646\n",
      "step 6472 loss 0.02571929432451725\n",
      "step 6473 loss 0.02571745403110981\n",
      "step 6474 loss 0.025715641677379608\n",
      "step 6475 loss 0.02571382001042366\n",
      "step 6476 loss 0.025711998343467712\n",
      "step 6477 loss 0.02571015991270542\n",
      "step 6478 loss 0.025708302855491638\n",
      "step 6479 loss 0.0257064551115036\n",
      "step 6480 loss 0.02570461295545101\n",
      "step 6481 loss 0.025702767074108124\n",
      "step 6482 loss 0.025700941681861877\n",
      "step 6483 loss 0.025699080899357796\n",
      "step 6484 loss 0.0256972573697567\n",
      "step 6485 loss 0.025695418938994408\n",
      "step 6486 loss 0.02569357305765152\n",
      "step 6487 loss 0.025691745802760124\n",
      "step 6488 loss 0.02568989247083664\n",
      "step 6489 loss 0.025688057765364647\n",
      "step 6490 loss 0.025686223059892654\n",
      "step 6491 loss 0.025684408843517303\n",
      "step 6492 loss 0.025682566687464714\n",
      "step 6493 loss 0.025680743157863617\n",
      "step 6494 loss 0.025678914040327072\n",
      "step 6495 loss 0.02567710354924202\n",
      "step 6496 loss 0.025675272569060326\n",
      "step 6497 loss 0.025673456490039825\n",
      "step 6498 loss 0.02567162923514843\n",
      "step 6499 loss 0.025669820606708527\n",
      "step 6500 loss 0.025668000802397728\n",
      "step 6501 loss 0.02566617541015148\n",
      "step 6502 loss 0.02566438727080822\n",
      "step 6503 loss 0.02566256932914257\n",
      "step 6504 loss 0.02566077746450901\n",
      "step 6505 loss 0.025658966973423958\n",
      "step 6506 loss 0.025657158344984055\n",
      "step 6507 loss 0.0256553515791893\n",
      "step 6508 loss 0.025653567165136337\n",
      "step 6509 loss 0.02565176971256733\n",
      "step 6510 loss 0.02564997225999832\n",
      "step 6511 loss 0.025648174807429314\n",
      "step 6512 loss 0.025646386668086052\n",
      "step 6513 loss 0.025644605979323387\n",
      "step 6514 loss 0.025642801076173782\n",
      "step 6515 loss 0.025641033425927162\n",
      "step 6516 loss 0.025639254599809647\n",
      "step 6517 loss 0.025637470185756683\n",
      "step 6518 loss 0.025635695084929466\n",
      "step 6519 loss 0.0256339218467474\n",
      "step 6520 loss 0.025632154196500778\n",
      "step 6521 loss 0.025630388408899307\n",
      "step 6522 loss 0.025628598406910896\n",
      "step 6523 loss 0.025626828894019127\n",
      "step 6524 loss 0.025625072419643402\n",
      "step 6525 loss 0.025623319670557976\n",
      "step 6526 loss 0.025621570646762848\n",
      "step 6527 loss 0.02561979740858078\n",
      "step 6528 loss 0.02561803348362446\n",
      "step 6529 loss 0.025616269558668137\n",
      "step 6530 loss 0.02561451867222786\n",
      "step 6531 loss 0.025612765923142433\n",
      "step 6532 loss 0.0256110318005085\n",
      "step 6533 loss 0.02560928836464882\n",
      "step 6534 loss 0.025607548654079437\n",
      "step 6535 loss 0.025605788454413414\n",
      "step 6536 loss 0.025604061782360077\n",
      "step 6537 loss 0.02560231275856495\n",
      "step 6538 loss 0.02560058981180191\n",
      "step 6539 loss 0.02559884637594223\n",
      "step 6540 loss 0.0255971010774374\n",
      "step 6541 loss 0.02559538185596466\n",
      "step 6542 loss 0.02559366077184677\n",
      "step 6543 loss 0.025591926649212837\n",
      "step 6544 loss 0.025590183213353157\n",
      "step 6545 loss 0.02558847889304161\n",
      "step 6546 loss 0.025586767122149467\n",
      "step 6547 loss 0.025585031136870384\n",
      "step 6548 loss 0.025583326816558838\n",
      "step 6549 loss 0.025581613183021545\n",
      "step 6550 loss 0.02557990327477455\n",
      "step 6551 loss 0.025578193366527557\n",
      "step 6552 loss 0.025576477870345116\n",
      "step 6553 loss 0.02557476796209812\n",
      "step 6554 loss 0.02557305246591568\n",
      "step 6555 loss 0.02557135373353958\n",
      "step 6556 loss 0.025569677352905273\n",
      "step 6557 loss 0.025567958131432533\n",
      "step 6558 loss 0.02556626871228218\n",
      "step 6559 loss 0.025564566254615784\n",
      "step 6560 loss 0.02556288242340088\n",
      "step 6561 loss 0.025561179965734482\n",
      "step 6562 loss 0.025559496134519577\n",
      "step 6563 loss 0.025557808578014374\n",
      "step 6564 loss 0.025556130334734917\n",
      "step 6565 loss 0.02555445209145546\n",
      "step 6566 loss 0.02555275894701481\n",
      "step 6567 loss 0.025551075115799904\n",
      "step 6568 loss 0.025549402460455894\n",
      "step 6569 loss 0.025547724217176437\n",
      "step 6570 loss 0.025546055287122726\n",
      "step 6571 loss 0.02554437704384327\n",
      "step 6572 loss 0.025542711839079857\n",
      "step 6573 loss 0.025541048496961594\n",
      "step 6574 loss 0.02553938329219818\n",
      "step 6575 loss 0.02553771436214447\n",
      "step 6576 loss 0.025536054745316505\n",
      "step 6577 loss 0.025534402579069138\n",
      "step 6578 loss 0.025532733649015427\n",
      "step 6579 loss 0.025531070306897163\n",
      "step 6580 loss 0.025529421865940094\n",
      "step 6581 loss 0.025527777150273323\n",
      "step 6582 loss 0.025526123121380806\n",
      "step 6583 loss 0.025524474680423737\n",
      "step 6584 loss 0.025522833690047264\n",
      "step 6585 loss 0.0255211740732193\n",
      "step 6586 loss 0.025519542396068573\n",
      "step 6587 loss 0.025517895817756653\n",
      "step 6588 loss 0.025516266003251076\n",
      "step 6589 loss 0.02551461197435856\n",
      "step 6590 loss 0.02551298588514328\n",
      "step 6591 loss 0.025511352345347404\n",
      "step 6592 loss 0.025509720668196678\n",
      "step 6593 loss 0.025508083403110504\n",
      "step 6594 loss 0.025506462901830673\n",
      "step 6595 loss 0.025504833087325096\n",
      "step 6596 loss 0.025503192096948624\n",
      "step 6597 loss 0.025501571595668793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 6598 loss 0.025499964132905006\n",
      "step 6599 loss 0.025498338043689728\n",
      "step 6600 loss 0.02549673058092594\n",
      "step 6601 loss 0.025495119392871857\n",
      "step 6602 loss 0.025493497028946877\n",
      "step 6603 loss 0.02549189142882824\n",
      "step 6604 loss 0.025490274652838707\n",
      "step 6605 loss 0.025488657876849174\n",
      "step 6606 loss 0.02548707090318203\n",
      "step 6607 loss 0.025485454127192497\n",
      "step 6608 loss 0.02548384480178356\n",
      "step 6609 loss 0.02548222988843918\n",
      "step 6610 loss 0.025480646640062332\n",
      "step 6611 loss 0.02547905035316944\n",
      "step 6612 loss 0.025477470830082893\n",
      "step 6613 loss 0.025475867092609406\n",
      "step 6614 loss 0.025474276393651962\n",
      "step 6615 loss 0.025472668930888176\n",
      "step 6616 loss 0.02547108568251133\n",
      "step 6617 loss 0.025469504296779633\n",
      "step 6618 loss 0.02546791546046734\n",
      "step 6619 loss 0.025466330349445343\n",
      "step 6620 loss 0.02546474151313305\n",
      "step 6621 loss 0.02546318620443344\n",
      "step 6622 loss 0.02546158991754055\n",
      "step 6623 loss 0.0254600141197443\n",
      "step 6624 loss 0.025458497926592827\n",
      "step 6625 loss 0.02545689046382904\n",
      "step 6626 loss 0.02545531839132309\n",
      "step 6627 loss 0.025453748181462288\n",
      "step 6628 loss 0.025452177971601486\n",
      "step 6629 loss 0.025450613349676132\n",
      "step 6630 loss 0.025449061766266823\n",
      "step 6631 loss 0.025447502732276917\n",
      "step 6632 loss 0.025445938110351562\n",
      "step 6633 loss 0.025444388389587402\n",
      "step 6634 loss 0.025442838668823242\n",
      "step 6635 loss 0.025441279634833336\n",
      "step 6636 loss 0.02543972060084343\n",
      "step 6637 loss 0.025438180193305016\n",
      "step 6638 loss 0.02543662115931511\n",
      "step 6639 loss 0.02543509006500244\n",
      "step 6640 loss 0.02543354034423828\n",
      "step 6641 loss 0.025431983172893524\n",
      "step 6642 loss 0.025430448353290558\n",
      "step 6643 loss 0.02542891725897789\n",
      "step 6644 loss 0.02542736567556858\n",
      "step 6645 loss 0.02542581968009472\n",
      "step 6646 loss 0.0254242941737175\n",
      "step 6647 loss 0.025422759354114532\n",
      "step 6648 loss 0.025421228259801865\n",
      "step 6649 loss 0.0254196934401989\n",
      "step 6650 loss 0.025418171659111977\n",
      "step 6651 loss 0.02541663683950901\n",
      "step 6652 loss 0.02541511133313179\n",
      "step 6653 loss 0.02541358955204487\n",
      "step 6654 loss 0.0254120584577322\n",
      "step 6655 loss 0.02541053295135498\n",
      "step 6656 loss 0.02540900744497776\n",
      "step 6657 loss 0.02540748566389084\n",
      "step 6658 loss 0.025405975058674812\n",
      "step 6659 loss 0.025404464453458786\n",
      "step 6660 loss 0.025402957573533058\n",
      "step 6661 loss 0.025401435792446136\n",
      "step 6662 loss 0.02539992332458496\n",
      "step 6663 loss 0.025398418307304382\n",
      "step 6664 loss 0.025396905839443207\n",
      "step 6665 loss 0.025395408272743225\n",
      "step 6666 loss 0.025393905118107796\n",
      "step 6667 loss 0.025392398238182068\n",
      "step 6668 loss 0.025390896946191788\n",
      "step 6669 loss 0.025389404967427254\n",
      "step 6670 loss 0.025387892499566078\n",
      "step 6671 loss 0.02538640983402729\n",
      "step 6672 loss 0.025384902954101562\n",
      "step 6673 loss 0.02538342960178852\n",
      "step 6674 loss 0.025381917133927345\n",
      "step 6675 loss 0.025380438193678856\n",
      "step 6676 loss 0.025378957390785217\n",
      "step 6677 loss 0.025377459824085236\n",
      "step 6678 loss 0.025375990197062492\n",
      "step 6679 loss 0.02537449821829796\n",
      "step 6680 loss 0.02537301555275917\n",
      "step 6681 loss 0.025371527299284935\n",
      "step 6682 loss 0.02537006139755249\n",
      "step 6683 loss 0.025368571281433105\n",
      "step 6684 loss 0.02536710724234581\n",
      "step 6685 loss 0.025365618988871574\n",
      "step 6686 loss 0.02536415494978428\n",
      "step 6687 loss 0.025362692773342133\n",
      "step 6688 loss 0.02536122128367424\n",
      "step 6689 loss 0.0253597442060709\n",
      "step 6690 loss 0.0253582950681448\n",
      "step 6691 loss 0.025356821715831757\n",
      "step 6692 loss 0.025355344638228416\n",
      "step 6693 loss 0.02535388432443142\n",
      "step 6694 loss 0.025352442637085915\n",
      "step 6695 loss 0.025350965559482574\n",
      "step 6696 loss 0.025349512696266174\n",
      "step 6697 loss 0.025348061695694923\n",
      "step 6698 loss 0.025346621870994568\n",
      "step 6699 loss 0.025345155969262123\n",
      "step 6700 loss 0.02534370869398117\n",
      "step 6701 loss 0.02534225955605507\n",
      "step 6702 loss 0.02534080669283867\n",
      "step 6703 loss 0.025339365005493164\n",
      "step 6704 loss 0.02533792518079281\n",
      "step 6705 loss 0.025336479768157005\n",
      "step 6706 loss 0.025335030630230904\n",
      "step 6707 loss 0.0253335852175951\n",
      "step 6708 loss 0.025332164019346237\n",
      "step 6709 loss 0.025330713018774986\n",
      "step 6710 loss 0.025329284369945526\n",
      "step 6711 loss 0.025327838957309723\n",
      "step 6712 loss 0.02532641962170601\n",
      "step 6713 loss 0.025325005874037743\n",
      "step 6714 loss 0.02532355859875679\n",
      "step 6715 loss 0.025322139263153076\n",
      "step 6716 loss 0.025320693850517273\n",
      "step 6717 loss 0.025319278240203857\n",
      "step 6718 loss 0.025317873805761337\n",
      "step 6719 loss 0.02531643956899643\n",
      "step 6720 loss 0.02531503140926361\n",
      "step 6721 loss 0.025313591584563255\n",
      "step 6722 loss 0.025312170386314392\n",
      "step 6723 loss 0.025310760363936424\n",
      "step 6724 loss 0.025309329852461815\n",
      "step 6725 loss 0.025307931005954742\n",
      "step 6726 loss 0.025306524708867073\n",
      "step 6727 loss 0.02530510351061821\n",
      "step 6728 loss 0.02530369721353054\n",
      "step 6729 loss 0.025302281603217125\n",
      "step 6730 loss 0.0253008846193552\n",
      "step 6731 loss 0.02529946155846119\n",
      "step 6732 loss 0.025298068299889565\n",
      "step 6733 loss 0.025296688079833984\n",
      "step 6734 loss 0.02529526688158512\n",
      "step 6735 loss 0.025293871760368347\n",
      "step 6736 loss 0.025292472913861275\n",
      "step 6737 loss 0.025291090831160545\n",
      "step 6738 loss 0.025289691984653473\n",
      "step 6739 loss 0.025288300588726997\n",
      "step 6740 loss 0.025286920368671417\n",
      "step 6741 loss 0.025285517796874046\n",
      "step 6742 loss 0.02528412826359272\n",
      "step 6743 loss 0.025282733142375946\n",
      "step 6744 loss 0.025281358510255814\n",
      "step 6745 loss 0.025279970839619637\n",
      "step 6746 loss 0.025278592482209206\n",
      "step 6747 loss 0.02527719736099243\n",
      "step 6748 loss 0.02527582459151745\n",
      "step 6749 loss 0.025274448096752167\n",
      "step 6750 loss 0.025273077189922333\n",
      "step 6751 loss 0.025271696969866753\n",
      "step 6752 loss 0.025270314887166023\n",
      "step 6753 loss 0.02526894025504589\n",
      "step 6754 loss 0.0252675823867321\n",
      "step 6755 loss 0.025266194716095924\n",
      "step 6756 loss 0.025264834985136986\n",
      "step 6757 loss 0.02526347152888775\n",
      "step 6758 loss 0.025262093171477318\n",
      "step 6759 loss 0.025260737165808678\n",
      "step 6760 loss 0.02525937557220459\n",
      "step 6761 loss 0.025258027017116547\n",
      "step 6762 loss 0.02525661699473858\n",
      "step 6763 loss 0.02525518462061882\n",
      "step 6764 loss 0.025253763422369957\n",
      "step 6765 loss 0.0252523273229599\n",
      "step 6766 loss 0.025250893086194992\n",
      "step 6767 loss 0.025249440222978592\n",
      "step 6768 loss 0.02524799294769764\n",
      "step 6769 loss 0.025246568024158478\n",
      "step 6770 loss 0.02524510771036148\n",
      "step 6771 loss 0.025243673473596573\n",
      "step 6772 loss 0.025242233648896217\n",
      "step 6773 loss 0.025240784510970116\n",
      "step 6774 loss 0.025239329785108566\n",
      "step 6775 loss 0.025237899273633957\n",
      "step 6776 loss 0.025236429646611214\n",
      "step 6777 loss 0.02523498609662056\n",
      "step 6778 loss 0.02523353509604931\n",
      "step 6779 loss 0.025232095271348953\n",
      "step 6780 loss 0.02523064613342285\n",
      "step 6781 loss 0.02522919699549675\n",
      "step 6782 loss 0.025227751582860947\n",
      "step 6783 loss 0.025226306170225143\n",
      "step 6784 loss 0.025224857032299042\n",
      "step 6785 loss 0.025223417207598686\n",
      "step 6786 loss 0.025221968069672585\n",
      "step 6787 loss 0.02522052824497223\n",
      "step 6788 loss 0.02521909773349762\n",
      "step 6789 loss 0.02521764300763607\n",
      "step 6790 loss 0.02521621622145176\n",
      "step 6791 loss 0.025214776396751404\n",
      "step 6792 loss 0.02521332912147045\n",
      "step 6793 loss 0.02521185204386711\n",
      "step 6794 loss 0.025210337713360786\n",
      "step 6795 loss 0.025208856910467148\n",
      "step 6796 loss 0.025207342579960823\n",
      "step 6797 loss 0.02520585060119629\n",
      "step 6798 loss 0.02520434372127056\n",
      "step 6799 loss 0.02520284429192543\n",
      "step 6800 loss 0.025201329961419106\n",
      "step 6801 loss 0.02519981563091278\n",
      "step 6802 loss 0.025198297575116158\n",
      "step 6803 loss 0.025196800008416176\n",
      "step 6804 loss 0.025195278227329254\n",
      "step 6805 loss 0.025193769484758377\n",
      "step 6806 loss 0.025192270055413246\n",
      "step 6807 loss 0.02519076317548752\n",
      "step 6808 loss 0.025189261883497238\n",
      "step 6809 loss 0.025187745690345764\n",
      "step 6810 loss 0.025186238810420036\n",
      "step 6811 loss 0.02518474869430065\n",
      "step 6812 loss 0.02518322877585888\n",
      "step 6813 loss 0.025181712582707405\n",
      "step 6814 loss 0.02518022619187832\n",
      "step 6815 loss 0.025178732350468636\n",
      "step 6816 loss 0.025177232921123505\n",
      "step 6817 loss 0.025175724178552628\n",
      "step 6818 loss 0.02517424151301384\n",
      "step 6819 loss 0.02517273835837841\n",
      "step 6820 loss 0.025171231478452682\n",
      "step 6821 loss 0.025169745087623596\n",
      "step 6822 loss 0.02516825497150421\n",
      "step 6823 loss 0.025166766718029976\n",
      "step 6824 loss 0.025165285915136337\n",
      "step 6825 loss 0.025163797661662102\n",
      "step 6826 loss 0.025162316858768463\n",
      "step 6827 loss 0.02516084350645542\n",
      "step 6828 loss 0.025159349665045738\n",
      "step 6829 loss 0.025157880038022995\n",
      "step 6830 loss 0.025156397372484207\n",
      "step 6831 loss 0.02515491470694542\n",
      "step 6832 loss 0.02515343576669693\n",
      "step 6833 loss 0.02515195868909359\n",
      "step 6834 loss 0.02515050582587719\n",
      "step 6835 loss 0.025149032473564148\n",
      "step 6836 loss 0.025147564709186554\n",
      "step 6837 loss 0.025146102532744408\n",
      "step 6838 loss 0.025144629180431366\n",
      "step 6839 loss 0.025143178179860115\n",
      "step 6840 loss 0.02514171414077282\n",
      "step 6841 loss 0.02514026127755642\n",
      "step 6842 loss 0.025138797238469124\n",
      "step 6843 loss 0.025137340649962425\n",
      "step 6844 loss 0.025135885924100876\n",
      "step 6845 loss 0.02513444423675537\n",
      "step 6846 loss 0.02513299509882927\n",
      "step 6847 loss 0.025131558999419212\n",
      "step 6848 loss 0.025130094960331917\n",
      "step 6849 loss 0.02512865699827671\n",
      "step 6850 loss 0.025127215310931206\n",
      "step 6851 loss 0.025125769898295403\n",
      "step 6852 loss 0.02512432634830475\n",
      "step 6853 loss 0.02512289583683014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 6854 loss 0.02512146532535553\n",
      "step 6855 loss 0.025120044127106667\n",
      "step 6856 loss 0.025118594989180565\n",
      "step 6857 loss 0.02511714957654476\n",
      "step 6858 loss 0.0251157283782959\n",
      "step 6859 loss 0.02511429972946644\n",
      "step 6860 loss 0.02511286549270153\n",
      "step 6861 loss 0.025111442431807518\n",
      "step 6862 loss 0.025110021233558655\n",
      "step 6863 loss 0.025108611211180687\n",
      "step 6864 loss 0.025107188150286674\n",
      "step 6865 loss 0.025105763226747513\n",
      "step 6866 loss 0.025104355067014694\n",
      "step 6867 loss 0.025102945044636726\n",
      "step 6868 loss 0.02510153129696846\n",
      "step 6869 loss 0.025100117549300194\n",
      "step 6870 loss 0.02509869821369648\n",
      "step 6871 loss 0.02509731613099575\n",
      "step 6872 loss 0.025095896795392036\n",
      "step 6873 loss 0.025094499811530113\n",
      "step 6874 loss 0.02509310282766819\n",
      "step 6875 loss 0.025091689079999924\n",
      "step 6876 loss 0.025090299546718597\n",
      "step 6877 loss 0.025088904425501823\n",
      "step 6878 loss 0.025087499991059303\n",
      "step 6879 loss 0.02508610300719738\n",
      "step 6880 loss 0.02508472464978695\n",
      "step 6881 loss 0.025083322077989578\n",
      "step 6882 loss 0.025081932544708252\n",
      "step 6883 loss 0.025080552324652672\n",
      "step 6884 loss 0.02507915534079075\n",
      "step 6885 loss 0.025077784433960915\n",
      "step 6886 loss 0.025076404213905334\n",
      "step 6887 loss 0.025075020268559456\n",
      "step 6888 loss 0.025073647499084473\n",
      "step 6889 loss 0.025072254240512848\n",
      "step 6890 loss 0.02507087029516697\n",
      "step 6891 loss 0.025069499388337135\n",
      "step 6892 loss 0.02506813034415245\n",
      "step 6893 loss 0.02506675198674202\n",
      "step 6894 loss 0.02506539411842823\n",
      "step 6895 loss 0.0250640157610178\n",
      "step 6896 loss 0.02506265603005886\n",
      "step 6897 loss 0.025061283260583878\n",
      "step 6898 loss 0.025059927254915237\n",
      "step 6899 loss 0.02505854330956936\n",
      "step 6900 loss 0.025057194754481316\n",
      "step 6901 loss 0.025055833160877228\n",
      "step 6902 loss 0.02505449205636978\n",
      "step 6903 loss 0.025053132325410843\n",
      "step 6904 loss 0.025051778182387352\n",
      "step 6905 loss 0.025050412863492966\n",
      "step 6906 loss 0.02504906617105007\n",
      "step 6907 loss 0.02504773437976837\n",
      "step 6908 loss 0.025046367198228836\n",
      "step 6909 loss 0.02504502795636654\n",
      "step 6910 loss 0.025043688714504242\n",
      "step 6911 loss 0.025042332708835602\n",
      "step 6912 loss 0.0250410083681345\n",
      "step 6913 loss 0.025039654225111008\n",
      "step 6914 loss 0.025038324296474457\n",
      "step 6915 loss 0.025036975741386414\n",
      "step 6916 loss 0.025035634636878967\n",
      "step 6917 loss 0.02503431774675846\n",
      "step 6918 loss 0.02503299154341221\n",
      "step 6919 loss 0.025031642988324165\n",
      "step 6920 loss 0.02503032051026821\n",
      "step 6921 loss 0.02502899244427681\n",
      "step 6922 loss 0.025027649477124214\n",
      "step 6923 loss 0.02502633072435856\n",
      "step 6924 loss 0.0250250156968832\n",
      "step 6925 loss 0.0250236839056015\n",
      "step 6926 loss 0.025022368878126144\n",
      "step 6927 loss 0.02502104453742504\n",
      "step 6928 loss 0.02501973696053028\n",
      "step 6929 loss 0.025018412619829178\n",
      "step 6930 loss 0.025017092004418373\n",
      "step 6931 loss 0.025015778839588165\n",
      "step 6932 loss 0.025014467537403107\n",
      "step 6933 loss 0.025013156235218048\n",
      "step 6934 loss 0.025011863559484482\n",
      "step 6935 loss 0.02501055784523487\n",
      "step 6936 loss 0.02500924840569496\n",
      "step 6937 loss 0.025007935240864754\n",
      "step 6938 loss 0.02500663325190544\n",
      "step 6939 loss 0.025005316361784935\n",
      "step 6940 loss 0.025004034861922264\n",
      "step 6941 loss 0.02500273287296295\n",
      "step 6942 loss 0.025001434609293938\n",
      "step 6943 loss 0.025000136345624924\n",
      "step 6944 loss 0.024998845532536507\n",
      "step 6945 loss 0.024997547268867493\n",
      "step 6946 loss 0.02499624714255333\n",
      "step 6947 loss 0.024994954466819763\n",
      "step 6948 loss 0.024993691593408585\n",
      "step 6949 loss 0.02499239519238472\n",
      "step 6950 loss 0.024991106241941452\n",
      "step 6951 loss 0.024989815428853035\n",
      "step 6952 loss 0.024988537654280663\n",
      "step 6953 loss 0.02498726174235344\n",
      "step 6954 loss 0.02498597465455532\n",
      "step 6955 loss 0.024984702467918396\n",
      "step 6956 loss 0.02498341165482998\n",
      "step 6957 loss 0.0249821525067091\n",
      "step 6958 loss 0.024980859830975533\n",
      "step 6959 loss 0.02497958578169346\n",
      "step 6960 loss 0.02497832290828228\n",
      "step 6961 loss 0.02497703582048416\n",
      "step 6962 loss 0.02497577853500843\n",
      "step 6963 loss 0.024974515661597252\n",
      "step 6964 loss 0.024973245337605476\n",
      "step 6965 loss 0.024971989914774895\n",
      "step 6966 loss 0.02497072145342827\n",
      "step 6967 loss 0.024969449266791344\n",
      "step 6968 loss 0.024968182668089867\n",
      "step 6969 loss 0.02496694028377533\n",
      "step 6970 loss 0.024965666234493256\n",
      "step 6971 loss 0.024964410811662674\n",
      "step 6972 loss 0.024963155388832092\n",
      "step 6973 loss 0.02496190555393696\n",
      "step 6974 loss 0.024960651993751526\n",
      "step 6975 loss 0.024959394708275795\n",
      "step 6976 loss 0.02495814859867096\n",
      "step 6977 loss 0.02495688758790493\n",
      "step 6978 loss 0.024955658242106438\n",
      "step 6979 loss 0.024954402819275856\n",
      "step 6980 loss 0.02495315857231617\n",
      "step 6981 loss 0.024951916188001633\n",
      "step 6982 loss 0.024950673803687096\n",
      "step 6983 loss 0.02494943141937256\n",
      "step 6984 loss 0.024948187172412872\n",
      "step 6985 loss 0.02494695596396923\n",
      "step 6986 loss 0.024945730343461037\n",
      "step 6987 loss 0.024944476783275604\n",
      "step 6988 loss 0.02494325116276741\n",
      "step 6989 loss 0.02494201622903347\n",
      "step 6990 loss 0.024940786883234978\n",
      "step 6991 loss 0.024939551949501038\n",
      "step 6992 loss 0.024938318878412247\n",
      "step 6993 loss 0.024937110021710396\n",
      "step 6994 loss 0.024935869500041008\n",
      "step 6995 loss 0.02493465133011341\n",
      "step 6996 loss 0.02493341453373432\n",
      "step 6997 loss 0.02493220381438732\n",
      "step 6998 loss 0.024930963292717934\n",
      "step 6999 loss 0.024929756298661232\n",
      "step 7000 loss 0.024928541854023933\n",
      "step 7001 loss 0.024927319958806038\n",
      "step 7002 loss 0.024926098063588142\n",
      "step 7003 loss 0.024924883618950844\n",
      "step 7004 loss 0.0249236561357975\n",
      "step 7005 loss 0.02492244727909565\n",
      "step 7006 loss 0.024921244010329247\n",
      "step 7007 loss 0.024920029565691948\n",
      "step 7008 loss 0.024918818846344948\n",
      "step 7009 loss 0.02491762675344944\n",
      "step 7010 loss 0.02491641230881214\n",
      "step 7011 loss 0.02491520345211029\n",
      "step 7012 loss 0.024914009496569633\n",
      "step 7013 loss 0.024912796914577484\n",
      "step 7014 loss 0.02491157315671444\n",
      "step 7015 loss 0.02491038292646408\n",
      "step 7016 loss 0.024909188970923424\n",
      "step 7017 loss 0.024908000603318214\n",
      "step 7018 loss 0.024906795471906662\n",
      "step 7019 loss 0.02490559034049511\n",
      "step 7020 loss 0.024904398247599602\n",
      "step 7021 loss 0.02490321546792984\n",
      "step 7022 loss 0.02490202896296978\n",
      "step 7023 loss 0.024900827556848526\n",
      "step 7024 loss 0.02489965409040451\n",
      "step 7025 loss 0.024898461997509003\n",
      "step 7026 loss 0.024897266179323196\n",
      "step 7027 loss 0.024896077811717987\n",
      "step 7028 loss 0.02489488571882248\n",
      "step 7029 loss 0.02489371784031391\n",
      "step 7030 loss 0.02489253133535385\n",
      "step 7031 loss 0.02489134855568409\n",
      "step 7032 loss 0.02489016205072403\n",
      "step 7033 loss 0.024889007210731506\n",
      "step 7034 loss 0.024887816980481148\n",
      "step 7035 loss 0.02488662488758564\n",
      "step 7036 loss 0.02488545887172222\n",
      "step 7037 loss 0.024884281679987907\n",
      "step 7038 loss 0.024883128702640533\n",
      "step 7039 loss 0.024882007390260696\n",
      "step 7040 loss 0.024880917742848396\n",
      "step 7041 loss 0.02487984113395214\n",
      "step 7042 loss 0.024878768250346184\n",
      "step 7043 loss 0.02487771585583687\n",
      "step 7044 loss 0.024876659736037254\n",
      "step 7045 loss 0.02487560734152794\n",
      "step 7046 loss 0.02487456053495407\n",
      "step 7047 loss 0.024873537942767143\n",
      "step 7048 loss 0.024872491136193275\n",
      "step 7049 loss 0.0248714592307806\n",
      "step 7050 loss 0.02487042360007763\n",
      "step 7051 loss 0.024869417771697044\n",
      "step 7052 loss 0.024868378415703773\n",
      "step 7053 loss 0.024867350235581398\n",
      "step 7054 loss 0.02486635372042656\n",
      "step 7055 loss 0.02486533112823963\n",
      "step 7056 loss 0.02486429736018181\n",
      "step 7057 loss 0.02486329711973667\n",
      "step 7058 loss 0.024862289428710938\n",
      "step 7059 loss 0.024861274287104607\n",
      "step 7060 loss 0.024860287085175514\n",
      "step 7061 loss 0.024859266355633736\n",
      "step 7062 loss 0.024858271703124046\n",
      "step 7063 loss 0.024857264012098312\n",
      "step 7064 loss 0.02485628053545952\n",
      "step 7065 loss 0.02485528402030468\n",
      "step 7066 loss 0.024854278191924095\n",
      "step 7067 loss 0.024853281676769257\n",
      "step 7068 loss 0.024852318689227104\n",
      "step 7069 loss 0.02485131099820137\n",
      "step 7070 loss 0.024850327521562576\n",
      "step 7071 loss 0.024849344044923782\n",
      "step 7072 loss 0.024848364293575287\n",
      "step 7073 loss 0.024847369641065598\n",
      "step 7074 loss 0.02484639175236225\n",
      "step 7075 loss 0.024845389649271965\n",
      "step 7076 loss 0.024844419211149216\n",
      "step 7077 loss 0.024843422695994377\n",
      "step 7078 loss 0.02484244666993618\n",
      "step 7079 loss 0.02484147995710373\n",
      "step 7080 loss 0.024840515106916428\n",
      "step 7081 loss 0.02483951486647129\n",
      "step 7082 loss 0.024838538840413094\n",
      "step 7083 loss 0.02483757585287094\n",
      "step 7084 loss 0.024836605414748192\n",
      "step 7085 loss 0.024835623800754547\n",
      "step 7086 loss 0.024834660813212395\n",
      "step 7087 loss 0.024833688512444496\n",
      "step 7088 loss 0.024832725524902344\n",
      "step 7089 loss 0.02483176440000534\n",
      "step 7090 loss 0.024830786511301994\n",
      "step 7091 loss 0.024829821661114693\n",
      "step 7092 loss 0.024828849360346794\n",
      "step 7093 loss 0.024827880784869194\n",
      "step 7094 loss 0.02482692524790764\n",
      "step 7095 loss 0.024825962260365486\n",
      "step 7096 loss 0.02482501044869423\n",
      "step 7097 loss 0.024824045598506927\n",
      "step 7098 loss 0.024823080748319626\n",
      "step 7099 loss 0.024822132661938667\n",
      "step 7100 loss 0.024821152910590172\n",
      "step 7101 loss 0.024820098653435707\n",
      "step 7102 loss 0.024819038808345795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 7103 loss 0.024817928671836853\n",
      "step 7104 loss 0.02481684647500515\n",
      "step 7105 loss 0.024815740063786507\n",
      "step 7106 loss 0.024814631789922714\n",
      "step 7107 loss 0.02481350675225258\n",
      "step 7108 loss 0.024812381714582443\n",
      "step 7109 loss 0.024811243638396263\n",
      "step 7110 loss 0.024810118600726128\n",
      "step 7111 loss 0.024808984249830246\n",
      "step 7112 loss 0.02480783686041832\n",
      "step 7113 loss 0.024806689471006393\n",
      "step 7114 loss 0.024805564433336258\n",
      "step 7115 loss 0.024804405868053436\n",
      "step 7116 loss 0.024803269654512405\n",
      "step 7117 loss 0.024802112951874733\n",
      "step 7118 loss 0.024800973013043404\n",
      "step 7119 loss 0.024799823760986328\n",
      "step 7120 loss 0.024798674508929253\n",
      "step 7121 loss 0.024797525256872177\n",
      "step 7122 loss 0.0247963760048151\n",
      "step 7123 loss 0.02479521743953228\n",
      "step 7124 loss 0.0247940793633461\n",
      "step 7125 loss 0.024792928248643875\n",
      "step 7126 loss 0.0247917789965868\n",
      "step 7127 loss 0.024790635332465172\n",
      "step 7128 loss 0.02478947676718235\n",
      "step 7129 loss 0.024788346141576767\n",
      "step 7130 loss 0.024787193164229393\n",
      "step 7131 loss 0.02478603646159172\n",
      "step 7132 loss 0.024784903973340988\n",
      "step 7133 loss 0.024783752858638763\n",
      "step 7134 loss 0.02478262595832348\n",
      "step 7135 loss 0.0247814804315567\n",
      "step 7136 loss 0.02478032559156418\n",
      "step 7137 loss 0.024779194965958595\n",
      "step 7138 loss 0.02477804571390152\n",
      "step 7139 loss 0.02477692998945713\n",
      "step 7140 loss 0.02477579191327095\n",
      "step 7141 loss 0.02477465569972992\n",
      "step 7142 loss 0.024773521348834038\n",
      "step 7143 loss 0.024772388860583305\n",
      "step 7144 loss 0.02477126568555832\n",
      "step 7145 loss 0.024770129472017288\n",
      "step 7146 loss 0.024769015610218048\n",
      "step 7147 loss 0.024767881259322166\n",
      "step 7148 loss 0.02476675994694233\n",
      "step 7149 loss 0.02476564235985279\n",
      "step 7150 loss 0.02476450428366661\n",
      "step 7151 loss 0.02476338855922222\n",
      "step 7152 loss 0.024762267246842384\n",
      "step 7153 loss 0.024761172011494637\n",
      "step 7154 loss 0.024760032072663307\n",
      "step 7155 loss 0.02475893683731556\n",
      "step 7156 loss 0.024757813662290573\n",
      "step 7157 loss 0.024756712839007378\n",
      "step 7158 loss 0.02475559711456299\n",
      "step 7159 loss 0.02475447952747345\n",
      "step 7160 loss 0.024753374978899956\n",
      "step 7161 loss 0.024752268567681313\n",
      "step 7162 loss 0.024751154705882072\n",
      "step 7163 loss 0.024750055745244026\n",
      "step 7164 loss 0.024748945608735085\n",
      "step 7165 loss 0.024747850373387337\n",
      "step 7166 loss 0.024746742099523544\n",
      "step 7167 loss 0.024745652452111244\n",
      "step 7168 loss 0.024744555354118347\n",
      "step 7169 loss 0.02474345453083515\n",
      "step 7170 loss 0.024742359295487404\n",
      "step 7171 loss 0.024741273373365402\n",
      "step 7172 loss 0.02474016696214676\n",
      "step 7173 loss 0.0247390978038311\n",
      "step 7174 loss 0.024738002568483353\n",
      "step 7175 loss 0.0247369185090065\n",
      "step 7176 loss 0.024735823273658752\n",
      "step 7177 loss 0.024734744802117348\n",
      "step 7178 loss 0.024733668193221092\n",
      "step 7179 loss 0.02473258413374424\n",
      "step 7180 loss 0.024731511250138283\n",
      "step 7181 loss 0.02473042532801628\n",
      "step 7182 loss 0.024729344993829727\n",
      "step 7183 loss 0.02472827583551407\n",
      "step 7184 loss 0.024727197363972664\n",
      "step 7185 loss 0.024726124480366707\n",
      "step 7186 loss 0.024725042283535004\n",
      "step 7187 loss 0.024723978713154793\n",
      "step 7188 loss 0.024722902104258537\n",
      "step 7189 loss 0.02472182735800743\n",
      "step 7190 loss 0.024720769375562668\n",
      "step 7191 loss 0.024719703942537308\n",
      "step 7192 loss 0.0247186366468668\n",
      "step 7193 loss 0.024717584252357483\n",
      "step 7194 loss 0.024716470390558243\n",
      "step 7195 loss 0.024715369567275047\n",
      "step 7196 loss 0.02471424639225006\n",
      "step 7197 loss 0.024713149294257164\n",
      "step 7198 loss 0.02471204288303852\n",
      "step 7199 loss 0.02471093088388443\n",
      "step 7200 loss 0.024709803983569145\n",
      "step 7201 loss 0.02470869943499565\n",
      "step 7202 loss 0.024707600474357605\n",
      "step 7203 loss 0.024706456810235977\n",
      "step 7204 loss 0.02470535784959793\n",
      "step 7205 loss 0.024704234674572945\n",
      "step 7206 loss 0.024703139439225197\n",
      "step 7207 loss 0.024702010676264763\n",
      "step 7208 loss 0.024700913578271866\n",
      "step 7209 loss 0.024699795991182327\n",
      "step 7210 loss 0.024698689579963684\n",
      "step 7211 loss 0.024697573855519295\n",
      "step 7212 loss 0.02469644322991371\n",
      "step 7213 loss 0.024695347994565964\n",
      "step 7214 loss 0.024694232270121574\n",
      "step 7215 loss 0.024693137034773827\n",
      "step 7216 loss 0.02469204179942608\n",
      "step 7217 loss 0.024690920487046242\n",
      "step 7218 loss 0.024689821526408195\n",
      "step 7219 loss 0.024688713252544403\n",
      "step 7220 loss 0.024687623605132103\n",
      "step 7221 loss 0.02468651719391346\n",
      "step 7222 loss 0.02468542568385601\n",
      "step 7223 loss 0.02468431554734707\n",
      "step 7224 loss 0.024683179333806038\n",
      "step 7225 loss 0.02468196675181389\n",
      "step 7226 loss 0.02468067593872547\n",
      "step 7227 loss 0.024679331108927727\n",
      "step 7228 loss 0.024677934125065804\n",
      "step 7229 loss 0.024676501750946045\n",
      "step 7230 loss 0.024675002321600914\n",
      "step 7231 loss 0.024673499166965485\n",
      "step 7232 loss 0.02467196062207222\n",
      "step 7233 loss 0.02467038854956627\n",
      "step 7234 loss 0.02466881275177002\n",
      "step 7235 loss 0.02466721460223198\n",
      "step 7236 loss 0.024665579199790955\n",
      "step 7237 loss 0.02466396801173687\n",
      "step 7238 loss 0.0246623232960701\n",
      "step 7239 loss 0.02466069906949997\n",
      "step 7240 loss 0.02465907111763954\n",
      "step 7241 loss 0.024657439440488815\n",
      "step 7242 loss 0.024655790999531746\n",
      "step 7243 loss 0.024654168635606766\n",
      "step 7244 loss 0.024652542546391487\n",
      "step 7245 loss 0.024650922045111656\n",
      "step 7246 loss 0.024649305269122124\n",
      "step 7247 loss 0.024647701531648636\n",
      "step 7248 loss 0.024646105244755745\n",
      "step 7249 loss 0.024644512683153152\n",
      "step 7250 loss 0.024642936885356903\n",
      "step 7251 loss 0.024641375988721848\n",
      "step 7252 loss 0.024639807641506195\n",
      "step 7253 loss 0.024638252332806587\n",
      "step 7254 loss 0.02463671751320362\n",
      "step 7255 loss 0.0246351957321167\n",
      "step 7256 loss 0.024633651599287987\n",
      "step 7257 loss 0.024632170796394348\n",
      "step 7258 loss 0.024630658328533173\n",
      "step 7259 loss 0.02462918311357498\n",
      "step 7260 loss 0.024627692997455597\n",
      "step 7261 loss 0.024626221507787704\n",
      "step 7262 loss 0.024624772369861603\n",
      "step 7263 loss 0.024623312056064606\n",
      "step 7264 loss 0.024621881544589996\n",
      "step 7265 loss 0.024620449170470238\n",
      "step 7266 loss 0.024619026109576225\n",
      "step 7267 loss 0.024617623537778854\n",
      "step 7268 loss 0.024616217240691185\n",
      "step 7269 loss 0.024614810943603516\n",
      "step 7270 loss 0.024613432586193085\n",
      "step 7271 loss 0.024612069129943848\n",
      "step 7272 loss 0.02461068145930767\n",
      "step 7273 loss 0.024609319865703583\n",
      "step 7274 loss 0.024607978761196136\n",
      "step 7275 loss 0.024606622755527496\n",
      "step 7276 loss 0.0246052797883749\n",
      "step 7277 loss 0.02460394613444805\n",
      "step 7278 loss 0.024602623656392097\n",
      "step 7279 loss 0.024601321667432785\n",
      "step 7280 loss 0.024600012227892876\n",
      "step 7281 loss 0.02459868974983692\n",
      "step 7282 loss 0.02459738962352276\n",
      "step 7283 loss 0.024596095085144043\n",
      "step 7284 loss 0.024594807997345924\n",
      "step 7285 loss 0.024593520909547806\n",
      "step 7286 loss 0.024592246860265732\n",
      "step 7287 loss 0.024590976536273956\n",
      "step 7288 loss 0.024589719250798225\n",
      "step 7289 loss 0.024588439613580704\n",
      "step 7290 loss 0.024587202817201614\n",
      "step 7291 loss 0.024585939943790436\n",
      "step 7292 loss 0.0245846938341856\n",
      "step 7293 loss 0.024583466351032257\n",
      "step 7294 loss 0.024582214653491974\n",
      "step 7295 loss 0.024580974131822586\n",
      "step 7296 loss 0.024579748511314392\n",
      "step 7297 loss 0.024578504264354706\n",
      "step 7298 loss 0.0245773047208786\n",
      "step 7299 loss 0.024576082825660706\n",
      "step 7300 loss 0.024574870243668556\n",
      "step 7301 loss 0.024573668837547302\n",
      "step 7302 loss 0.024572238326072693\n",
      "step 7303 loss 0.02457071840763092\n",
      "step 7304 loss 0.024569161236286163\n",
      "step 7305 loss 0.024567553773522377\n",
      "step 7306 loss 0.024565907195210457\n",
      "step 7307 loss 0.024564223363995552\n",
      "step 7308 loss 0.024562522768974304\n",
      "step 7309 loss 0.024560775607824326\n",
      "step 7310 loss 0.0245590228587389\n",
      "step 7311 loss 0.02455725707113743\n",
      "step 7312 loss 0.024555476382374763\n",
      "step 7313 loss 0.02455366775393486\n",
      "step 7314 loss 0.024551862850785255\n",
      "step 7315 loss 0.0245500560849905\n",
      "step 7316 loss 0.0245482437312603\n",
      "step 7317 loss 0.024546422064304352\n",
      "step 7318 loss 0.024544596672058105\n",
      "step 7319 loss 0.02454279735684395\n",
      "step 7320 loss 0.024540968239307404\n",
      "step 7321 loss 0.024539170786738396\n",
      "step 7322 loss 0.024537356570363045\n",
      "step 7323 loss 0.024535570293664932\n",
      "step 7324 loss 0.024533763527870178\n",
      "step 7325 loss 0.024531979113817215\n",
      "step 7326 loss 0.0245302002876997\n",
      "step 7327 loss 0.02452843263745308\n",
      "step 7328 loss 0.02452666684985161\n",
      "step 7329 loss 0.024524902924895287\n",
      "step 7330 loss 0.02452317625284195\n",
      "step 7331 loss 0.02452143281698227\n",
      "step 7332 loss 0.024519696831703186\n",
      "step 7333 loss 0.02451799064874649\n",
      "step 7334 loss 0.024516262114048004\n",
      "step 7335 loss 0.024514582008123398\n",
      "step 7336 loss 0.02451290935277939\n",
      "step 7337 loss 0.024511218070983887\n",
      "step 7338 loss 0.024509547278285027\n",
      "step 7339 loss 0.02450789511203766\n",
      "step 7340 loss 0.02450624480843544\n",
      "step 7341 loss 0.024504590779542923\n",
      "step 7342 loss 0.024502964690327644\n",
      "step 7343 loss 0.02450135163962841\n",
      "step 7344 loss 0.024499736726284027\n",
      "step 7345 loss 0.024498114362359047\n",
      "step 7346 loss 0.024496544152498245\n",
      "step 7347 loss 0.024494938552379608\n",
      "step 7348 loss 0.024493355304002762\n",
      "step 7349 loss 0.024491796270012856\n",
      "step 7350 loss 0.024490246549248695\n",
      "step 7351 loss 0.024488696828484535\n",
      "step 7352 loss 0.024487122893333435\n",
      "step 7353 loss 0.02448561228811741\n",
      "step 7354 loss 0.024484075605869293\n",
      "step 7355 loss 0.02448255568742752\n",
      "step 7356 loss 0.024481048807501793\n",
      "step 7357 loss 0.024479523301124573\n",
      "step 7358 loss 0.024478033185005188\n",
      "step 7359 loss 0.024476533755660057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 7360 loss 0.02447505295276642\n",
      "step 7361 loss 0.024473562836647034\n",
      "step 7362 loss 0.024472098797559738\n",
      "step 7363 loss 0.024470625445246696\n",
      "step 7364 loss 0.0244691651314497\n",
      "step 7365 loss 0.024467715993523598\n",
      "step 7366 loss 0.0244662594050169\n",
      "step 7367 loss 0.024464817717671394\n",
      "step 7368 loss 0.024463387206196785\n",
      "step 7369 loss 0.024461962282657623\n",
      "step 7370 loss 0.024460529908537865\n",
      "step 7371 loss 0.02445909194648266\n",
      "step 7372 loss 0.02445768378674984\n",
      "step 7373 loss 0.024456270039081573\n",
      "step 7374 loss 0.024454860016703606\n",
      "step 7375 loss 0.02445346862077713\n",
      "step 7376 loss 0.02445206604897976\n",
      "step 7377 loss 0.024450676515698433\n",
      "step 7378 loss 0.024449294432997704\n",
      "step 7379 loss 0.024447916075587273\n",
      "step 7380 loss 0.024446547031402588\n",
      "step 7381 loss 0.02444516122341156\n",
      "step 7382 loss 0.024443797767162323\n",
      "step 7383 loss 0.024442443624138832\n",
      "step 7384 loss 0.024441078305244446\n",
      "step 7385 loss 0.024439727887511253\n",
      "step 7386 loss 0.02443837560713291\n",
      "step 7387 loss 0.02443702705204487\n",
      "step 7388 loss 0.024435684084892273\n",
      "step 7389 loss 0.024434344843029976\n",
      "step 7390 loss 0.02443300187587738\n",
      "step 7391 loss 0.024431686848402023\n",
      "step 7392 loss 0.02443036623299122\n",
      "step 7393 loss 0.02442903071641922\n",
      "step 7394 loss 0.024427717551589012\n",
      "step 7395 loss 0.02442639321088791\n",
      "step 7396 loss 0.024425072595477104\n",
      "step 7397 loss 0.024423770606517792\n",
      "step 7398 loss 0.024422459304332733\n",
      "step 7399 loss 0.024421149864792824\n",
      "step 7400 loss 0.02441985160112381\n",
      "step 7401 loss 0.024418557062745094\n",
      "step 7402 loss 0.024417266249656677\n",
      "step 7403 loss 0.024415967985987663\n",
      "step 7404 loss 0.024414699524641037\n",
      "step 7405 loss 0.024413397535681725\n",
      "step 7406 loss 0.024412130936980247\n",
      "step 7407 loss 0.02441084384918213\n",
      "step 7408 loss 0.02440956048667431\n",
      "step 7409 loss 0.024408288300037384\n",
      "step 7410 loss 0.024407021701335907\n",
      "step 7411 loss 0.024405743926763535\n",
      "step 7412 loss 0.024404481053352356\n",
      "step 7413 loss 0.024403229355812073\n",
      "step 7414 loss 0.024401970207691193\n",
      "step 7415 loss 0.024400705471634865\n",
      "step 7416 loss 0.02439945749938488\n",
      "step 7417 loss 0.024398213252425194\n",
      "step 7418 loss 0.024396950379014015\n",
      "step 7419 loss 0.024395709857344627\n",
      "step 7420 loss 0.02439446933567524\n",
      "step 7421 loss 0.024393219500780106\n",
      "step 7422 loss 0.024391982704401016\n",
      "step 7423 loss 0.024390749633312225\n",
      "step 7424 loss 0.02438950538635254\n",
      "step 7425 loss 0.024388279765844345\n",
      "step 7426 loss 0.024387046694755554\n",
      "step 7427 loss 0.024385815486311913\n",
      "step 7428 loss 0.02438458986580372\n",
      "step 7429 loss 0.02438337542116642\n",
      "step 7430 loss 0.02438213676214218\n",
      "step 7431 loss 0.02438092976808548\n",
      "step 7432 loss 0.02437971904873848\n",
      "step 7433 loss 0.02437848597764969\n",
      "step 7434 loss 0.024377288296818733\n",
      "step 7435 loss 0.024376068264245987\n",
      "step 7436 loss 0.02437487058341503\n",
      "step 7437 loss 0.02437365986406803\n",
      "step 7438 loss 0.024372467771172523\n",
      "step 7439 loss 0.024371251463890076\n",
      "step 7440 loss 0.024370059370994568\n",
      "step 7441 loss 0.024368857964873314\n",
      "step 7442 loss 0.02436765842139721\n",
      "step 7443 loss 0.024366470053792\n",
      "step 7444 loss 0.024365268647670746\n",
      "step 7445 loss 0.024364087730646133\n",
      "step 7446 loss 0.02436288446187973\n",
      "step 7447 loss 0.02436172030866146\n",
      "step 7448 loss 0.024360522627830505\n",
      "step 7449 loss 0.02435934543609619\n",
      "step 7450 loss 0.02435815893113613\n",
      "step 7451 loss 0.024356985464692116\n",
      "step 7452 loss 0.024355793371796608\n",
      "step 7453 loss 0.02435462921857834\n",
      "step 7454 loss 0.02435343898832798\n",
      "step 7455 loss 0.02435228042304516\n",
      "step 7456 loss 0.024351103231310844\n",
      "step 7457 loss 0.02434995397925377\n",
      "step 7458 loss 0.024348769336938858\n",
      "step 7459 loss 0.02434762567281723\n",
      "step 7460 loss 0.024346446618437767\n",
      "step 7461 loss 0.0243452787399292\n",
      "step 7462 loss 0.024344120174646378\n",
      "step 7463 loss 0.02434295229613781\n",
      "step 7464 loss 0.02434181049466133\n",
      "step 7465 loss 0.02434064820408821\n",
      "step 7466 loss 0.02433951385319233\n",
      "step 7467 loss 0.024338340386748314\n",
      "step 7468 loss 0.02433718740940094\n",
      "step 7469 loss 0.02433605119585991\n",
      "step 7470 loss 0.02433484047651291\n",
      "step 7471 loss 0.02433362416923046\n",
      "step 7472 loss 0.024332396686077118\n",
      "step 7473 loss 0.024331163614988327\n",
      "step 7474 loss 0.024329937994480133\n",
      "step 7475 loss 0.024328703060746193\n",
      "step 7476 loss 0.024327455088496208\n",
      "step 7477 loss 0.024326222017407417\n",
      "step 7478 loss 0.024324985221028328\n",
      "step 7479 loss 0.024323729798197746\n",
      "step 7480 loss 0.024322491139173508\n",
      "step 7481 loss 0.024321241304278374\n",
      "step 7482 loss 0.024319997057318687\n",
      "step 7483 loss 0.024318752810359\n",
      "step 7484 loss 0.024317512288689613\n",
      "step 7485 loss 0.024316253140568733\n",
      "step 7486 loss 0.024315018206834793\n",
      "step 7487 loss 0.024313753470778465\n",
      "step 7488 loss 0.024312520399689674\n",
      "step 7489 loss 0.024311266839504242\n",
      "step 7490 loss 0.024310018867254257\n",
      "step 7491 loss 0.02430877462029457\n",
      "step 7492 loss 0.024307534098625183\n",
      "step 7493 loss 0.024306291714310646\n",
      "step 7494 loss 0.02430504746735096\n",
      "step 7495 loss 0.024303819984197617\n",
      "step 7496 loss 0.02430257759988308\n",
      "step 7497 loss 0.02430134266614914\n",
      "step 7498 loss 0.02430010959506035\n",
      "step 7499 loss 0.024298863485455513\n",
      "step 7500 loss 0.02429763786494732\n",
      "step 7501 loss 0.024296408519148827\n",
      "step 7502 loss 0.024295158684253693\n",
      "step 7503 loss 0.024293944239616394\n",
      "step 7504 loss 0.0242927186191082\n",
      "step 7505 loss 0.024291491135954857\n",
      "step 7506 loss 0.024290265515446663\n",
      "step 7507 loss 0.02428903989493847\n",
      "step 7508 loss 0.02428782917559147\n",
      "step 7509 loss 0.024286599829792976\n",
      "step 7510 loss 0.024285387247800827\n",
      "step 7511 loss 0.024284161627292633\n",
      "step 7512 loss 0.02428295463323593\n",
      "step 7513 loss 0.024281730875372887\n",
      "step 7514 loss 0.024280523881316185\n",
      "step 7515 loss 0.02427932247519493\n",
      "step 7516 loss 0.024278121069073677\n",
      "step 7517 loss 0.024276910349726677\n",
      "step 7518 loss 0.02427568845450878\n",
      "step 7519 loss 0.024274500086903572\n",
      "step 7520 loss 0.02427329309284687\n",
      "step 7521 loss 0.024272091686725616\n",
      "step 7522 loss 0.02427089214324951\n",
      "step 7523 loss 0.024269675835967064\n",
      "step 7524 loss 0.024268489331007004\n",
      "step 7525 loss 0.02426729165017605\n",
      "step 7526 loss 0.024266114458441734\n",
      "step 7527 loss 0.024264926090836525\n",
      "step 7528 loss 0.024263722822070122\n",
      "step 7529 loss 0.024262478575110435\n",
      "step 7530 loss 0.02426123060286045\n",
      "step 7531 loss 0.02425997704267502\n",
      "step 7532 loss 0.024258732795715332\n",
      "step 7533 loss 0.024257464334368706\n",
      "step 7534 loss 0.024256212636828423\n",
      "step 7535 loss 0.02425495535135269\n",
      "step 7536 loss 0.024253692477941513\n",
      "step 7537 loss 0.024252433329820633\n",
      "step 7538 loss 0.024251161143183708\n",
      "step 7539 loss 0.024249911308288574\n",
      "step 7540 loss 0.024248652160167694\n",
      "step 7541 loss 0.024247393012046814\n",
      "step 7542 loss 0.02424613945186138\n",
      "step 7543 loss 0.024244876578450203\n",
      "step 7544 loss 0.024243611842393875\n",
      "step 7545 loss 0.02424236759543419\n",
      "step 7546 loss 0.024241089820861816\n",
      "step 7547 loss 0.02423984557390213\n",
      "step 7548 loss 0.024238592013716698\n",
      "step 7549 loss 0.024237336590886116\n",
      "step 7550 loss 0.024236073717474937\n",
      "step 7551 loss 0.02423483319580555\n",
      "step 7552 loss 0.02423357032239437\n",
      "step 7553 loss 0.024232327938079834\n",
      "step 7554 loss 0.024231068789958954\n",
      "step 7555 loss 0.024229830130934715\n",
      "step 7556 loss 0.024228591471910477\n",
      "step 7557 loss 0.024227332323789597\n",
      "step 7558 loss 0.02422609180212021\n",
      "step 7559 loss 0.02422484941780567\n",
      "step 7560 loss 0.024223601445555687\n",
      "step 7561 loss 0.024222388863563538\n",
      "step 7562 loss 0.024221129715442657\n",
      "step 7563 loss 0.024219896644353867\n",
      "step 7564 loss 0.024218661710619926\n",
      "step 7565 loss 0.02421743795275688\n",
      "step 7566 loss 0.024216189980506897\n",
      "step 7567 loss 0.02421490103006363\n",
      "step 7568 loss 0.02421361394226551\n",
      "step 7569 loss 0.024212300777435303\n",
      "step 7570 loss 0.02421099878847599\n",
      "step 7571 loss 0.02420968934893608\n",
      "step 7572 loss 0.024208378046751022\n",
      "step 7573 loss 0.024207090958952904\n",
      "step 7574 loss 0.024205762892961502\n",
      "step 7575 loss 0.02420445904135704\n",
      "step 7576 loss 0.024203144013881683\n",
      "step 7577 loss 0.02420182153582573\n",
      "step 7578 loss 0.02420051395893097\n",
      "step 7579 loss 0.024199210107326508\n",
      "step 7580 loss 0.02419789880514145\n",
      "step 7581 loss 0.024196576327085495\n",
      "step 7582 loss 0.024195285513997078\n",
      "step 7583 loss 0.02419394999742508\n",
      "step 7584 loss 0.024192657321691513\n",
      "step 7585 loss 0.02419135347008705\n",
      "step 7586 loss 0.024190044030547142\n",
      "step 7587 loss 0.02418873831629753\n",
      "step 7588 loss 0.02418743073940277\n",
      "step 7589 loss 0.02418612316250801\n",
      "step 7590 loss 0.024184823036193848\n",
      "step 7591 loss 0.024183521047234535\n",
      "step 7592 loss 0.024182232096791267\n",
      "step 7593 loss 0.024180922657251358\n",
      "step 7594 loss 0.024179592728614807\n",
      "step 7595 loss 0.024178238585591316\n",
      "step 7596 loss 0.02417690120637417\n",
      "step 7597 loss 0.024175547063350677\n",
      "step 7598 loss 0.024174204096198082\n",
      "step 7599 loss 0.024172859266400337\n",
      "step 7600 loss 0.0241714995354414\n",
      "step 7601 loss 0.024170175194740295\n",
      "step 7602 loss 0.024168817326426506\n",
      "step 7603 loss 0.024167461320757866\n",
      "step 7604 loss 0.02416612207889557\n",
      "step 7605 loss 0.02416478656232357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 7606 loss 0.02416342683136463\n",
      "step 7607 loss 0.02416207827627659\n",
      "step 7608 loss 0.024160748347640038\n",
      "step 7609 loss 0.024159403517842293\n",
      "step 7610 loss 0.024158060550689697\n",
      "step 7611 loss 0.024156734347343445\n",
      "step 7612 loss 0.024155378341674805\n",
      "step 7613 loss 0.024154042825102806\n",
      "step 7614 loss 0.024152712896466255\n",
      "step 7615 loss 0.024151325225830078\n",
      "step 7616 loss 0.024149952456355095\n",
      "step 7617 loss 0.02414855547249317\n",
      "step 7618 loss 0.02414718084037304\n",
      "step 7619 loss 0.02414577081799507\n",
      "step 7620 loss 0.024144383147358894\n",
      "step 7621 loss 0.024142982438206673\n",
      "step 7622 loss 0.024141602218151093\n",
      "step 7623 loss 0.02414020337164402\n",
      "step 7624 loss 0.02413882315158844\n",
      "step 7625 loss 0.024137413129210472\n",
      "step 7626 loss 0.024136021733283997\n",
      "step 7627 loss 0.02413463033735752\n",
      "step 7628 loss 0.024133257567882538\n",
      "step 7629 loss 0.024131856858730316\n",
      "step 7630 loss 0.024130478501319885\n",
      "step 7631 loss 0.02412908710539341\n",
      "step 7632 loss 0.02412768453359604\n",
      "step 7633 loss 0.0241263248026371\n",
      "step 7634 loss 0.02412494271993637\n",
      "step 7635 loss 0.024123555049300194\n",
      "step 7636 loss 0.02412218414247036\n",
      "step 7637 loss 0.024120807647705078\n",
      "step 7638 loss 0.024119429290294647\n",
      "step 7639 loss 0.024118050932884216\n",
      "step 7640 loss 0.02411668747663498\n",
      "step 7641 loss 0.0241153072565794\n",
      "step 7642 loss 0.024113941937685013\n",
      "step 7643 loss 0.024112576618790627\n",
      "step 7644 loss 0.024111200124025345\n",
      "step 7645 loss 0.024109849706292152\n",
      "step 7646 loss 0.02410850115120411\n",
      "step 7647 loss 0.024107126519083977\n",
      "step 7648 loss 0.024105777963995934\n",
      "step 7649 loss 0.02410440519452095\n",
      "step 7650 loss 0.024103064090013504\n",
      "step 7651 loss 0.024101728573441505\n",
      "step 7652 loss 0.02410035766661167\n",
      "step 7653 loss 0.02409902587532997\n",
      "step 7654 loss 0.02409767545759678\n",
      "step 7655 loss 0.024096336215734482\n",
      "step 7656 loss 0.024094996973872185\n",
      "step 7657 loss 0.02409365028142929\n",
      "step 7658 loss 0.02409231662750244\n",
      "step 7659 loss 0.02409099042415619\n",
      "step 7660 loss 0.02408965304493904\n",
      "step 7661 loss 0.024088332429528236\n",
      "step 7662 loss 0.024087000638246536\n",
      "step 7663 loss 0.02408565953373909\n",
      "step 7664 loss 0.024084346368908882\n",
      "step 7665 loss 0.024083023890852928\n",
      "step 7666 loss 0.024081707000732422\n",
      "step 7667 loss 0.02408037893474102\n",
      "step 7668 loss 0.024079065769910812\n",
      "step 7669 loss 0.024077754467725754\n",
      "step 7670 loss 0.024076443165540695\n",
      "step 7671 loss 0.024075133726000786\n",
      "step 7672 loss 0.02407381683588028\n",
      "step 7673 loss 0.024072512984275818\n",
      "step 7674 loss 0.0240712221711874\n",
      "step 7675 loss 0.024069907143712044\n",
      "step 7676 loss 0.02406860515475273\n",
      "step 7677 loss 0.02406730130314827\n",
      "step 7678 loss 0.024066006764769554\n",
      "step 7679 loss 0.024064717814326286\n",
      "step 7680 loss 0.024063417688012123\n",
      "step 7681 loss 0.024062128737568855\n",
      "step 7682 loss 0.02406083047389984\n",
      "step 7683 loss 0.02405954897403717\n",
      "step 7684 loss 0.024058258160948753\n",
      "step 7685 loss 0.024056976661086082\n",
      "step 7686 loss 0.024055708199739456\n",
      "step 7687 loss 0.024054432287812233\n",
      "step 7688 loss 0.024053147062659264\n",
      "step 7689 loss 0.024051886051893234\n",
      "step 7690 loss 0.02405058778822422\n",
      "step 7691 loss 0.02404932491481304\n",
      "step 7692 loss 0.024048062041401863\n",
      "step 7693 loss 0.024046778678894043\n",
      "step 7694 loss 0.024045519530773163\n",
      "step 7695 loss 0.02404424361884594\n",
      "step 7696 loss 0.024042995646595955\n",
      "step 7697 loss 0.02404172718524933\n",
      "step 7698 loss 0.024040462449193\n",
      "step 7699 loss 0.024039220064878464\n",
      "step 7700 loss 0.024037959054112434\n",
      "step 7701 loss 0.024036698043346405\n",
      "step 7702 loss 0.02403544820845127\n",
      "step 7703 loss 0.02403419278562069\n",
      "step 7704 loss 0.0240329522639513\n",
      "step 7705 loss 0.024031683802604675\n",
      "step 7706 loss 0.024030456319451332\n",
      "step 7707 loss 0.02402920462191105\n",
      "step 7708 loss 0.02402796968817711\n",
      "step 7709 loss 0.024026744067668915\n",
      "step 7710 loss 0.02402549237012863\n",
      "step 7711 loss 0.024024253711104393\n",
      "step 7712 loss 0.02402302250266075\n",
      "step 7713 loss 0.02402178943157196\n",
      "step 7714 loss 0.024020547047257423\n",
      "step 7715 loss 0.02401932328939438\n",
      "step 7716 loss 0.024018097668886185\n",
      "step 7717 loss 0.024016857147216797\n",
      "step 7718 loss 0.02401564083993435\n",
      "step 7719 loss 0.024014413356781006\n",
      "step 7720 loss 0.024013210088014603\n",
      "step 7721 loss 0.024011988192796707\n",
      "step 7722 loss 0.02401077188551426\n",
      "step 7723 loss 0.02400955744087696\n",
      "step 7724 loss 0.024008331820368767\n",
      "step 7725 loss 0.024007104337215424\n",
      "step 7726 loss 0.02400589920580387\n",
      "step 7727 loss 0.02400468848645687\n",
      "step 7728 loss 0.024003490805625916\n",
      "step 7729 loss 0.02400227263569832\n",
      "step 7730 loss 0.024001069366931915\n",
      "step 7731 loss 0.023999880999326706\n",
      "step 7732 loss 0.023998666554689407\n",
      "step 7733 loss 0.02399747632443905\n",
      "step 7734 loss 0.02399626187980175\n",
      "step 7735 loss 0.02399507910013199\n",
      "step 7736 loss 0.023993883281946182\n",
      "step 7737 loss 0.023992685601115227\n",
      "step 7738 loss 0.023991502821445465\n",
      "step 7739 loss 0.023990297690033913\n",
      "step 7740 loss 0.023989105597138405\n",
      "step 7741 loss 0.023987922817468643\n",
      "step 7742 loss 0.023986732587218285\n",
      "step 7743 loss 0.02398555725812912\n",
      "step 7744 loss 0.023984363302588463\n",
      "step 7745 loss 0.023983195424079895\n",
      "step 7746 loss 0.023982014507055283\n",
      "step 7747 loss 0.023980841040611267\n",
      "step 7748 loss 0.023979641497135162\n",
      "step 7749 loss 0.023978471755981445\n",
      "step 7750 loss 0.023977311328053474\n",
      "step 7751 loss 0.023976119235157967\n",
      "step 7752 loss 0.023974966257810593\n",
      "step 7753 loss 0.02397380769252777\n",
      "step 7754 loss 0.023972615599632263\n",
      "step 7755 loss 0.023971466347575188\n",
      "step 7756 loss 0.02397029474377632\n",
      "step 7757 loss 0.02396913804113865\n",
      "step 7758 loss 0.02396797202527523\n",
      "step 7759 loss 0.023966800421476364\n",
      "step 7760 loss 0.02396564558148384\n",
      "step 7761 loss 0.02396448887884617\n",
      "step 7762 loss 0.02396334707736969\n",
      "step 7763 loss 0.023962177336215973\n",
      "step 7764 loss 0.0239610243588686\n",
      "step 7765 loss 0.023959875106811523\n",
      "step 7766 loss 0.023958725854754448\n",
      "step 7767 loss 0.023957591503858566\n",
      "step 7768 loss 0.0239564199000597\n",
      "step 7769 loss 0.02395528368651867\n",
      "step 7770 loss 0.023954134434461594\n",
      "step 7771 loss 0.023952992632985115\n",
      "step 7772 loss 0.02395184524357319\n",
      "step 7773 loss 0.023950712755322456\n",
      "step 7774 loss 0.023949505761265755\n",
      "step 7775 loss 0.023948317393660545\n",
      "step 7776 loss 0.023947102949023247\n",
      "step 7777 loss 0.023945895954966545\n",
      "step 7778 loss 0.023944681510329247\n",
      "step 7779 loss 0.023943472653627396\n",
      "step 7780 loss 0.02394227311015129\n",
      "step 7781 loss 0.023941047489643097\n",
      "step 7782 loss 0.023939847946166992\n",
      "step 7783 loss 0.023938631638884544\n",
      "step 7784 loss 0.02393740601837635\n",
      "step 7785 loss 0.023936189711093903\n",
      "step 7786 loss 0.023934975266456604\n",
      "step 7787 loss 0.02393377013504505\n",
      "step 7788 loss 0.023932548239827156\n",
      "step 7789 loss 0.023931333795189857\n",
      "step 7790 loss 0.02393011748790741\n",
      "step 7791 loss 0.02392890490591526\n",
      "step 7792 loss 0.02392769418656826\n",
      "step 7793 loss 0.02392648719251156\n",
      "step 7794 loss 0.02392527647316456\n",
      "step 7795 loss 0.02392406202852726\n",
      "step 7796 loss 0.023922858759760857\n",
      "step 7797 loss 0.023921651765704155\n",
      "step 7798 loss 0.023920444771647453\n",
      "step 7799 loss 0.023919230327010155\n",
      "step 7800 loss 0.02391802705824375\n",
      "step 7801 loss 0.023916831240057945\n",
      "step 7802 loss 0.02391562983393669\n",
      "step 7803 loss 0.023914441466331482\n",
      "step 7804 loss 0.02391323633491993\n",
      "step 7805 loss 0.023912038654088974\n",
      "step 7806 loss 0.02391083911061287\n",
      "step 7807 loss 0.023909643292427063\n",
      "step 7808 loss 0.023908451199531555\n",
      "step 7809 loss 0.02390725538134575\n",
      "step 7810 loss 0.023906078189611435\n",
      "step 7811 loss 0.02390488237142563\n",
      "step 7812 loss 0.023903703317046165\n",
      "step 7813 loss 0.023902516812086105\n",
      "step 7814 loss 0.023901324719190598\n",
      "step 7815 loss 0.02390015311539173\n",
      "step 7816 loss 0.023898959159851074\n",
      "step 7817 loss 0.023897767066955566\n",
      "step 7818 loss 0.023896599188447\n",
      "step 7819 loss 0.02389543317258358\n",
      "step 7820 loss 0.023894239217042923\n",
      "step 7821 loss 0.023893043398857117\n",
      "step 7822 loss 0.023891817778348923\n",
      "step 7823 loss 0.02389059029519558\n",
      "step 7824 loss 0.02388937771320343\n",
      "step 7825 loss 0.023888159543275833\n",
      "step 7826 loss 0.023886926472187042\n",
      "step 7827 loss 0.02388569340109825\n",
      "step 7828 loss 0.023884473368525505\n",
      "step 7829 loss 0.023883242160081863\n",
      "step 7830 loss 0.02388201281428337\n",
      "step 7831 loss 0.02388077788054943\n",
      "step 7832 loss 0.023879555985331535\n",
      "step 7833 loss 0.02387833409011364\n",
      "step 7834 loss 0.023877110332250595\n",
      "step 7835 loss 0.023875877261161804\n",
      "step 7836 loss 0.023874640464782715\n",
      "step 7837 loss 0.02387341484427452\n",
      "step 7838 loss 0.023872213438153267\n",
      "step 7839 loss 0.02387096919119358\n",
      "step 7840 loss 0.023869747295975685\n",
      "step 7841 loss 0.023868519812822342\n",
      "step 7842 loss 0.023867318406701088\n",
      "step 7843 loss 0.02386610023677349\n",
      "step 7844 loss 0.023864882066845894\n",
      "step 7845 loss 0.023863648995757103\n",
      "step 7846 loss 0.02386244386434555\n",
      "step 7847 loss 0.023861227557063103\n",
      "step 7848 loss 0.023860011249780655\n",
      "step 7849 loss 0.02385880798101425\n",
      "step 7850 loss 0.023857591673731804\n",
      "step 7851 loss 0.02385638654232025\n",
      "step 7852 loss 0.02385517582297325\n",
      "step 7853 loss 0.02385396882891655\n",
      "step 7854 loss 0.02385275438427925\n",
      "step 7855 loss 0.023851562291383743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 7856 loss 0.02385035715997219\n",
      "step 7857 loss 0.02384914830327034\n",
      "step 7858 loss 0.023847948759794235\n",
      "step 7859 loss 0.023846764117479324\n",
      "step 7860 loss 0.023845558986067772\n",
      "step 7861 loss 0.023844370618462563\n",
      "step 7862 loss 0.0238431915640831\n",
      "step 7863 loss 0.023841992020606995\n",
      "step 7864 loss 0.02384079620242119\n",
      "step 7865 loss 0.023839609697461128\n",
      "step 7866 loss 0.023838428780436516\n",
      "step 7867 loss 0.02383723109960556\n",
      "step 7868 loss 0.0238360483199358\n",
      "step 7869 loss 0.023834872990846634\n",
      "step 7870 loss 0.02383367531001568\n",
      "step 7871 loss 0.023832492530345917\n",
      "step 7872 loss 0.023831330239772797\n",
      "step 7873 loss 0.02383015677332878\n",
      "step 7874 loss 0.02382897399365902\n",
      "step 7875 loss 0.023827798664569855\n",
      "step 7876 loss 0.023826615884900093\n",
      "step 7877 loss 0.02382545731961727\n",
      "step 7878 loss 0.023824285715818405\n",
      "step 7879 loss 0.023823106661438942\n",
      "step 7880 loss 0.023821944370865822\n",
      "step 7881 loss 0.0238207820802927\n",
      "step 7882 loss 0.023819614201784134\n",
      "step 7883 loss 0.02381846122443676\n",
      "step 7884 loss 0.023817291483283043\n",
      "step 7885 loss 0.023816142231225967\n",
      "step 7886 loss 0.023814987391233444\n",
      "step 7887 loss 0.023813815787434578\n",
      "step 7888 loss 0.023812662810087204\n",
      "step 7889 loss 0.02381150797009468\n",
      "step 7890 loss 0.023810358718037605\n",
      "step 7891 loss 0.023809194564819336\n",
      "step 7892 loss 0.023808056488633156\n",
      "step 7893 loss 0.02380690537393093\n",
      "step 7894 loss 0.023805765435099602\n",
      "step 7895 loss 0.02380462922155857\n",
      "step 7896 loss 0.0238034725189209\n",
      "step 7897 loss 0.02380230836570263\n",
      "step 7898 loss 0.023801185190677643\n",
      "step 7899 loss 0.023800048977136612\n",
      "step 7900 loss 0.023798905313014984\n",
      "step 7901 loss 0.023797769099473953\n",
      "step 7902 loss 0.02379663474857807\n",
      "step 7903 loss 0.023795494809746742\n",
      "step 7904 loss 0.02379436045885086\n",
      "step 7905 loss 0.023793239146471024\n",
      "step 7906 loss 0.023792104795575142\n",
      "step 7907 loss 0.02379097230732441\n",
      "step 7908 loss 0.023789824917912483\n",
      "step 7909 loss 0.02378872223198414\n",
      "step 7910 loss 0.0237876009196043\n",
      "step 7911 loss 0.023786477744579315\n",
      "step 7912 loss 0.02378535270690918\n",
      "step 7913 loss 0.023784227669239044\n",
      "step 7914 loss 0.02378309704363346\n",
      "step 7915 loss 0.02378198131918907\n",
      "step 7916 loss 0.02378086745738983\n",
      "step 7917 loss 0.023779744282364845\n",
      "step 7918 loss 0.023778634145855904\n",
      "step 7919 loss 0.023777518421411514\n",
      "step 7920 loss 0.023776402696967125\n",
      "step 7921 loss 0.023775294423103333\n",
      "step 7922 loss 0.02377418987452984\n",
      "step 7923 loss 0.023773064836859703\n",
      "step 7924 loss 0.023771969601511955\n",
      "step 7925 loss 0.023770857602357864\n",
      "step 7926 loss 0.023769758641719818\n",
      "step 7927 loss 0.023768648505210876\n",
      "step 7928 loss 0.02376754768192768\n",
      "step 7929 loss 0.02376643568277359\n",
      "step 7930 loss 0.02376534417271614\n",
      "step 7931 loss 0.023764243349432945\n",
      "step 7932 loss 0.02376314252614975\n",
      "step 7933 loss 0.023762064054608345\n",
      "step 7934 loss 0.023760948330163956\n",
      "step 7935 loss 0.023759856820106506\n",
      "step 7936 loss 0.023758774623274803\n",
      "step 7937 loss 0.023757686838507652\n",
      "step 7938 loss 0.023756591603159904\n",
      "step 7939 loss 0.023755492642521858\n",
      "step 7940 loss 0.023754416033625603\n",
      "step 7941 loss 0.02375331148505211\n",
      "step 7942 loss 0.023752231150865555\n",
      "step 7943 loss 0.023751161992549896\n",
      "step 7944 loss 0.023750072345137596\n",
      "step 7945 loss 0.023748988285660744\n",
      "step 7946 loss 0.023747894912958145\n",
      "step 7947 loss 0.02374682016670704\n",
      "step 7948 loss 0.023745737969875336\n",
      "step 7949 loss 0.02374466508626938\n",
      "step 7950 loss 0.02374359220266342\n",
      "step 7951 loss 0.023742519319057465\n",
      "step 7952 loss 0.023741429671645164\n",
      "step 7953 loss 0.023740356788039207\n",
      "step 7954 loss 0.023739293217658997\n",
      "step 7955 loss 0.02373821660876274\n",
      "step 7956 loss 0.023737164214253426\n",
      "step 7957 loss 0.02373608574271202\n",
      "step 7958 loss 0.023735003545880318\n",
      "step 7959 loss 0.02373390458524227\n",
      "step 7960 loss 0.023732798174023628\n",
      "step 7961 loss 0.02373170293867588\n",
      "step 7962 loss 0.02373058721423149\n",
      "step 7963 loss 0.02372945472598076\n",
      "step 7964 loss 0.023728296160697937\n",
      "step 7965 loss 0.023727159947156906\n",
      "step 7966 loss 0.023726027458906174\n",
      "step 7967 loss 0.02372484654188156\n",
      "step 7968 loss 0.0237236637622118\n",
      "step 7969 loss 0.023722480982542038\n",
      "step 7970 loss 0.023721303790807724\n",
      "step 7971 loss 0.023720094934105873\n",
      "step 7972 loss 0.02371889539062977\n",
      "step 7973 loss 0.02371770702302456\n",
      "step 7974 loss 0.023716511204838753\n",
      "step 7975 loss 0.02371528558433056\n",
      "step 7976 loss 0.0237140990793705\n",
      "step 7977 loss 0.023712890222668648\n",
      "step 7978 loss 0.0237116739153862\n",
      "step 7979 loss 0.02371046505868435\n",
      "step 7980 loss 0.0237092487514019\n",
      "step 7981 loss 0.023708034306764603\n",
      "step 7982 loss 0.023706821724772453\n",
      "step 7983 loss 0.023705601692199707\n",
      "step 7984 loss 0.023704390972852707\n",
      "step 7985 loss 0.023703183978796005\n",
      "step 7986 loss 0.023701976984739304\n",
      "step 7987 loss 0.02370074763894081\n",
      "step 7988 loss 0.023699546232819557\n",
      "step 7989 loss 0.023698315024375916\n",
      "step 7990 loss 0.02369711361825466\n",
      "step 7991 loss 0.023695901036262512\n",
      "step 7992 loss 0.023694686591625214\n",
      "step 7993 loss 0.023693479597568512\n",
      "step 7994 loss 0.023692259564995766\n",
      "step 7995 loss 0.02369105815887451\n",
      "step 7996 loss 0.02368984743952751\n",
      "step 7997 loss 0.023688632994890213\n",
      "step 7998 loss 0.023687414824962616\n",
      "step 7999 loss 0.023686224594712257\n",
      "step 8000 loss 0.02368501015007496\n",
      "step 8001 loss 0.0236838199198246\n",
      "step 8002 loss 0.023682603612542152\n",
      "step 8003 loss 0.023681391030550003\n",
      "step 8004 loss 0.023680200800299644\n",
      "step 8005 loss 0.023679012432694435\n",
      "step 8006 loss 0.02367778867483139\n",
      "step 8007 loss 0.02367660030722618\n",
      "step 8008 loss 0.023675398901104927\n",
      "step 8009 loss 0.023674214258790016\n",
      "step 8010 loss 0.023673025891184807\n",
      "step 8011 loss 0.023671822622418404\n",
      "step 8012 loss 0.023670630529522896\n",
      "step 8013 loss 0.02366943284869194\n",
      "step 8014 loss 0.023668240755796432\n",
      "step 8015 loss 0.02366705983877182\n",
      "step 8016 loss 0.023665860295295715\n",
      "step 8017 loss 0.023664671927690506\n",
      "step 8018 loss 0.023663492873311043\n",
      "step 8019 loss 0.02366228587925434\n",
      "step 8020 loss 0.023661114275455475\n",
      "step 8021 loss 0.023659929633140564\n",
      "step 8022 loss 0.02365875244140625\n",
      "step 8023 loss 0.023657575249671936\n",
      "step 8024 loss 0.02365640178322792\n",
      "step 8025 loss 0.023655209690332413\n",
      "step 8026 loss 0.0236540324985981\n",
      "step 8027 loss 0.023652859032154083\n",
      "step 8028 loss 0.023651670664548874\n",
      "step 8029 loss 0.023650499060750008\n",
      "step 8030 loss 0.02364932931959629\n",
      "step 8031 loss 0.023648157715797424\n",
      "step 8032 loss 0.023646974936127663\n",
      "step 8033 loss 0.02364582009613514\n",
      "step 8034 loss 0.023644650354981422\n",
      "step 8035 loss 0.02364346943795681\n",
      "step 8036 loss 0.02364231087267399\n",
      "step 8037 loss 0.02364114485681057\n",
      "step 8038 loss 0.023639969527721405\n",
      "step 8039 loss 0.02363881655037403\n",
      "step 8040 loss 0.023637646809220314\n",
      "step 8041 loss 0.023636484518647194\n",
      "step 8042 loss 0.023635320365428925\n",
      "step 8043 loss 0.02363416738808155\n",
      "step 8044 loss 0.023632992058992386\n",
      "step 8045 loss 0.023631839081645012\n",
      "step 8046 loss 0.02363068424165249\n",
      "step 8047 loss 0.023629523813724518\n",
      "step 8048 loss 0.02362837828695774\n",
      "step 8049 loss 0.02362721972167492\n",
      "step 8050 loss 0.023626074194908142\n",
      "step 8051 loss 0.023624911904335022\n",
      "step 8052 loss 0.023623758926987648\n",
      "step 8053 loss 0.023622607812285423\n",
      "step 8054 loss 0.02362145110964775\n",
      "step 8055 loss 0.023620307445526123\n",
      "step 8056 loss 0.023619160056114197\n",
      "step 8057 loss 0.02361801080405712\n",
      "step 8058 loss 0.023616861552000046\n",
      "step 8059 loss 0.023615727201104164\n",
      "step 8060 loss 0.02361457608640194\n",
      "step 8061 loss 0.02361343428492546\n",
      "step 8062 loss 0.023612288758158684\n",
      "step 8063 loss 0.023611150681972504\n",
      "step 8064 loss 0.023610001429915428\n",
      "step 8065 loss 0.02360885590314865\n",
      "step 8066 loss 0.023607736453413963\n",
      "step 8067 loss 0.023606600239872932\n",
      "step 8068 loss 0.023605450987815857\n",
      "step 8069 loss 0.023604288697242737\n",
      "step 8070 loss 0.023603131994605064\n",
      "step 8071 loss 0.023601971566677094\n",
      "step 8072 loss 0.02360079251229763\n",
      "step 8073 loss 0.023599648848176003\n",
      "step 8074 loss 0.02359846979379654\n",
      "step 8075 loss 0.02359730750322342\n",
      "step 8076 loss 0.023596148937940598\n",
      "step 8077 loss 0.023594984784722328\n",
      "step 8078 loss 0.02359382063150406\n",
      "step 8079 loss 0.02359265834093094\n",
      "step 8080 loss 0.02359149232506752\n",
      "step 8081 loss 0.023590316995978355\n",
      "step 8082 loss 0.023589149117469788\n",
      "step 8083 loss 0.023587984964251518\n",
      "step 8084 loss 0.023586828261613846\n",
      "step 8085 loss 0.023585667833685875\n",
      "step 8086 loss 0.023584503680467606\n",
      "step 8087 loss 0.023583343252539635\n",
      "step 8088 loss 0.023582179099321365\n",
      "step 8089 loss 0.023581016808748245\n",
      "step 8090 loss 0.02357986755669117\n",
      "step 8091 loss 0.023578710854053497\n",
      "step 8092 loss 0.02357754483819008\n",
      "step 8093 loss 0.023576393723487854\n",
      "step 8094 loss 0.023575235158205032\n",
      "step 8095 loss 0.023574087768793106\n",
      "step 8096 loss 0.023572923615574837\n",
      "step 8097 loss 0.023571766912937164\n",
      "step 8098 loss 0.02357061207294464\n",
      "step 8099 loss 0.023569459095597267\n",
      "step 8100 loss 0.023568300530314445\n",
      "step 8101 loss 0.023567181080579758\n",
      "step 8102 loss 0.02356601320207119\n",
      "step 8103 loss 0.023564867675304413\n",
      "step 8104 loss 0.023563724011182785\n",
      "step 8105 loss 0.02356257475912571\n",
      "step 8106 loss 0.023561419919133186\n",
      "step 8107 loss 0.023560278117656708\n",
      "step 8108 loss 0.02355913817882538\n",
      "step 8109 loss 0.023558001965284348\n",
      "step 8110 loss 0.02355686016380787\n",
      "step 8111 loss 0.023555701598525047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 8112 loss 0.023554572835564613\n",
      "step 8113 loss 0.02355342172086239\n",
      "step 8114 loss 0.023552289232611656\n",
      "step 8115 loss 0.02355116419494152\n",
      "step 8116 loss 0.023550014942884445\n",
      "step 8117 loss 0.02354889176785946\n",
      "step 8118 loss 0.023547740653157234\n",
      "step 8119 loss 0.023546608164906502\n",
      "step 8120 loss 0.02354547008872032\n",
      "step 8121 loss 0.023544348776340485\n",
      "step 8122 loss 0.023543214425444603\n",
      "step 8123 loss 0.023542087525129318\n",
      "step 8124 loss 0.023540949448943138\n",
      "step 8125 loss 0.023539822548627853\n",
      "step 8126 loss 0.02353869378566742\n",
      "step 8127 loss 0.02353757433593273\n",
      "step 8128 loss 0.023536434397101402\n",
      "step 8129 loss 0.023535318672657013\n",
      "step 8130 loss 0.02353418804705143\n",
      "step 8131 loss 0.02353307232260704\n",
      "step 8132 loss 0.02353191375732422\n",
      "step 8133 loss 0.023530764505267143\n",
      "step 8134 loss 0.02352961152791977\n",
      "step 8135 loss 0.023528441786766052\n",
      "step 8136 loss 0.023527290672063828\n",
      "step 8137 loss 0.023526139557361603\n",
      "step 8138 loss 0.023524966090917587\n",
      "step 8139 loss 0.023523801937699318\n",
      "step 8140 loss 0.023522652685642242\n",
      "step 8141 loss 0.023521481081843376\n",
      "step 8142 loss 0.02352030761539936\n",
      "step 8143 loss 0.023519152775406837\n",
      "step 8144 loss 0.02351798489689827\n",
      "step 8145 loss 0.023516763001680374\n",
      "step 8146 loss 0.02351551502943039\n",
      "step 8147 loss 0.02351425588130951\n",
      "step 8148 loss 0.023513000458478928\n",
      "step 8149 loss 0.023511730134487152\n",
      "step 8150 loss 0.023510446771979332\n",
      "step 8151 loss 0.023509161546826363\n",
      "step 8152 loss 0.02350788563489914\n",
      "step 8153 loss 0.023506592959165573\n",
      "step 8154 loss 0.023505302146077156\n",
      "step 8155 loss 0.023504013195633888\n",
      "step 8156 loss 0.023502714931964874\n",
      "step 8157 loss 0.02350141480565071\n",
      "step 8158 loss 0.023500120267271996\n",
      "step 8159 loss 0.023498816415667534\n",
      "step 8160 loss 0.023497510701417923\n",
      "step 8161 loss 0.023496199399232864\n",
      "step 8162 loss 0.023494893684983253\n",
      "step 8163 loss 0.023493580520153046\n",
      "step 8164 loss 0.02349228225648403\n",
      "step 8165 loss 0.023490991443395615\n",
      "step 8166 loss 0.023489678278565407\n",
      "step 8167 loss 0.023488378152251244\n",
      "step 8168 loss 0.023487085476517677\n",
      "step 8169 loss 0.02348577231168747\n",
      "step 8170 loss 0.02348446287214756\n",
      "step 8171 loss 0.023483166471123695\n",
      "step 8172 loss 0.023481862619519234\n",
      "step 8173 loss 0.02348054014146328\n",
      "step 8174 loss 0.023479163646697998\n",
      "step 8175 loss 0.023477774113416672\n",
      "step 8176 loss 0.023476380854845047\n",
      "step 8177 loss 0.023474980145692825\n",
      "step 8178 loss 0.023473568260669708\n",
      "step 8179 loss 0.02347218059003353\n",
      "step 8180 loss 0.02347078174352646\n",
      "step 8181 loss 0.023469381034374237\n",
      "step 8182 loss 0.023467959836125374\n",
      "step 8183 loss 0.023466553539037704\n",
      "step 8184 loss 0.023465130478143692\n",
      "step 8185 loss 0.023463711142539978\n",
      "step 8186 loss 0.023462310433387756\n",
      "step 8187 loss 0.023460889235138893\n",
      "step 8188 loss 0.023459462448954582\n",
      "step 8189 loss 0.02345806546509266\n",
      "step 8190 loss 0.023456642404198647\n",
      "step 8191 loss 0.023455221205949783\n",
      "step 8192 loss 0.023453809320926666\n",
      "step 8193 loss 0.023452406749129295\n",
      "step 8194 loss 0.02345099300146103\n",
      "step 8195 loss 0.023449566215276718\n",
      "step 8196 loss 0.023448163643479347\n",
      "step 8197 loss 0.023446736857295036\n",
      "step 8198 loss 0.02344531938433647\n",
      "step 8199 loss 0.02344391494989395\n",
      "step 8200 loss 0.023442497476935387\n",
      "step 8201 loss 0.02344108186662197\n",
      "step 8202 loss 0.023439643904566765\n",
      "step 8203 loss 0.02343824692070484\n",
      "step 8204 loss 0.023436812683939934\n",
      "step 8205 loss 0.02343539521098137\n",
      "step 8206 loss 0.023433968424797058\n",
      "step 8207 loss 0.023432554677128792\n",
      "step 8208 loss 0.023431139066815376\n",
      "step 8209 loss 0.02342972345650196\n",
      "step 8210 loss 0.023428315296769142\n",
      "step 8211 loss 0.023426905274391174\n",
      "step 8212 loss 0.023425478488206863\n",
      "step 8213 loss 0.02342407964169979\n",
      "step 8214 loss 0.023422665894031525\n",
      "step 8215 loss 0.02342127077281475\n",
      "step 8216 loss 0.02341984212398529\n",
      "step 8217 loss 0.023418432101607323\n",
      "step 8218 loss 0.023417020216584206\n",
      "step 8219 loss 0.02341562695801258\n",
      "step 8220 loss 0.02341422066092491\n",
      "step 8221 loss 0.02341281622648239\n",
      "step 8222 loss 0.023411402478814125\n",
      "step 8223 loss 0.023410024121403694\n",
      "step 8224 loss 0.023408623412251472\n",
      "step 8225 loss 0.02340722270309925\n",
      "step 8226 loss 0.023405827581882477\n",
      "step 8227 loss 0.023404428735375404\n",
      "step 8228 loss 0.023403039202094078\n",
      "step 8229 loss 0.023401649668812752\n",
      "step 8230 loss 0.02340027317404747\n",
      "step 8231 loss 0.023398874327540398\n",
      "step 8232 loss 0.02339748851954937\n",
      "step 8233 loss 0.023396102711558342\n",
      "step 8234 loss 0.02339472621679306\n",
      "step 8235 loss 0.02339334785938263\n",
      "step 8236 loss 0.023391954600811005\n",
      "step 8237 loss 0.02339058928191662\n",
      "step 8238 loss 0.02338920161128044\n",
      "step 8239 loss 0.02338782511651516\n",
      "step 8240 loss 0.023386461660265923\n",
      "step 8241 loss 0.023385081440210342\n",
      "step 8242 loss 0.02338372729718685\n",
      "step 8243 loss 0.02338234893977642\n",
      "step 8244 loss 0.023380985483527184\n",
      "step 8245 loss 0.0233796164393425\n",
      "step 8246 loss 0.023378238081932068\n",
      "step 8247 loss 0.023376865312457085\n",
      "step 8248 loss 0.023375511169433594\n",
      "step 8249 loss 0.02337416261434555\n",
      "step 8250 loss 0.023372795432806015\n",
      "step 8251 loss 0.023371433839201927\n",
      "step 8252 loss 0.023370062932372093\n",
      "step 8253 loss 0.023368731141090393\n",
      "step 8254 loss 0.023367369547486305\n",
      "step 8255 loss 0.023366009816527367\n",
      "step 8256 loss 0.023364663124084473\n",
      "step 8257 loss 0.023363327607512474\n",
      "step 8258 loss 0.02336195856332779\n",
      "step 8259 loss 0.0233606044203043\n",
      "step 8260 loss 0.02335927076637745\n",
      "step 8261 loss 0.023357920348644257\n",
      "step 8262 loss 0.02335658296942711\n",
      "step 8263 loss 0.02335524745285511\n",
      "step 8264 loss 0.02335391193628311\n",
      "step 8265 loss 0.02335256151854992\n",
      "step 8266 loss 0.02335122786462307\n",
      "step 8267 loss 0.02334989234805107\n",
      "step 8268 loss 0.02334856241941452\n",
      "step 8269 loss 0.02334721013903618\n",
      "step 8270 loss 0.023345880210399628\n",
      "step 8271 loss 0.023344527930021286\n",
      "step 8272 loss 0.023343199864029884\n",
      "step 8273 loss 0.02334185130894184\n",
      "step 8274 loss 0.023340512067079544\n",
      "step 8275 loss 0.0233391672372818\n",
      "step 8276 loss 0.023337818682193756\n",
      "step 8277 loss 0.023336488753557205\n",
      "step 8278 loss 0.023335136473178864\n",
      "step 8279 loss 0.02333381026983261\n",
      "step 8280 loss 0.023332465440034866\n",
      "step 8281 loss 0.023331137374043465\n",
      "step 8282 loss 0.023329799994826317\n",
      "step 8283 loss 0.023328464478254318\n",
      "step 8284 loss 0.023327136412262917\n",
      "step 8285 loss 0.02332579530775547\n",
      "step 8286 loss 0.02332446537911892\n",
      "step 8287 loss 0.02332313545048237\n",
      "step 8288 loss 0.02332180179655552\n",
      "step 8289 loss 0.023320484906435013\n",
      "step 8290 loss 0.023319143801927567\n",
      "step 8291 loss 0.023317819461226463\n",
      "step 8292 loss 0.02331649139523506\n",
      "step 8293 loss 0.023315181955695152\n",
      "step 8294 loss 0.02331385761499405\n",
      "step 8295 loss 0.02331254445016384\n",
      "step 8296 loss 0.023311220109462738\n",
      "step 8297 loss 0.023309903219342232\n",
      "step 8298 loss 0.023308580741286278\n",
      "step 8299 loss 0.02330727130174637\n",
      "step 8300 loss 0.02330596372485161\n",
      "step 8301 loss 0.023304639384150505\n",
      "step 8302 loss 0.023303313180804253\n",
      "step 8303 loss 0.02330201119184494\n",
      "step 8304 loss 0.023300712928175926\n",
      "step 8305 loss 0.02329939417541027\n",
      "step 8306 loss 0.023298082873225212\n",
      "step 8307 loss 0.023296790197491646\n",
      "step 8308 loss 0.02329547144472599\n",
      "step 8309 loss 0.02329418435692787\n",
      "step 8310 loss 0.023292861878871918\n",
      "step 8311 loss 0.023291561752557755\n",
      "step 8312 loss 0.02329026721417904\n",
      "step 8313 loss 0.023288967087864876\n",
      "step 8314 loss 0.023287635296583176\n",
      "step 8315 loss 0.02328631468117237\n",
      "step 8316 loss 0.023284979164600372\n",
      "step 8317 loss 0.023283669725060463\n",
      "step 8318 loss 0.02328232303261757\n",
      "step 8319 loss 0.023280993103981018\n",
      "step 8320 loss 0.02327968180179596\n",
      "step 8321 loss 0.023278340697288513\n",
      "step 8322 loss 0.02327701263129711\n",
      "step 8323 loss 0.023275673389434814\n",
      "step 8324 loss 0.023274345323443413\n",
      "step 8325 loss 0.02327301912009716\n",
      "step 8326 loss 0.023271696642041206\n",
      "step 8327 loss 0.023270370438694954\n",
      "step 8328 loss 0.023269042372703552\n",
      "step 8329 loss 0.023267708718776703\n",
      "step 8330 loss 0.023266376927495003\n",
      "step 8331 loss 0.02326505444943905\n",
      "step 8332 loss 0.023263737559318542\n",
      "step 8333 loss 0.02326240949332714\n",
      "step 8334 loss 0.023261072114109993\n",
      "step 8335 loss 0.023259751498699188\n",
      "step 8336 loss 0.023258429020643234\n",
      "step 8337 loss 0.02325710467994213\n",
      "step 8338 loss 0.02325579710304737\n",
      "step 8339 loss 0.023254480212926865\n",
      "step 8340 loss 0.02325315959751606\n",
      "step 8341 loss 0.02325182966887951\n",
      "step 8342 loss 0.023250525817275047\n",
      "step 8343 loss 0.023249201476573944\n",
      "step 8344 loss 0.023247890174388885\n",
      "step 8345 loss 0.023246588185429573\n",
      "step 8346 loss 0.02324526570737362\n",
      "step 8347 loss 0.02324398048222065\n",
      "step 8348 loss 0.02324264496564865\n",
      "step 8349 loss 0.023241329938173294\n",
      "step 8350 loss 0.023240040987730026\n",
      "step 8351 loss 0.023238737136125565\n",
      "step 8352 loss 0.023237425833940506\n",
      "step 8353 loss 0.023236127570271492\n",
      "step 8354 loss 0.023234819993376732\n",
      "step 8355 loss 0.02323351427912712\n",
      "step 8356 loss 0.023232227191329002\n",
      "step 8357 loss 0.023230919614434242\n",
      "step 8358 loss 0.02322961390018463\n",
      "step 8359 loss 0.023228324949741364\n",
      "step 8360 loss 0.0232270285487175\n",
      "step 8361 loss 0.023225735872983932\n",
      "step 8362 loss 0.02322443015873432\n",
      "step 8363 loss 0.023223143070936203\n",
      "step 8364 loss 0.02322184480726719\n",
      "step 8365 loss 0.02322055771946907\n",
      "step 8366 loss 0.023219265043735504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 8367 loss 0.02321799285709858\n",
      "step 8368 loss 0.023216690868139267\n",
      "step 8369 loss 0.023215401917696\n",
      "step 8370 loss 0.023214129731059074\n",
      "step 8371 loss 0.023212851956486702\n",
      "step 8372 loss 0.023211564868688583\n",
      "step 8373 loss 0.02321028709411621\n",
      "step 8374 loss 0.023209013044834137\n",
      "step 8375 loss 0.023207731544971466\n",
      "step 8376 loss 0.023206448182463646\n",
      "step 8377 loss 0.023205172270536423\n",
      "step 8378 loss 0.023203890770673752\n",
      "step 8379 loss 0.023202624171972275\n",
      "step 8380 loss 0.02320135198533535\n",
      "step 8381 loss 0.023200063034892082\n",
      "step 8382 loss 0.023198815062642097\n",
      "step 8383 loss 0.023197537288069725\n",
      "step 8384 loss 0.0231962688267231\n",
      "step 8385 loss 0.023194976150989532\n",
      "step 8386 loss 0.02319370210170746\n",
      "step 8387 loss 0.023192429915070534\n",
      "step 8388 loss 0.023191137239336967\n",
      "step 8389 loss 0.023189857602119446\n",
      "step 8390 loss 0.023188598453998566\n",
      "step 8391 loss 0.02318730764091015\n",
      "step 8392 loss 0.023186039179563522\n",
      "step 8393 loss 0.023184766992926598\n",
      "step 8394 loss 0.023183496668934822\n",
      "step 8395 loss 0.023182228207588196\n",
      "step 8396 loss 0.02318093553185463\n",
      "step 8397 loss 0.023179613053798676\n",
      "step 8398 loss 0.023178281262516975\n",
      "step 8399 loss 0.023176951333880424\n",
      "step 8400 loss 0.023175612092018127\n",
      "step 8401 loss 0.02317427657544613\n",
      "step 8402 loss 0.023172931745648384\n",
      "step 8403 loss 0.02317158877849579\n",
      "step 8404 loss 0.023170238360762596\n",
      "step 8405 loss 0.023168887943029404\n",
      "step 8406 loss 0.02316754311323166\n",
      "step 8407 loss 0.023166194558143616\n",
      "step 8408 loss 0.023164834827184677\n",
      "step 8409 loss 0.02316349931061268\n",
      "step 8410 loss 0.023162154480814934\n",
      "step 8411 loss 0.023160787299275398\n",
      "step 8412 loss 0.023159436881542206\n",
      "step 8413 loss 0.02315809763967991\n",
      "step 8414 loss 0.023156750947237015\n",
      "step 8415 loss 0.023155415430665016\n",
      "step 8416 loss 0.023154087364673615\n",
      "step 8417 loss 0.02315276674926281\n",
      "step 8418 loss 0.023151425644755363\n",
      "step 8419 loss 0.023150037974119186\n",
      "step 8420 loss 0.023148655891418457\n",
      "step 8421 loss 0.02314727008342743\n",
      "step 8422 loss 0.023145871236920357\n",
      "step 8423 loss 0.023144476115703583\n",
      "step 8424 loss 0.02314307726919651\n",
      "step 8425 loss 0.023141680285334587\n",
      "step 8426 loss 0.023140287026762962\n",
      "step 8427 loss 0.02313889190554619\n",
      "step 8428 loss 0.023137478157877922\n",
      "step 8429 loss 0.023136081174016\n",
      "step 8430 loss 0.023134684190154076\n",
      "step 8431 loss 0.0231332927942276\n",
      "step 8432 loss 0.023131875321269035\n",
      "step 8433 loss 0.023130469024181366\n",
      "step 8434 loss 0.02312907949090004\n",
      "step 8435 loss 0.02312767319381237\n",
      "step 8436 loss 0.02312627248466015\n",
      "step 8437 loss 0.02312486618757248\n",
      "step 8438 loss 0.023123471066355705\n",
      "step 8439 loss 0.02312208153307438\n",
      "step 8440 loss 0.023120669648051262\n",
      "step 8441 loss 0.023119281977415085\n",
      "step 8442 loss 0.02311786822974682\n",
      "step 8443 loss 0.023116478696465492\n",
      "step 8444 loss 0.023115091025829315\n",
      "step 8445 loss 0.02311370149254799\n",
      "step 8446 loss 0.023112311959266663\n",
      "step 8447 loss 0.02311091683804989\n",
      "step 8448 loss 0.023109503090381622\n",
      "step 8449 loss 0.023108135908842087\n",
      "step 8450 loss 0.02310674451291561\n",
      "step 8451 loss 0.02310534007847309\n",
      "step 8452 loss 0.02310396172106266\n",
      "step 8453 loss 0.02310255728662014\n",
      "step 8454 loss 0.023101184517145157\n",
      "step 8455 loss 0.02309979870915413\n",
      "step 8456 loss 0.023098411038517952\n",
      "step 8457 loss 0.023097043856978416\n",
      "step 8458 loss 0.02309565059840679\n",
      "step 8459 loss 0.02309427410364151\n",
      "step 8460 loss 0.02309289015829563\n",
      "step 8461 loss 0.023091519251465797\n",
      "step 8462 loss 0.023090142756700516\n",
      "step 8463 loss 0.02308875508606434\n",
      "step 8464 loss 0.0230873916298151\n",
      "step 8465 loss 0.02308601886034012\n",
      "step 8466 loss 0.023084649816155434\n",
      "step 8467 loss 0.023083267733454704\n",
      "step 8468 loss 0.023081909865140915\n",
      "step 8469 loss 0.023080533370375633\n",
      "step 8470 loss 0.023079171776771545\n",
      "step 8471 loss 0.02307780273258686\n",
      "step 8472 loss 0.023076429963111877\n",
      "step 8473 loss 0.023075081408023834\n",
      "step 8474 loss 0.023073710501194\n",
      "step 8475 loss 0.02307235263288021\n",
      "step 8476 loss 0.023070992901921272\n",
      "step 8477 loss 0.023069629445672035\n",
      "step 8478 loss 0.023068280890583992\n",
      "step 8479 loss 0.023066911846399307\n",
      "step 8480 loss 0.02306555211544037\n",
      "step 8481 loss 0.023064203560352325\n",
      "step 8482 loss 0.023062843829393387\n",
      "step 8483 loss 0.023061491549015045\n",
      "step 8484 loss 0.023060135543346405\n",
      "step 8485 loss 0.02305878885090351\n",
      "step 8486 loss 0.023057442158460617\n",
      "step 8487 loss 0.023056112229824066\n",
      "step 8488 loss 0.023054752498865128\n",
      "step 8489 loss 0.023053402081131935\n",
      "step 8490 loss 0.02305205911397934\n",
      "step 8491 loss 0.023050708696246147\n",
      "step 8492 loss 0.02304936572909355\n",
      "step 8493 loss 0.023048032075166702\n",
      "step 8494 loss 0.023046698421239853\n",
      "step 8495 loss 0.023045342415571213\n",
      "step 8496 loss 0.02304401621222496\n",
      "step 8497 loss 0.023042673245072365\n",
      "step 8498 loss 0.023041339591145515\n",
      "step 8499 loss 0.023040004074573517\n",
      "step 8500 loss 0.023038668558001518\n",
      "step 8501 loss 0.023037346079945564\n",
      "step 8502 loss 0.023036012426018715\n",
      "step 8503 loss 0.023034676909446716\n",
      "step 8504 loss 0.02303333953022957\n",
      "step 8505 loss 0.023032020777463913\n",
      "step 8506 loss 0.023030687123537064\n",
      "step 8507 loss 0.02302936092019081\n",
      "step 8508 loss 0.02302803471684456\n",
      "step 8509 loss 0.023026704788208008\n",
      "step 8510 loss 0.023025386035442352\n",
      "step 8511 loss 0.0230240635573864\n",
      "step 8512 loss 0.02302274852991104\n",
      "step 8513 loss 0.023021431639790535\n",
      "step 8514 loss 0.02302009053528309\n",
      "step 8515 loss 0.023018784821033478\n",
      "step 8516 loss 0.02301747351884842\n",
      "step 8517 loss 0.02301616221666336\n",
      "step 8518 loss 0.023014841601252556\n",
      "step 8519 loss 0.02301352471113205\n",
      "step 8520 loss 0.023012222722172737\n",
      "step 8521 loss 0.023010915145277977\n",
      "step 8522 loss 0.02300960011780262\n",
      "step 8523 loss 0.02300828881561756\n",
      "step 8524 loss 0.023006977513432503\n",
      "step 8525 loss 0.02300567552447319\n",
      "step 8526 loss 0.023004358634352684\n",
      "step 8527 loss 0.02300305850803852\n",
      "step 8528 loss 0.02300175279378891\n",
      "step 8529 loss 0.023000430315732956\n",
      "step 8530 loss 0.022999076172709465\n",
      "step 8531 loss 0.02299773134291172\n",
      "step 8532 loss 0.022996382787823677\n",
      "step 8533 loss 0.022995028644800186\n",
      "step 8534 loss 0.022993678227066994\n",
      "step 8535 loss 0.022992320358753204\n",
      "step 8536 loss 0.022990969941020012\n",
      "step 8537 loss 0.02298962138593197\n",
      "step 8538 loss 0.02298826538026333\n",
      "step 8539 loss 0.02298690751194954\n",
      "step 8540 loss 0.022985540330410004\n",
      "step 8541 loss 0.022984180599451065\n",
      "step 8542 loss 0.022982826456427574\n",
      "step 8543 loss 0.022981463000178337\n",
      "step 8544 loss 0.022980112582445145\n",
      "step 8545 loss 0.022978756576776505\n",
      "step 8546 loss 0.022977400571107864\n",
      "step 8547 loss 0.022976035252213478\n",
      "step 8548 loss 0.022974679246544838\n",
      "step 8549 loss 0.02297331765294075\n",
      "step 8550 loss 0.022971970960497856\n",
      "step 8551 loss 0.022970614954829216\n",
      "step 8552 loss 0.022969268262386322\n",
      "step 8553 loss 0.02296791784465313\n",
      "step 8554 loss 0.022966565564274788\n",
      "step 8555 loss 0.022965218871831894\n",
      "step 8556 loss 0.022963855415582657\n",
      "step 8557 loss 0.02296251244843006\n",
      "step 8558 loss 0.022961167618632317\n",
      "step 8559 loss 0.022959813475608826\n",
      "step 8560 loss 0.022958464920520782\n",
      "step 8561 loss 0.022957121953368187\n",
      "step 8562 loss 0.022955775260925293\n",
      "step 8563 loss 0.022954432293772697\n",
      "step 8564 loss 0.022953087463974953\n",
      "step 8565 loss 0.02295174077153206\n",
      "step 8566 loss 0.02295040525496006\n",
      "step 8567 loss 0.022949067875742912\n",
      "step 8568 loss 0.022947734221816063\n",
      "step 8569 loss 0.022946394979953766\n",
      "step 8570 loss 0.022945040836930275\n",
      "step 8571 loss 0.022943725809454918\n",
      "step 8572 loss 0.02294238843023777\n",
      "step 8573 loss 0.022941051051020622\n",
      "step 8574 loss 0.022939706221222878\n",
      "step 8575 loss 0.02293839491903782\n",
      "step 8576 loss 0.022937053814530373\n",
      "step 8577 loss 0.022935720160603523\n",
      "step 8578 loss 0.022934403270483017\n",
      "step 8579 loss 0.022933073341846466\n",
      "step 8580 loss 0.022931747138500214\n",
      "step 8581 loss 0.02293042652308941\n",
      "step 8582 loss 0.022929105907678604\n",
      "step 8583 loss 0.022927766665816307\n",
      "step 8584 loss 0.022926444187760353\n",
      "step 8585 loss 0.02292512357234955\n",
      "step 8586 loss 0.022923817858099937\n",
      "step 8587 loss 0.022922484204173088\n",
      "step 8588 loss 0.02292117103934288\n",
      "step 8589 loss 0.02291986718773842\n",
      "step 8590 loss 0.022918548434972763\n",
      "step 8591 loss 0.022917233407497406\n",
      "step 8592 loss 0.022915927693247795\n",
      "step 8593 loss 0.02291461080312729\n",
      "step 8594 loss 0.022913292050361633\n",
      "step 8595 loss 0.022911980748176575\n",
      "step 8596 loss 0.022910669445991516\n",
      "step 8597 loss 0.02290935069322586\n",
      "step 8598 loss 0.022908050566911697\n",
      "step 8599 loss 0.022906752303242683\n",
      "step 8600 loss 0.02290545403957367\n",
      "step 8601 loss 0.02290414273738861\n",
      "step 8602 loss 0.022902848199009895\n",
      "step 8603 loss 0.022901540622115135\n",
      "step 8604 loss 0.022900238633155823\n",
      "step 8605 loss 0.02289894036948681\n",
      "step 8606 loss 0.02289762906730175\n",
      "step 8607 loss 0.022896332666277885\n",
      "step 8608 loss 0.022895054891705513\n",
      "step 8609 loss 0.022893738001585007\n",
      "step 8610 loss 0.022892456501722336\n",
      "step 8611 loss 0.02289113961160183\n",
      "step 8612 loss 0.022889867424964905\n",
      "step 8613 loss 0.02288857288658619\n",
      "step 8614 loss 0.022887278348207474\n",
      "step 8615 loss 0.022885985672473907\n",
      "step 8616 loss 0.02288469485938549\n",
      "step 8617 loss 0.022883407771587372\n",
      "step 8618 loss 0.022882113233208656\n",
      "step 8619 loss 0.022880828008055687\n",
      "step 8620 loss 0.02287955768406391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 8621 loss 0.022878283634781837\n",
      "step 8622 loss 0.022877003997564316\n",
      "step 8623 loss 0.0228757094591856\n",
      "step 8624 loss 0.022874413058161736\n",
      "step 8625 loss 0.02287314459681511\n",
      "step 8626 loss 0.02287185750901699\n",
      "step 8627 loss 0.02287057787179947\n",
      "step 8628 loss 0.022869322448968887\n",
      "step 8629 loss 0.022868037223815918\n",
      "step 8630 loss 0.022866763174533844\n",
      "step 8631 loss 0.022865476086735725\n",
      "step 8632 loss 0.022864213213324547\n",
      "step 8633 loss 0.02286294475197792\n",
      "step 8634 loss 0.0228616651147604\n",
      "step 8635 loss 0.022860389202833176\n",
      "step 8636 loss 0.022859133780002594\n",
      "step 8637 loss 0.022857852280139923\n",
      "step 8638 loss 0.022856594994664192\n",
      "step 8639 loss 0.022855330258607864\n",
      "step 8640 loss 0.022854061797261238\n",
      "step 8641 loss 0.022852787747979164\n",
      "step 8642 loss 0.022851528599858284\n",
      "step 8643 loss 0.022850269451737404\n",
      "step 8644 loss 0.022849012166261673\n",
      "step 8645 loss 0.022847747430205345\n",
      "step 8646 loss 0.022846495732665062\n",
      "step 8647 loss 0.022845236584544182\n",
      "step 8648 loss 0.02284395880997181\n",
      "step 8649 loss 0.02284272201359272\n",
      "step 8650 loss 0.02284146100282669\n",
      "step 8651 loss 0.02284020371735096\n",
      "step 8652 loss 0.02283894456923008\n",
      "step 8653 loss 0.022837698459625244\n",
      "step 8654 loss 0.02283642813563347\n",
      "step 8655 loss 0.02283521741628647\n",
      "step 8656 loss 0.022833948954939842\n",
      "step 8657 loss 0.02283269353210926\n",
      "step 8658 loss 0.022831439971923828\n",
      "step 8659 loss 0.022830195724964142\n",
      "step 8660 loss 0.022828945890069008\n",
      "step 8661 loss 0.02282770350575447\n",
      "step 8662 loss 0.02282646670937538\n",
      "step 8663 loss 0.022825216874480247\n",
      "step 8664 loss 0.022823963314294815\n",
      "step 8665 loss 0.022822728380560875\n",
      "step 8666 loss 0.02282147854566574\n",
      "step 8667 loss 0.022820262238383293\n",
      "step 8668 loss 0.022819016128778458\n",
      "step 8669 loss 0.022817764431238174\n",
      "step 8670 loss 0.02281654067337513\n",
      "step 8671 loss 0.022815298289060593\n",
      "step 8672 loss 0.022814059630036354\n",
      "step 8673 loss 0.02281283400952816\n",
      "step 8674 loss 0.022811586037278175\n",
      "step 8675 loss 0.02281036414206028\n",
      "step 8676 loss 0.022809132933616638\n",
      "step 8677 loss 0.022807897999882698\n",
      "step 8678 loss 0.022806670516729355\n",
      "step 8679 loss 0.022805452346801758\n",
      "step 8680 loss 0.02280421555042267\n",
      "step 8681 loss 0.022802991792559624\n",
      "step 8682 loss 0.02280176430940628\n",
      "step 8683 loss 0.02280053123831749\n",
      "step 8684 loss 0.022799314931035042\n",
      "step 8685 loss 0.022798093035817146\n",
      "step 8686 loss 0.02279687114059925\n",
      "step 8687 loss 0.022795643657445908\n",
      "step 8688 loss 0.02279442548751831\n",
      "step 8689 loss 0.022793207317590714\n",
      "step 8690 loss 0.022791972383856773\n",
      "step 8691 loss 0.02279076538980007\n",
      "step 8692 loss 0.02278953604400158\n",
      "step 8693 loss 0.022788330912590027\n",
      "step 8694 loss 0.022787103429436684\n",
      "step 8695 loss 0.02278590202331543\n",
      "step 8696 loss 0.022784685716032982\n",
      "step 8697 loss 0.022783473134040833\n",
      "step 8698 loss 0.02278226613998413\n",
      "step 8699 loss 0.02278105542063713\n",
      "step 8700 loss 0.022779831662774086\n",
      "step 8701 loss 0.022778596729040146\n",
      "step 8702 loss 0.022777356207370758\n",
      "step 8703 loss 0.022776136174798012\n",
      "step 8704 loss 0.02277490869164467\n",
      "step 8705 loss 0.022773656994104385\n",
      "step 8706 loss 0.022772422060370445\n",
      "step 8707 loss 0.02277117781341076\n",
      "step 8708 loss 0.02276993915438652\n",
      "step 8709 loss 0.02276870235800743\n",
      "step 8710 loss 0.022767486050724983\n",
      "step 8711 loss 0.02276623249053955\n",
      "step 8712 loss 0.02276499569416046\n",
      "step 8713 loss 0.02276376262307167\n",
      "step 8714 loss 0.022762518376111984\n",
      "step 8715 loss 0.022761277854442596\n",
      "step 8716 loss 0.022760042920708656\n",
      "step 8717 loss 0.022758815437555313\n",
      "step 8718 loss 0.022757578641176224\n",
      "step 8719 loss 0.02275635302066803\n",
      "step 8720 loss 0.02275511808693409\n",
      "step 8721 loss 0.022753868252038956\n",
      "step 8722 loss 0.022752640768885612\n",
      "step 8723 loss 0.02275141328573227\n",
      "step 8724 loss 0.022750182077288628\n",
      "step 8725 loss 0.022748952731490135\n",
      "step 8726 loss 0.022747721523046494\n",
      "step 8727 loss 0.022746514528989792\n",
      "step 8728 loss 0.022745266556739807\n",
      "step 8729 loss 0.02274404466152191\n",
      "step 8730 loss 0.02274281531572342\n",
      "step 8731 loss 0.022741589695215225\n",
      "step 8732 loss 0.02274036407470703\n",
      "step 8733 loss 0.02273913472890854\n",
      "step 8734 loss 0.02273791842162609\n",
      "step 8735 loss 0.022736676037311554\n",
      "step 8736 loss 0.0227354709059\n",
      "step 8737 loss 0.022734250873327255\n",
      "step 8738 loss 0.022733043879270554\n",
      "step 8739 loss 0.02273181639611721\n",
      "step 8740 loss 0.022730600088834763\n",
      "step 8741 loss 0.02272937260568142\n",
      "step 8742 loss 0.02272815629839897\n",
      "step 8743 loss 0.02272694557905197\n",
      "step 8744 loss 0.022725732997059822\n",
      "step 8745 loss 0.022724533453583717\n",
      "step 8746 loss 0.022723305970430374\n",
      "step 8747 loss 0.022722091525793076\n",
      "step 8748 loss 0.02272089757025242\n",
      "step 8749 loss 0.022719670087099075\n",
      "step 8750 loss 0.022718481719493866\n",
      "step 8751 loss 0.022717243060469627\n",
      "step 8752 loss 0.022715991362929344\n",
      "step 8753 loss 0.02271473966538906\n",
      "step 8754 loss 0.022713489830493927\n",
      "step 8755 loss 0.022712254896759987\n",
      "step 8756 loss 0.022711005061864853\n",
      "step 8757 loss 0.02270975150167942\n",
      "step 8758 loss 0.022708499804139137\n",
      "step 8759 loss 0.022707242518663406\n",
      "step 8760 loss 0.022706005722284317\n",
      "step 8761 loss 0.022704744711518288\n",
      "step 8762 loss 0.022703485563397408\n",
      "step 8763 loss 0.022702211514115334\n",
      "step 8764 loss 0.02270093932747841\n",
      "step 8765 loss 0.022699665278196335\n",
      "step 8766 loss 0.022698387503623962\n",
      "step 8767 loss 0.022697102278470993\n",
      "step 8768 loss 0.02269584685564041\n",
      "step 8769 loss 0.022694550454616547\n",
      "step 8770 loss 0.022693291306495667\n",
      "step 8771 loss 0.022692007943987846\n",
      "step 8772 loss 0.022690732032060623\n",
      "step 8773 loss 0.022689461708068848\n",
      "step 8774 loss 0.022688187658786774\n",
      "step 8775 loss 0.02268691547214985\n",
      "step 8776 loss 0.02268563024699688\n",
      "step 8777 loss 0.022684363648295403\n",
      "step 8778 loss 0.02268308214843273\n",
      "step 8779 loss 0.022681808099150658\n",
      "step 8780 loss 0.022680535912513733\n",
      "step 8781 loss 0.022679254412651062\n",
      "step 8782 loss 0.02267799898982048\n",
      "step 8783 loss 0.02267671562731266\n",
      "step 8784 loss 0.02267545647919178\n",
      "step 8785 loss 0.022674180567264557\n",
      "step 8786 loss 0.02267291583120823\n",
      "step 8787 loss 0.022671647369861603\n",
      "step 8788 loss 0.022670386359095573\n",
      "step 8789 loss 0.02266911044716835\n",
      "step 8790 loss 0.022667843848466873\n",
      "step 8791 loss 0.022666579112410545\n",
      "step 8792 loss 0.022665319964289665\n",
      "step 8793 loss 0.022664042189717293\n",
      "step 8794 loss 0.022662783041596413\n",
      "step 8795 loss 0.02266153320670128\n",
      "step 8796 loss 0.022660277783870697\n",
      "step 8797 loss 0.022659001871943474\n",
      "step 8798 loss 0.022657744586467743\n",
      "step 8799 loss 0.02265648916363716\n",
      "step 8800 loss 0.02265523001551628\n",
      "step 8801 loss 0.022653980180621147\n",
      "step 8802 loss 0.02265271730720997\n",
      "step 8803 loss 0.02265148237347603\n",
      "step 8804 loss 0.02265021950006485\n",
      "step 8805 loss 0.022648965939879417\n",
      "step 8806 loss 0.022647710517048836\n",
      "step 8807 loss 0.022646451368927956\n",
      "step 8808 loss 0.02264520153403282\n",
      "step 8809 loss 0.022643957287073135\n",
      "step 8810 loss 0.022642705589532852\n",
      "step 8811 loss 0.022641463205218315\n",
      "step 8812 loss 0.022640209645032883\n",
      "step 8813 loss 0.02263895981013775\n",
      "step 8814 loss 0.022637730464339256\n",
      "step 8815 loss 0.022636475041508675\n",
      "step 8816 loss 0.02263523079454899\n",
      "step 8817 loss 0.02263399213552475\n",
      "step 8818 loss 0.022632746025919914\n",
      "step 8819 loss 0.02263151854276657\n",
      "step 8820 loss 0.022630266845226288\n",
      "step 8821 loss 0.02262902818620205\n",
      "step 8822 loss 0.02262778952717781\n",
      "step 8823 loss 0.022626562044024467\n",
      "step 8824 loss 0.022625334560871124\n",
      "step 8825 loss 0.02262408658862114\n",
      "step 8826 loss 0.022622860968112946\n",
      "step 8827 loss 0.02262161672115326\n",
      "step 8828 loss 0.02262038178741932\n",
      "step 8829 loss 0.022619156166911125\n",
      "step 8830 loss 0.022617926821112633\n",
      "step 8831 loss 0.022616686299443245\n",
      "step 8832 loss 0.022615451365709305\n",
      "step 8833 loss 0.02261423133313656\n",
      "step 8834 loss 0.022613005712628365\n",
      "step 8835 loss 0.02261178568005562\n",
      "step 8836 loss 0.022610556334257126\n",
      "step 8837 loss 0.02260933443903923\n",
      "step 8838 loss 0.022608110681176186\n",
      "step 8839 loss 0.02260688692331314\n",
      "step 8840 loss 0.022605672478675842\n",
      "step 8841 loss 0.022604437544941902\n",
      "step 8842 loss 0.022603221237659454\n",
      "step 8843 loss 0.02260199747979641\n",
      "step 8844 loss 0.02260078862309456\n",
      "step 8845 loss 0.022599559277296066\n",
      "step 8846 loss 0.022598344832658768\n",
      "step 8847 loss 0.02259712666273117\n",
      "step 8848 loss 0.022595906630158424\n",
      "step 8849 loss 0.02259470522403717\n",
      "step 8850 loss 0.022593487054109573\n",
      "step 8851 loss 0.02259228192269802\n",
      "step 8852 loss 0.022591063752770424\n",
      "step 8853 loss 0.022589851170778275\n",
      "step 8854 loss 0.022588638588786125\n",
      "step 8855 loss 0.022587433457374573\n",
      "step 8856 loss 0.022586209699511528\n",
      "step 8857 loss 0.02258501946926117\n",
      "step 8858 loss 0.022583814337849617\n",
      "step 8859 loss 0.02258259803056717\n",
      "step 8860 loss 0.02258140593767166\n",
      "step 8861 loss 0.02258019894361496\n",
      "step 8862 loss 0.022578991949558258\n",
      "step 8863 loss 0.022577790543437004\n",
      "step 8864 loss 0.02257659286260605\n",
      "step 8865 loss 0.022575385868549347\n",
      "step 8866 loss 0.022574186325073242\n",
      "step 8867 loss 0.022572992369532585\n",
      "step 8868 loss 0.02257179282605648\n",
      "step 8869 loss 0.02257058210670948\n",
      "step 8870 loss 0.02256939560174942\n",
      "step 8871 loss 0.022568190470337868\n",
      "step 8872 loss 0.02256699837744236\n",
      "step 8873 loss 0.02256581373512745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 8874 loss 0.022564608603715897\n",
      "step 8875 loss 0.022563420236110687\n",
      "step 8876 loss 0.02256222814321518\n",
      "step 8877 loss 0.022561021149158478\n",
      "step 8878 loss 0.022559836506843567\n",
      "step 8879 loss 0.022558657452464104\n",
      "step 8880 loss 0.022557459771633148\n",
      "step 8881 loss 0.02255627140402794\n",
      "step 8882 loss 0.022555088624358177\n",
      "step 8883 loss 0.022553903982043266\n",
      "step 8884 loss 0.022552723065018654\n",
      "step 8885 loss 0.022551536560058594\n",
      "step 8886 loss 0.022550344467163086\n",
      "step 8887 loss 0.022549167275428772\n",
      "step 8888 loss 0.022547975182533264\n",
      "step 8889 loss 0.022546805441379547\n",
      "step 8890 loss 0.02254561334848404\n",
      "step 8891 loss 0.022544436156749725\n",
      "step 8892 loss 0.02254326082766056\n",
      "step 8893 loss 0.022542081773281097\n",
      "step 8894 loss 0.022540900856256485\n",
      "step 8895 loss 0.02253972925245762\n",
      "step 8896 loss 0.02253853715956211\n",
      "step 8897 loss 0.022537359967827797\n",
      "step 8898 loss 0.022536201402544975\n",
      "step 8899 loss 0.022535018622875214\n",
      "step 8900 loss 0.022533850744366646\n",
      "step 8901 loss 0.022532666102051735\n",
      "step 8902 loss 0.022531509399414062\n",
      "step 8903 loss 0.02253032848238945\n",
      "step 8904 loss 0.022529153153300285\n",
      "step 8905 loss 0.022527994588017464\n",
      "step 8906 loss 0.02252683788537979\n",
      "step 8907 loss 0.022525662556290627\n",
      "step 8908 loss 0.02252449095249176\n",
      "step 8909 loss 0.02252332493662834\n",
      "step 8910 loss 0.022522157058119774\n",
      "step 8911 loss 0.022520994767546654\n",
      "step 8912 loss 0.022519832476973534\n",
      "step 8913 loss 0.022518668323755264\n",
      "step 8914 loss 0.022517500445246696\n",
      "step 8915 loss 0.022516336292028427\n",
      "step 8916 loss 0.02251516282558441\n",
      "step 8917 loss 0.022514013573527336\n",
      "step 8918 loss 0.022512860596179962\n",
      "step 8919 loss 0.022511692717671394\n",
      "step 8920 loss 0.02251053787767887\n",
      "step 8921 loss 0.0225093811750412\n",
      "step 8922 loss 0.02250823564827442\n",
      "step 8923 loss 0.022507060319185257\n",
      "step 8924 loss 0.022505905479192734\n",
      "step 8925 loss 0.022504758089780807\n",
      "step 8926 loss 0.022503605112433434\n",
      "step 8927 loss 0.022502444684505463\n",
      "step 8928 loss 0.022501297295093536\n",
      "step 8929 loss 0.022500138729810715\n",
      "step 8930 loss 0.02249898388981819\n",
      "step 8931 loss 0.022497838363051414\n",
      "step 8932 loss 0.022496676072478294\n",
      "step 8933 loss 0.02249555103480816\n",
      "step 8934 loss 0.022494392469525337\n",
      "step 8935 loss 0.022493241354823112\n",
      "step 8936 loss 0.022492095828056335\n",
      "step 8937 loss 0.02249094471335411\n",
      "step 8938 loss 0.02248980477452278\n",
      "step 8939 loss 0.022488664835691452\n",
      "step 8940 loss 0.022487521171569824\n",
      "step 8941 loss 0.02248637191951275\n",
      "step 8942 loss 0.02248523198068142\n",
      "step 8943 loss 0.022484097629785538\n",
      "step 8944 loss 0.022482948377728462\n",
      "step 8945 loss 0.022481804713606834\n",
      "step 8946 loss 0.022480664774775505\n",
      "step 8947 loss 0.022479524835944176\n",
      "step 8948 loss 0.022478384897112846\n",
      "step 8949 loss 0.02247725985944271\n",
      "step 8950 loss 0.022476114332675934\n",
      "step 8951 loss 0.0224749855697155\n",
      "step 8952 loss 0.022473834455013275\n",
      "step 8953 loss 0.02247270569205284\n",
      "step 8954 loss 0.022471580654382706\n",
      "step 8955 loss 0.022470440715551376\n",
      "step 8956 loss 0.02246931754052639\n",
      "step 8957 loss 0.022468194365501404\n",
      "step 8958 loss 0.02246704138815403\n",
      "step 8959 loss 0.022465916350483894\n",
      "step 8960 loss 0.022464780136942863\n",
      "step 8961 loss 0.022463655099272728\n",
      "step 8962 loss 0.022462524473667145\n",
      "step 8963 loss 0.022461390122771263\n",
      "step 8964 loss 0.022460266947746277\n",
      "step 8965 loss 0.02245914191007614\n",
      "step 8966 loss 0.022458018735051155\n",
      "step 8967 loss 0.022456886246800423\n",
      "step 8968 loss 0.022455761209130287\n",
      "step 8969 loss 0.02245463617146015\n",
      "step 8970 loss 0.022453520447015762\n",
      "step 8971 loss 0.02245239168405533\n",
      "step 8972 loss 0.02245127223432064\n",
      "step 8973 loss 0.022450150921940804\n",
      "step 8974 loss 0.022449040785431862\n",
      "step 8975 loss 0.02244790643453598\n",
      "step 8976 loss 0.02244679629802704\n",
      "step 8977 loss 0.02244567684829235\n",
      "step 8978 loss 0.022444553673267365\n",
      "step 8979 loss 0.022443432360887527\n",
      "step 8980 loss 0.022442316636443138\n",
      "step 8981 loss 0.022441206499934196\n",
      "step 8982 loss 0.022440094500780106\n",
      "step 8983 loss 0.022438980638980865\n",
      "step 8984 loss 0.02243783511221409\n",
      "step 8985 loss 0.02243674360215664\n",
      "step 8986 loss 0.022435637190937996\n",
      "step 8987 loss 0.02243451029062271\n",
      "step 8988 loss 0.022433409467339516\n",
      "step 8989 loss 0.022432299330830574\n",
      "step 8990 loss 0.02243116870522499\n",
      "step 8991 loss 0.022430062294006348\n",
      "step 8992 loss 0.022428961470723152\n",
      "step 8993 loss 0.02242785133421421\n",
      "step 8994 loss 0.022426757961511612\n",
      "step 8995 loss 0.022425632923841476\n",
      "step 8996 loss 0.02242453023791313\n",
      "step 8997 loss 0.022423414513468742\n",
      "step 8998 loss 0.0224223043769598\n",
      "step 8999 loss 0.022421209141612053\n",
      "step 9000 loss 0.022420108318328857\n",
      "step 9001 loss 0.022419016808271408\n",
      "step 9002 loss 0.02241790108382702\n",
      "step 9003 loss 0.022416802123188972\n",
      "step 9004 loss 0.02241569571197033\n",
      "step 9005 loss 0.02241460233926773\n",
      "step 9006 loss 0.02241349406540394\n",
      "step 9007 loss 0.022412391379475594\n",
      "step 9008 loss 0.022411296144127846\n",
      "step 9009 loss 0.0224101971834898\n",
      "step 9010 loss 0.022409096360206604\n",
      "step 9011 loss 0.02240798994898796\n",
      "step 9012 loss 0.022406907752156258\n",
      "step 9013 loss 0.02240579202771187\n",
      "step 9014 loss 0.022404713556170464\n",
      "step 9015 loss 0.022403614595532417\n",
      "step 9016 loss 0.022402513772249222\n",
      "step 9017 loss 0.02240142412483692\n",
      "step 9018 loss 0.022400327026844025\n",
      "step 9019 loss 0.022399239242076874\n",
      "step 9020 loss 0.022398140281438828\n",
      "step 9021 loss 0.022397056221961975\n",
      "step 9022 loss 0.02239595353603363\n",
      "step 9023 loss 0.022394860163331032\n",
      "step 9024 loss 0.022393789142370224\n",
      "step 9025 loss 0.02239268273115158\n",
      "step 9026 loss 0.02239161543548107\n",
      "step 9027 loss 0.02239050343632698\n",
      "step 9028 loss 0.022389430552721024\n",
      "step 9029 loss 0.022388333454728127\n",
      "step 9030 loss 0.022387251257896423\n",
      "step 9031 loss 0.022386159747838974\n",
      "step 9032 loss 0.022385086864233017\n",
      "step 9033 loss 0.022383980453014374\n",
      "step 9034 loss 0.022382907569408417\n",
      "step 9035 loss 0.02238183096051216\n",
      "step 9036 loss 0.022380737587809563\n",
      "step 9037 loss 0.022379664704203606\n",
      "step 9038 loss 0.02237858809530735\n",
      "step 9039 loss 0.022377489134669304\n",
      "step 9040 loss 0.0223764069378376\n",
      "step 9041 loss 0.022375335916876793\n",
      "step 9042 loss 0.02237424999475479\n",
      "step 9043 loss 0.022373173385858536\n",
      "step 9044 loss 0.022372093051671982\n",
      "step 9045 loss 0.022371012717485428\n",
      "step 9046 loss 0.022369947284460068\n",
      "step 9047 loss 0.02236887253820896\n",
      "step 9048 loss 0.02236778289079666\n",
      "step 9049 loss 0.022366713732481003\n",
      "step 9050 loss 0.022365637123584747\n",
      "step 9051 loss 0.02236456423997879\n",
      "step 9052 loss 0.022363483905792236\n",
      "step 9053 loss 0.022362401708960533\n",
      "step 9054 loss 0.022361332550644875\n",
      "step 9055 loss 0.022360272705554962\n",
      "step 9056 loss 0.022359199821949005\n",
      "step 9057 loss 0.022358136251568794\n",
      "step 9058 loss 0.022357063367962837\n",
      "step 9059 loss 0.022355979308485985\n",
      "step 9060 loss 0.02235490083694458\n",
      "step 9061 loss 0.02235383912920952\n",
      "step 9062 loss 0.02235277183353901\n",
      "step 9063 loss 0.022351693361997604\n",
      "step 9064 loss 0.02235064096748829\n",
      "step 9065 loss 0.022349558770656586\n",
      "step 9066 loss 0.022348495200276375\n",
      "step 9067 loss 0.02234744466841221\n",
      "step 9068 loss 0.02234637178480625\n",
      "step 9069 loss 0.02234531380236149\n",
      "step 9070 loss 0.02234424091875553\n",
      "step 9071 loss 0.02234317548573017\n",
      "step 9072 loss 0.02234211191534996\n",
      "step 9073 loss 0.022341040894389153\n",
      "step 9074 loss 0.022339988499879837\n",
      "step 9075 loss 0.022338928654789925\n",
      "step 9076 loss 0.02233785204589367\n",
      "step 9077 loss 0.022336803376674652\n",
      "step 9078 loss 0.022335730493068695\n",
      "step 9079 loss 0.02233467623591423\n",
      "step 9080 loss 0.02233361266553402\n",
      "step 9081 loss 0.022332563996315002\n",
      "step 9082 loss 0.022331498563289642\n",
      "step 9083 loss 0.022330444306135178\n",
      "step 9084 loss 0.022329380735754967\n",
      "step 9085 loss 0.022328343242406845\n",
      "step 9086 loss 0.022327270358800888\n",
      "step 9087 loss 0.022326212376356125\n",
      "step 9088 loss 0.022325165569782257\n",
      "step 9089 loss 0.022324107587337494\n",
      "step 9090 loss 0.022323040291666985\n",
      "step 9091 loss 0.02232198789715767\n",
      "step 9092 loss 0.0223209448158741\n",
      "step 9093 loss 0.022319884970784187\n",
      "step 9094 loss 0.02231883443892002\n",
      "step 9095 loss 0.022317785769701004\n",
      "step 9096 loss 0.022316735237836838\n",
      "step 9097 loss 0.02231568656861782\n",
      "step 9098 loss 0.022314630448818207\n",
      "step 9099 loss 0.02231357991695404\n",
      "step 9100 loss 0.022312529385089874\n",
      "step 9101 loss 0.022311486303806305\n",
      "step 9102 loss 0.022310428321361542\n",
      "step 9103 loss 0.022309381514787674\n",
      "step 9104 loss 0.022308334708213806\n",
      "step 9105 loss 0.022307291626930237\n",
      "step 9106 loss 0.02230624109506607\n",
      "step 9107 loss 0.022305188700556755\n",
      "step 9108 loss 0.022304149344563484\n",
      "step 9109 loss 0.022303104400634766\n",
      "step 9110 loss 0.0223020501434803\n",
      "step 9111 loss 0.022301016375422478\n",
      "step 9112 loss 0.02229996956884861\n",
      "step 9113 loss 0.022298917174339294\n",
      "step 9114 loss 0.022297892719507217\n",
      "step 9115 loss 0.022296814247965813\n",
      "step 9116 loss 0.022295750677585602\n",
      "step 9117 loss 0.022294674068689346\n",
      "step 9118 loss 0.022293586283922195\n",
      "step 9119 loss 0.02229251153767109\n",
      "step 9120 loss 0.02229144051671028\n",
      "step 9121 loss 0.02229035459458828\n",
      "step 9122 loss 0.022289268672466278\n",
      "step 9123 loss 0.02228819765150547\n",
      "step 9124 loss 0.02228710614144802\n",
      "step 9125 loss 0.022286009043455124\n",
      "step 9126 loss 0.02228493243455887\n",
      "step 9127 loss 0.022283853963017464\n",
      "step 9128 loss 0.022282777354121208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 9129 loss 0.022281670942902565\n",
      "step 9130 loss 0.02228059247136116\n",
      "step 9131 loss 0.02227950654923916\n",
      "step 9132 loss 0.022278420627117157\n",
      "step 9133 loss 0.022277340292930603\n",
      "step 9134 loss 0.0222762580960989\n",
      "step 9135 loss 0.02227516658604145\n",
      "step 9136 loss 0.022274075075984\n",
      "step 9137 loss 0.022273002192378044\n",
      "step 9138 loss 0.022271905094385147\n",
      "step 9139 loss 0.022270822897553444\n",
      "step 9140 loss 0.02226974256336689\n",
      "step 9141 loss 0.022268662229180336\n",
      "step 9142 loss 0.022267580032348633\n",
      "step 9143 loss 0.022266492247581482\n",
      "step 9144 loss 0.022265411913394928\n",
      "step 9145 loss 0.022264327853918076\n",
      "step 9146 loss 0.022263234481215477\n",
      "step 9147 loss 0.022262172773480415\n",
      "step 9148 loss 0.022261084988713264\n",
      "step 9149 loss 0.02226000279188156\n",
      "step 9150 loss 0.02225891314446926\n",
      "step 9151 loss 0.022257840260863304\n",
      "step 9152 loss 0.02225676365196705\n",
      "step 9153 loss 0.022255681455135345\n",
      "step 9154 loss 0.022254614159464836\n",
      "step 9155 loss 0.022253528237342834\n",
      "step 9156 loss 0.022252462804317474\n",
      "step 9157 loss 0.022251378744840622\n",
      "step 9158 loss 0.022250305861234665\n",
      "step 9159 loss 0.022249216213822365\n",
      "step 9160 loss 0.022248148918151855\n",
      "step 9161 loss 0.022247081622481346\n",
      "step 9162 loss 0.022246012464165688\n",
      "step 9163 loss 0.022244928404688835\n",
      "step 9164 loss 0.02224385179579258\n",
      "step 9165 loss 0.022242778912186623\n",
      "step 9166 loss 0.022241702303290367\n",
      "step 9167 loss 0.022240648046135902\n",
      "step 9168 loss 0.022239577025175095\n",
      "step 9169 loss 0.022238507866859436\n",
      "step 9170 loss 0.022237427532672882\n",
      "step 9171 loss 0.02223636582493782\n",
      "step 9172 loss 0.022235307842493057\n",
      "step 9173 loss 0.022234225645661354\n",
      "step 9174 loss 0.022233152762055397\n",
      "step 9175 loss 0.022232087329030037\n",
      "step 9176 loss 0.022231027483940125\n",
      "step 9177 loss 0.02222994714975357\n",
      "step 9178 loss 0.022228894755244255\n",
      "step 9179 loss 0.022227825596928596\n",
      "step 9180 loss 0.022226765751838684\n",
      "step 9181 loss 0.022225694730877876\n",
      "step 9182 loss 0.022224636748433113\n",
      "step 9183 loss 0.022223567590117455\n",
      "step 9184 loss 0.022222502157092094\n",
      "step 9185 loss 0.022221438586711884\n",
      "step 9186 loss 0.022220391780138016\n",
      "step 9187 loss 0.022219331935048103\n",
      "step 9188 loss 0.022218255326151848\n",
      "step 9189 loss 0.02221721224486828\n",
      "step 9190 loss 0.02221614681184292\n",
      "step 9191 loss 0.022215090692043304\n",
      "step 9192 loss 0.022214027121663094\n",
      "step 9193 loss 0.022212963551282883\n",
      "step 9194 loss 0.022211909294128418\n",
      "step 9195 loss 0.022210845723748207\n",
      "step 9196 loss 0.022209791466593742\n",
      "step 9197 loss 0.022208716720342636\n",
      "step 9198 loss 0.02220766991376877\n",
      "step 9199 loss 0.02220659889280796\n",
      "step 9200 loss 0.022205539047718048\n",
      "step 9201 loss 0.02220449037849903\n",
      "step 9202 loss 0.022203417494893074\n",
      "step 9203 loss 0.022202378138899803\n",
      "step 9204 loss 0.022201314568519592\n",
      "step 9205 loss 0.02220025472342968\n",
      "step 9206 loss 0.022199181839823723\n",
      "step 9207 loss 0.022198129445314407\n",
      "step 9208 loss 0.022197069600224495\n",
      "step 9209 loss 0.022196020931005478\n",
      "step 9210 loss 0.022194944322109222\n",
      "step 9211 loss 0.022193865850567818\n",
      "step 9212 loss 0.022192802280187607\n",
      "step 9213 loss 0.02219172567129135\n",
      "step 9214 loss 0.022190632298588753\n",
      "step 9215 loss 0.022189555689692497\n",
      "step 9216 loss 0.022188467904925346\n",
      "step 9217 loss 0.02218738943338394\n",
      "step 9218 loss 0.022186284884810448\n",
      "step 9219 loss 0.022185197100043297\n",
      "step 9220 loss 0.022184133529663086\n",
      "step 9221 loss 0.022183051332831383\n",
      "step 9222 loss 0.022181959822773933\n",
      "step 9223 loss 0.022180862724781036\n",
      "step 9224 loss 0.02217978797852993\n",
      "step 9225 loss 0.022178683429956436\n",
      "step 9226 loss 0.022177614271640778\n",
      "step 9227 loss 0.022176513448357582\n",
      "step 9228 loss 0.02217542566359043\n",
      "step 9229 loss 0.022174334153532982\n",
      "step 9230 loss 0.022173253819346428\n",
      "step 9231 loss 0.02217218466103077\n",
      "step 9232 loss 0.022171087563037872\n",
      "step 9233 loss 0.02216999977827072\n",
      "step 9234 loss 0.022168919444084167\n",
      "step 9235 loss 0.022167839109897614\n",
      "step 9236 loss 0.022166747599840164\n",
      "step 9237 loss 0.02216567099094391\n",
      "step 9238 loss 0.022164568305015564\n",
      "step 9239 loss 0.022163495421409607\n",
      "step 9240 loss 0.022162413224577904\n",
      "step 9241 loss 0.022161336615681648\n",
      "step 9242 loss 0.022160258144140244\n",
      "step 9243 loss 0.022159157320857048\n",
      "step 9244 loss 0.022158049046993256\n",
      "step 9245 loss 0.022156938910484314\n",
      "step 9246 loss 0.02215583249926567\n",
      "step 9247 loss 0.022154726088047028\n",
      "step 9248 loss 0.022153617814183235\n",
      "step 9249 loss 0.022152511402964592\n",
      "step 9250 loss 0.022151391953229904\n",
      "step 9251 loss 0.022150272503495216\n",
      "step 9252 loss 0.02214916981756687\n",
      "step 9253 loss 0.022148054093122482\n",
      "step 9254 loss 0.022146956995129585\n",
      "step 9255 loss 0.022145822644233704\n",
      "step 9256 loss 0.022144712507724762\n",
      "step 9257 loss 0.022143596783280373\n",
      "step 9258 loss 0.02214248664677143\n",
      "step 9259 loss 0.022141380235552788\n",
      "step 9260 loss 0.022140266373753548\n",
      "step 9261 loss 0.022139158099889755\n",
      "step 9262 loss 0.022138027474284172\n",
      "step 9263 loss 0.02213691733777523\n",
      "step 9264 loss 0.02213582582771778\n",
      "step 9265 loss 0.02213468961417675\n",
      "step 9266 loss 0.022133583202958107\n",
      "step 9267 loss 0.022132478654384613\n",
      "step 9268 loss 0.02213137224316597\n",
      "step 9269 loss 0.02213026024401188\n",
      "step 9270 loss 0.02212914265692234\n",
      "step 9271 loss 0.022128047421574593\n",
      "step 9272 loss 0.022126924246549606\n",
      "step 9273 loss 0.022125812247395515\n",
      "step 9274 loss 0.022124705836176872\n",
      "step 9275 loss 0.022123608738183975\n",
      "step 9276 loss 0.022122496739029884\n",
      "step 9277 loss 0.022121381014585495\n",
      "step 9278 loss 0.022120265290141106\n",
      "step 9279 loss 0.022119173780083656\n",
      "step 9280 loss 0.02211807295680046\n",
      "step 9281 loss 0.022116951644420624\n",
      "step 9282 loss 0.022115854546427727\n",
      "step 9283 loss 0.022114725783467293\n",
      "step 9284 loss 0.022113647311925888\n",
      "step 9285 loss 0.022112537175416946\n",
      "step 9286 loss 0.022111425176262856\n",
      "step 9287 loss 0.022110333666205406\n",
      "step 9288 loss 0.022109242156147957\n",
      "step 9289 loss 0.022108135744929314\n",
      "step 9290 loss 0.022107020020484924\n",
      "step 9291 loss 0.022105926647782326\n",
      "step 9292 loss 0.02210482396185398\n",
      "step 9293 loss 0.022103725001215935\n",
      "step 9294 loss 0.022102637216448784\n",
      "step 9295 loss 0.02210152894258499\n",
      "step 9296 loss 0.022100426256656647\n",
      "step 9297 loss 0.022099323570728302\n",
      "step 9298 loss 0.0220982413738966\n",
      "step 9299 loss 0.022097138687968254\n",
      "step 9300 loss 0.022096052765846252\n",
      "step 9301 loss 0.022094950079917908\n",
      "step 9302 loss 0.02209385856986046\n",
      "step 9303 loss 0.02209276147186756\n",
      "step 9304 loss 0.022091664373874664\n",
      "step 9305 loss 0.022090572863817215\n",
      "step 9306 loss 0.022089475765824318\n",
      "step 9307 loss 0.02208838425576687\n",
      "step 9308 loss 0.022087303921580315\n",
      "step 9309 loss 0.022086210548877716\n",
      "step 9310 loss 0.02208510786294937\n",
      "step 9311 loss 0.02208402007818222\n",
      "step 9312 loss 0.022082939743995667\n",
      "step 9313 loss 0.022081857547163963\n",
      "step 9314 loss 0.022080745548009872\n",
      "step 9315 loss 0.02207966521382332\n",
      "step 9316 loss 0.02207857556641102\n",
      "step 9317 loss 0.02207748405635357\n",
      "step 9318 loss 0.022076401859521866\n",
      "step 9319 loss 0.022075317800045013\n",
      "step 9320 loss 0.022074230015277863\n",
      "step 9321 loss 0.022073134779930115\n",
      "step 9322 loss 0.02207205258309841\n",
      "step 9323 loss 0.022070979699492455\n",
      "step 9324 loss 0.022069890052080154\n",
      "step 9325 loss 0.022068800404667854\n",
      "step 9326 loss 0.0220677237957716\n",
      "step 9327 loss 0.022066645324230194\n",
      "step 9328 loss 0.022065535187721252\n",
      "step 9329 loss 0.022064480930566788\n",
      "step 9330 loss 0.02206338942050934\n",
      "step 9331 loss 0.022062310948967934\n",
      "step 9332 loss 0.022061211988329887\n",
      "step 9333 loss 0.02206014096736908\n",
      "step 9334 loss 0.022059068083763123\n",
      "step 9335 loss 0.022057976573705673\n",
      "step 9336 loss 0.02205691486597061\n",
      "step 9337 loss 0.02205580659210682\n",
      "step 9338 loss 0.022054746747016907\n",
      "step 9339 loss 0.022053666412830353\n",
      "step 9340 loss 0.022052587941288948\n",
      "step 9341 loss 0.022051502019166946\n",
      "step 9342 loss 0.02205044776201248\n",
      "step 9343 loss 0.022049350664019585\n",
      "step 9344 loss 0.02204829268157482\n",
      "step 9345 loss 0.022047212347388268\n",
      "step 9346 loss 0.022046132013201714\n",
      "step 9347 loss 0.022045064717531204\n",
      "step 9348 loss 0.022043993696570396\n",
      "step 9349 loss 0.02204291895031929\n",
      "step 9350 loss 0.022041840478777885\n",
      "step 9351 loss 0.022040773183107376\n",
      "step 9352 loss 0.02203969471156597\n",
      "step 9353 loss 0.022038616240024567\n",
      "step 9354 loss 0.022037548944354057\n",
      "step 9355 loss 0.022036490961909294\n",
      "step 9356 loss 0.02203541062772274\n",
      "step 9357 loss 0.022034337744116783\n",
      "step 9358 loss 0.02203328348696232\n",
      "step 9359 loss 0.022032208740711212\n",
      "step 9360 loss 0.022031128406524658\n",
      "step 9361 loss 0.022030066698789597\n",
      "step 9362 loss 0.02202899381518364\n",
      "step 9363 loss 0.022027933970093727\n",
      "step 9364 loss 0.022026853635907173\n",
      "step 9365 loss 0.02202579751610756\n",
      "step 9366 loss 0.022024735808372498\n",
      "step 9367 loss 0.02202366106212139\n",
      "step 9368 loss 0.022022590041160583\n",
      "step 9369 loss 0.022021520882844925\n",
      "step 9370 loss 0.02202046662569046\n",
      "step 9371 loss 0.022019395604729652\n",
      "step 9372 loss 0.022018341347575188\n",
      "step 9373 loss 0.02201726846396923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 9374 loss 0.022016208618879318\n",
      "step 9375 loss 0.022015152499079704\n",
      "step 9376 loss 0.022014088928699493\n",
      "step 9377 loss 0.022013025358319283\n",
      "step 9378 loss 0.022011971101164818\n",
      "step 9379 loss 0.022010894492268562\n",
      "step 9380 loss 0.022009840235114098\n",
      "step 9381 loss 0.022008784115314484\n",
      "step 9382 loss 0.022007720544934273\n",
      "step 9383 loss 0.022006655111908913\n",
      "step 9384 loss 0.022005602717399597\n",
      "step 9385 loss 0.022004537284374237\n",
      "step 9386 loss 0.022003483027219772\n",
      "step 9387 loss 0.02200242690742016\n",
      "step 9388 loss 0.0220013614743948\n",
      "step 9389 loss 0.022000296041369438\n",
      "step 9390 loss 0.021999254822731018\n",
      "step 9391 loss 0.021998194977641106\n",
      "step 9392 loss 0.02199713885784149\n",
      "step 9393 loss 0.02199607901275158\n",
      "step 9394 loss 0.021995024755597115\n",
      "step 9395 loss 0.02199397049844265\n",
      "step 9396 loss 0.021992914378643036\n",
      "step 9397 loss 0.021991867572069168\n",
      "step 9398 loss 0.021990807726979256\n",
      "step 9399 loss 0.021989760920405388\n",
      "step 9400 loss 0.021988699212670326\n",
      "step 9401 loss 0.021987641230225563\n",
      "step 9402 loss 0.021986588835716248\n",
      "step 9403 loss 0.021985545754432678\n",
      "step 9404 loss 0.02198447659611702\n",
      "step 9405 loss 0.021983452141284943\n",
      "step 9406 loss 0.021982382982969284\n",
      "step 9407 loss 0.02198134921491146\n",
      "step 9408 loss 0.02198028564453125\n",
      "step 9409 loss 0.02197924070060253\n",
      "step 9410 loss 0.021978182718157768\n",
      "step 9411 loss 0.021977143362164497\n",
      "step 9412 loss 0.021976102143526077\n",
      "step 9413 loss 0.021975025534629822\n",
      "step 9414 loss 0.0219739843159914\n",
      "step 9415 loss 0.021972939372062683\n",
      "step 9416 loss 0.021971890702843666\n",
      "step 9417 loss 0.021970849484205246\n",
      "step 9418 loss 0.021969793364405632\n",
      "step 9419 loss 0.021968750283122063\n",
      "step 9420 loss 0.021967710927128792\n",
      "step 9421 loss 0.02196664735674858\n",
      "step 9422 loss 0.021965617313981056\n",
      "step 9423 loss 0.021964583545923233\n",
      "step 9424 loss 0.021963519975543022\n",
      "step 9425 loss 0.021962475031614304\n",
      "step 9426 loss 0.021961450576782227\n",
      "step 9427 loss 0.021960387006402016\n",
      "step 9428 loss 0.021959347650408745\n",
      "step 9429 loss 0.021958300843834877\n",
      "step 9430 loss 0.02195725403726101\n",
      "step 9431 loss 0.021956220269203186\n",
      "step 9432 loss 0.021955175325274467\n",
      "step 9433 loss 0.02195412665605545\n",
      "step 9434 loss 0.021953091025352478\n",
      "step 9435 loss 0.021952038630843163\n",
      "step 9436 loss 0.021951019763946533\n",
      "step 9437 loss 0.021949972957372665\n",
      "step 9438 loss 0.02194892428815365\n",
      "step 9439 loss 0.021947896108031273\n",
      "step 9440 loss 0.021946849301457405\n",
      "step 9441 loss 0.021945806220173836\n",
      "step 9442 loss 0.021944766864180565\n",
      "step 9443 loss 0.02194373309612274\n",
      "step 9444 loss 0.021942682564258575\n",
      "step 9445 loss 0.021941661834716797\n",
      "step 9446 loss 0.021940607577562332\n",
      "step 9447 loss 0.021939583122730255\n",
      "step 9448 loss 0.02193852886557579\n",
      "step 9449 loss 0.021937504410743713\n",
      "step 9450 loss 0.021936453878879547\n",
      "step 9451 loss 0.021935435011982918\n",
      "step 9452 loss 0.021934382617473602\n",
      "step 9453 loss 0.02193336747586727\n",
      "step 9454 loss 0.02193232625722885\n",
      "step 9455 loss 0.021931283175945282\n",
      "step 9456 loss 0.021930251270532608\n",
      "step 9457 loss 0.021929215639829636\n",
      "step 9458 loss 0.021928181871771812\n",
      "step 9459 loss 0.021927155554294586\n",
      "step 9460 loss 0.021926097571849823\n",
      "step 9461 loss 0.02192508615553379\n",
      "step 9462 loss 0.021924056112766266\n",
      "step 9463 loss 0.021923022344708443\n",
      "step 9464 loss 0.021921977400779724\n",
      "step 9465 loss 0.021920956671237946\n",
      "step 9466 loss 0.021919917315244675\n",
      "step 9467 loss 0.02191888727247715\n",
      "step 9468 loss 0.021917857229709625\n",
      "step 9469 loss 0.021916834637522697\n",
      "step 9470 loss 0.021915793418884277\n",
      "step 9471 loss 0.021914774551987648\n",
      "step 9472 loss 0.02191372960805893\n",
      "step 9473 loss 0.021912699565291405\n",
      "step 9474 loss 0.021911663934588432\n",
      "step 9475 loss 0.021910639479756355\n",
      "step 9476 loss 0.02190961129963398\n",
      "step 9477 loss 0.021908586844801903\n",
      "step 9478 loss 0.021907556802034378\n",
      "step 9479 loss 0.021906523033976555\n",
      "step 9480 loss 0.021905526518821716\n",
      "step 9481 loss 0.02190447598695755\n",
      "step 9482 loss 0.021903447806835175\n",
      "step 9483 loss 0.021902423352003098\n",
      "step 9484 loss 0.02190139703452587\n",
      "step 9485 loss 0.021900378167629242\n",
      "step 9486 loss 0.02189934253692627\n",
      "step 9487 loss 0.021898318082094193\n",
      "step 9488 loss 0.02189728245139122\n",
      "step 9489 loss 0.02189626544713974\n",
      "step 9490 loss 0.02189522236585617\n",
      "step 9491 loss 0.021894210949540138\n",
      "step 9492 loss 0.021893180906772614\n",
      "step 9493 loss 0.02189214900135994\n",
      "step 9494 loss 0.021891126409173012\n",
      "step 9495 loss 0.02189011313021183\n",
      "step 9496 loss 0.021889084950089455\n",
      "step 9497 loss 0.021888066083192825\n",
      "step 9498 loss 0.02188704162836075\n",
      "step 9499 loss 0.021886007860302925\n",
      "step 9500 loss 0.021884994581341743\n",
      "step 9501 loss 0.021883970126509666\n",
      "step 9502 loss 0.02188294380903244\n",
      "step 9503 loss 0.021881921216845512\n",
      "step 9504 loss 0.02188091352581978\n",
      "step 9505 loss 0.021879883483052254\n",
      "step 9506 loss 0.021878857165575027\n",
      "step 9507 loss 0.021877847611904144\n",
      "step 9508 loss 0.021876811981201172\n",
      "step 9509 loss 0.02187579870223999\n",
      "step 9510 loss 0.02187478356063366\n",
      "step 9511 loss 0.021873770281672478\n",
      "step 9512 loss 0.021872734650969505\n",
      "step 9513 loss 0.021871713921427727\n",
      "step 9514 loss 0.021870695054531097\n",
      "step 9515 loss 0.021869687363505363\n",
      "step 9516 loss 0.021868662908673286\n",
      "step 9517 loss 0.02186763286590576\n",
      "step 9518 loss 0.021866634488105774\n",
      "step 9519 loss 0.021865595132112503\n",
      "step 9520 loss 0.02186458744108677\n",
      "step 9521 loss 0.02186357043683529\n",
      "step 9522 loss 0.02186254970729351\n",
      "step 9523 loss 0.021861521527171135\n",
      "step 9524 loss 0.0218605138361454\n",
      "step 9525 loss 0.021859489381313324\n",
      "step 9526 loss 0.021858476102352142\n",
      "step 9527 loss 0.02185746654868126\n",
      "step 9528 loss 0.02185644395649433\n",
      "step 9529 loss 0.021855428814888\n",
      "step 9530 loss 0.02185440994799137\n",
      "step 9531 loss 0.021853413432836533\n",
      "step 9532 loss 0.021852392703294754\n",
      "step 9533 loss 0.021851371973752975\n",
      "step 9534 loss 0.021850349381566048\n",
      "step 9535 loss 0.021849341690540314\n",
      "step 9536 loss 0.021848315373063087\n",
      "step 9537 loss 0.021847298368811607\n",
      "step 9538 loss 0.02184629999101162\n",
      "step 9539 loss 0.021845288574695587\n",
      "step 9540 loss 0.02184426225721836\n",
      "step 9541 loss 0.021843252703547478\n",
      "step 9542 loss 0.02184222638607025\n",
      "step 9543 loss 0.02184123359620571\n",
      "step 9544 loss 0.021840224042534828\n",
      "step 9545 loss 0.02183920331299305\n",
      "step 9546 loss 0.021838173270225525\n",
      "step 9547 loss 0.021837176755070686\n",
      "step 9548 loss 0.02183615230023861\n",
      "step 9549 loss 0.021835150197148323\n",
      "step 9550 loss 0.02183414064347744\n",
      "step 9551 loss 0.021833127364516258\n",
      "step 9552 loss 0.021832117810845375\n",
      "step 9553 loss 0.021831104531884193\n",
      "step 9554 loss 0.021830109879374504\n",
      "step 9555 loss 0.02182907983660698\n",
      "step 9556 loss 0.02182805724442005\n",
      "step 9557 loss 0.021827053278684616\n",
      "step 9558 loss 0.02182604745030403\n",
      "step 9559 loss 0.02182503417134285\n",
      "step 9560 loss 0.021824035793542862\n",
      "step 9561 loss 0.02182302065193653\n",
      "step 9562 loss 0.02182200364768505\n",
      "step 9563 loss 0.021820997819304466\n",
      "step 9564 loss 0.02181997522711754\n",
      "step 9565 loss 0.021818989887833595\n",
      "step 9566 loss 0.021817971020936966\n",
      "step 9567 loss 0.02181697078049183\n",
      "step 9568 loss 0.021815961226820946\n",
      "step 9569 loss 0.02181493677198887\n",
      "step 9570 loss 0.021813921630382538\n",
      "step 9571 loss 0.021812930703163147\n",
      "step 9572 loss 0.02181192673742771\n",
      "step 9573 loss 0.021810904145240784\n",
      "step 9574 loss 0.02180991694331169\n",
      "step 9575 loss 0.02180889993906021\n",
      "step 9576 loss 0.021807894110679626\n",
      "step 9577 loss 0.021806880831718445\n",
      "step 9578 loss 0.02180587314069271\n",
      "step 9579 loss 0.021804863587021828\n",
      "step 9580 loss 0.02180386148393154\n",
      "step 9581 loss 0.02180285006761551\n",
      "step 9582 loss 0.02180185355246067\n",
      "step 9583 loss 0.02180083841085434\n",
      "step 9584 loss 0.021799851208925247\n",
      "step 9585 loss 0.021798837929964066\n",
      "step 9586 loss 0.02179783396422863\n",
      "step 9587 loss 0.021796815097332\n",
      "step 9588 loss 0.021795816719532013\n",
      "step 9589 loss 0.021794790402054787\n",
      "step 9590 loss 0.021793797612190247\n",
      "step 9591 loss 0.021792804822325706\n",
      "step 9592 loss 0.021791791543364525\n",
      "step 9593 loss 0.021790791302919388\n",
      "step 9594 loss 0.021789783611893654\n",
      "step 9595 loss 0.02178877405822277\n",
      "step 9596 loss 0.02178776264190674\n",
      "step 9597 loss 0.0217867661267519\n",
      "step 9598 loss 0.02178577333688736\n",
      "step 9599 loss 0.02178475819528103\n",
      "step 9600 loss 0.02178375795483589\n",
      "step 9601 loss 0.021782757714390755\n",
      "step 9602 loss 0.02178175188601017\n",
      "step 9603 loss 0.02178073488175869\n",
      "step 9604 loss 0.02177974209189415\n",
      "step 9605 loss 0.02177874557673931\n",
      "step 9606 loss 0.02177773229777813\n",
      "step 9607 loss 0.02177673764526844\n",
      "step 9608 loss 0.021775728091597557\n",
      "step 9609 loss 0.02177472412586212\n",
      "step 9610 loss 0.021773744374513626\n",
      "step 9611 loss 0.0217727143317461\n",
      "step 9612 loss 0.021771714091300964\n",
      "step 9613 loss 0.021770721301436424\n",
      "step 9614 loss 0.021769734099507332\n",
      "step 9615 loss 0.021768707782030106\n",
      "step 9616 loss 0.021767720580101013\n",
      "step 9617 loss 0.021766697987914085\n",
      "step 9618 loss 0.021765708923339844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 9619 loss 0.02176470309495926\n",
      "step 9620 loss 0.021763717755675316\n",
      "step 9621 loss 0.021762702614068985\n",
      "step 9622 loss 0.021761709824204445\n",
      "step 9623 loss 0.021760690957307816\n",
      "step 9624 loss 0.021759705618023872\n",
      "step 9625 loss 0.021758686751127243\n",
      "step 9626 loss 0.021757710725069046\n",
      "step 9627 loss 0.021756716072559357\n",
      "step 9628 loss 0.02175568975508213\n",
      "step 9629 loss 0.021754708141088486\n",
      "step 9630 loss 0.021753700450062752\n",
      "step 9631 loss 0.021752705797553062\n",
      "step 9632 loss 0.02175169438123703\n",
      "step 9633 loss 0.02175070531666279\n",
      "step 9634 loss 0.021749692037701607\n",
      "step 9635 loss 0.021748702973127365\n",
      "step 9636 loss 0.021747702732682228\n",
      "step 9637 loss 0.021746696904301643\n",
      "step 9638 loss 0.021745696663856506\n",
      "step 9639 loss 0.021744700148701668\n",
      "step 9640 loss 0.021743714809417725\n",
      "step 9641 loss 0.02174270711839199\n",
      "step 9642 loss 0.021741699427366257\n",
      "step 9643 loss 0.02174069918692112\n",
      "step 9644 loss 0.021739698946475983\n",
      "step 9645 loss 0.021738708019256592\n",
      "step 9646 loss 0.021737709641456604\n",
      "step 9647 loss 0.021736709401011467\n",
      "step 9648 loss 0.021735725924372673\n",
      "step 9649 loss 0.021734707057476044\n",
      "step 9650 loss 0.021733727306127548\n",
      "step 9651 loss 0.021732721477746964\n",
      "step 9652 loss 0.021731728687882423\n",
      "step 9653 loss 0.021730732172727585\n",
      "step 9654 loss 0.021729743108153343\n",
      "step 9655 loss 0.021728748455643654\n",
      "step 9656 loss 0.021727748215198517\n",
      "step 9657 loss 0.021726764738559723\n",
      "step 9658 loss 0.021725758910179138\n",
      "step 9659 loss 0.021724753081798553\n",
      "step 9660 loss 0.021723762154579163\n",
      "step 9661 loss 0.02172277867794037\n",
      "step 9662 loss 0.021721774712204933\n",
      "step 9663 loss 0.021720776334404945\n",
      "step 9664 loss 0.02171977609395981\n",
      "step 9665 loss 0.021718783304095268\n",
      "step 9666 loss 0.021717771887779236\n",
      "step 9667 loss 0.02171679586172104\n",
      "step 9668 loss 0.021715793758630753\n",
      "step 9669 loss 0.021714791655540466\n",
      "step 9670 loss 0.021713804453611374\n",
      "step 9671 loss 0.021712806075811386\n",
      "step 9672 loss 0.02171180211007595\n",
      "step 9673 loss 0.021710805594921112\n",
      "step 9674 loss 0.021709823980927467\n",
      "step 9675 loss 0.021708820015192032\n",
      "step 9676 loss 0.021707821637392044\n",
      "step 9677 loss 0.021706825122237206\n",
      "step 9678 loss 0.021705832332372665\n",
      "step 9679 loss 0.021704845130443573\n",
      "step 9680 loss 0.02170383930206299\n",
      "step 9681 loss 0.021702857688069344\n",
      "step 9682 loss 0.021701855584979057\n",
      "step 9683 loss 0.02170085348188877\n",
      "step 9684 loss 0.02169986255466938\n",
      "step 9685 loss 0.02169886603951454\n",
      "step 9686 loss 0.021697869524359703\n",
      "step 9687 loss 0.021696873009204865\n",
      "step 9688 loss 0.021695878356695175\n",
      "step 9689 loss 0.02169487439095974\n",
      "step 9690 loss 0.021693896502256393\n",
      "step 9691 loss 0.021692896261811256\n",
      "step 9692 loss 0.02169189043343067\n",
      "step 9693 loss 0.021690893918275833\n",
      "step 9694 loss 0.021689901128411293\n",
      "step 9695 loss 0.021688910201191902\n",
      "step 9696 loss 0.02168789878487587\n",
      "step 9697 loss 0.02168690599501133\n",
      "step 9698 loss 0.021685931831598282\n",
      "step 9699 loss 0.0216849222779274\n",
      "step 9700 loss 0.021683931350708008\n",
      "step 9701 loss 0.02168292924761772\n",
      "step 9702 loss 0.021681945770978928\n",
      "step 9703 loss 0.021680956706404686\n",
      "step 9704 loss 0.021679949015378952\n",
      "step 9705 loss 0.02167895995080471\n",
      "step 9706 loss 0.021677959710359573\n",
      "step 9707 loss 0.02167697437107563\n",
      "step 9708 loss 0.021675972267985344\n",
      "step 9709 loss 0.02167498506605625\n",
      "step 9710 loss 0.021673984825611115\n",
      "step 9711 loss 0.021672988310456276\n",
      "step 9712 loss 0.021672002971172333\n",
      "step 9713 loss 0.021670999005436897\n",
      "step 9714 loss 0.02166999876499176\n",
      "step 9715 loss 0.02166900597512722\n",
      "step 9716 loss 0.021668018773198128\n",
      "step 9717 loss 0.021667009219527245\n",
      "step 9718 loss 0.02166602574288845\n",
      "step 9719 loss 0.021665029227733612\n",
      "step 9720 loss 0.021664023399353027\n",
      "step 9721 loss 0.021663032472133636\n",
      "step 9722 loss 0.02166205458343029\n",
      "step 9723 loss 0.02166105806827545\n",
      "step 9724 loss 0.021660061553120613\n",
      "step 9725 loss 0.021659068763256073\n",
      "step 9726 loss 0.021658064797520638\n",
      "step 9727 loss 0.021657072007656097\n",
      "step 9728 loss 0.021656084805727005\n",
      "step 9729 loss 0.021655088290572166\n",
      "step 9730 loss 0.021654095500707626\n",
      "step 9731 loss 0.021653098985552788\n",
      "step 9732 loss 0.02165210247039795\n",
      "step 9733 loss 0.02165110781788826\n",
      "step 9734 loss 0.02165011316537857\n",
      "step 9735 loss 0.021649116650223732\n",
      "step 9736 loss 0.021648135036230087\n",
      "step 9737 loss 0.0216471329331398\n",
      "step 9738 loss 0.021646136417984962\n",
      "step 9739 loss 0.02164514549076557\n",
      "step 9740 loss 0.02164416015148163\n",
      "step 9741 loss 0.021643146872520447\n",
      "step 9742 loss 0.021642165258526802\n",
      "step 9743 loss 0.021641172468662262\n",
      "step 9744 loss 0.021640166640281677\n",
      "step 9745 loss 0.021639175713062286\n",
      "step 9746 loss 0.021638173609972\n",
      "step 9747 loss 0.021637165918946266\n",
      "step 9748 loss 0.021636152639985085\n",
      "step 9749 loss 0.021635137498378754\n",
      "step 9750 loss 0.02163410745561123\n",
      "step 9751 loss 0.021633077412843704\n",
      "step 9752 loss 0.021632038056850433\n",
      "step 9753 loss 0.021631009876728058\n",
      "step 9754 loss 0.02162998728454113\n",
      "step 9755 loss 0.02162894792854786\n",
      "step 9756 loss 0.02162792533636093\n",
      "step 9757 loss 0.021626897156238556\n",
      "step 9758 loss 0.02162584289908409\n",
      "step 9759 loss 0.021624814718961716\n",
      "step 9760 loss 0.021623779088258743\n",
      "step 9761 loss 0.02162274345755577\n",
      "step 9762 loss 0.021621698513627052\n",
      "step 9763 loss 0.021620668470859528\n",
      "step 9764 loss 0.02161961980164051\n",
      "step 9765 loss 0.021618595346808434\n",
      "step 9766 loss 0.021617546677589417\n",
      "step 9767 loss 0.021616509184241295\n",
      "step 9768 loss 0.021615471690893173\n",
      "step 9769 loss 0.021614447236061096\n",
      "step 9770 loss 0.02161339297890663\n",
      "step 9771 loss 0.02161235734820366\n",
      "step 9772 loss 0.021611321717500687\n",
      "step 9773 loss 0.021610278636217117\n",
      "step 9774 loss 0.0216092299669981\n",
      "step 9775 loss 0.021608199924230576\n",
      "step 9776 loss 0.021607154980301857\n",
      "step 9777 loss 0.021606119349598885\n",
      "step 9778 loss 0.021605083718895912\n",
      "step 9779 loss 0.021604027599096298\n",
      "step 9780 loss 0.021602988243103027\n",
      "step 9781 loss 0.021601958200335503\n",
      "step 9782 loss 0.021600918844342232\n",
      "step 9783 loss 0.021599870175123215\n",
      "step 9784 loss 0.02159883826971054\n",
      "step 9785 loss 0.02159779518842697\n",
      "step 9786 loss 0.021596752107143402\n",
      "step 9787 loss 0.02159571647644043\n",
      "step 9788 loss 0.02159467525780201\n",
      "step 9789 loss 0.021593641489744186\n",
      "step 9790 loss 0.021592583507299423\n",
      "step 9791 loss 0.021591560915112495\n",
      "step 9792 loss 0.02159051224589348\n",
      "step 9793 loss 0.021589474752545357\n",
      "step 9794 loss 0.021588435396552086\n",
      "step 9795 loss 0.021587390452623367\n",
      "step 9796 loss 0.021586356684565544\n",
      "step 9797 loss 0.021585317328572273\n",
      "step 9798 loss 0.021584266796708107\n",
      "step 9799 loss 0.021583233028650284\n",
      "step 9800 loss 0.021582193672657013\n",
      "step 9801 loss 0.021581154316663742\n",
      "step 9802 loss 0.02158011868596077\n",
      "step 9803 loss 0.021579088643193245\n",
      "step 9804 loss 0.02157803811132908\n",
      "step 9805 loss 0.021577006205916405\n",
      "step 9806 loss 0.02157595567405224\n",
      "step 9807 loss 0.021574925631284714\n",
      "step 9808 loss 0.021573886275291443\n",
      "step 9809 loss 0.021572846919298172\n",
      "step 9810 loss 0.0215718075633049\n",
      "step 9811 loss 0.021570758894085884\n",
      "step 9812 loss 0.02156972885131836\n",
      "step 9813 loss 0.02156868390738964\n",
      "step 9814 loss 0.021567661315202713\n",
      "step 9815 loss 0.021566614508628845\n",
      "step 9816 loss 0.021565580740571022\n",
      "step 9817 loss 0.021564526483416557\n",
      "step 9818 loss 0.021563511341810226\n",
      "step 9819 loss 0.021562468260526657\n",
      "step 9820 loss 0.02156141772866249\n",
      "step 9821 loss 0.021560387685894966\n",
      "step 9822 loss 0.021559348329901695\n",
      "step 9823 loss 0.021558312699198723\n",
      "step 9824 loss 0.0215572789311409\n",
      "step 9825 loss 0.021556230261921883\n",
      "step 9826 loss 0.021555200219154358\n",
      "step 9827 loss 0.02155415713787079\n",
      "step 9828 loss 0.02155311591923237\n",
      "step 9829 loss 0.02155207470059395\n",
      "step 9830 loss 0.02155105583369732\n",
      "step 9831 loss 0.021550001576542854\n",
      "step 9832 loss 0.02154896780848503\n",
      "step 9833 loss 0.021547924727201462\n",
      "step 9834 loss 0.02154689095914364\n",
      "step 9835 loss 0.02154586836695671\n",
      "step 9836 loss 0.021544814109802246\n",
      "step 9837 loss 0.021543772891163826\n",
      "step 9838 loss 0.021542739123106003\n",
      "step 9839 loss 0.02154170535504818\n",
      "step 9840 loss 0.021540656685829163\n",
      "step 9841 loss 0.02153961732983589\n",
      "step 9842 loss 0.021538587287068367\n",
      "step 9843 loss 0.021537553519010544\n",
      "step 9844 loss 0.021536512300372124\n",
      "step 9845 loss 0.0215354785323143\n",
      "step 9846 loss 0.021534428000450134\n",
      "step 9847 loss 0.021533403545618057\n",
      "step 9848 loss 0.021532373502850533\n",
      "step 9849 loss 0.021531321108341217\n",
      "step 9850 loss 0.021530285477638245\n",
      "step 9851 loss 0.021529249846935272\n",
      "step 9852 loss 0.021528219804167747\n",
      "step 9853 loss 0.02152719348669052\n",
      "step 9854 loss 0.021526141092181206\n",
      "step 9855 loss 0.02152511291205883\n",
      "step 9856 loss 0.021524064242839813\n",
      "step 9857 loss 0.02152303233742714\n",
      "step 9858 loss 0.02152199298143387\n",
      "step 9859 loss 0.02152097038924694\n",
      "step 9860 loss 0.02151992730796337\n",
      "step 9861 loss 0.0215188879519701\n",
      "step 9862 loss 0.02151784859597683\n",
      "step 9863 loss 0.021516814827919006\n",
      "step 9864 loss 0.021515775471925735\n",
      "step 9865 loss 0.021514737978577614\n",
      "step 9866 loss 0.021513700485229492\n",
      "step 9867 loss 0.021512655541300774\n",
      "step 9868 loss 0.02151162177324295\n",
      "step 9869 loss 0.021510595455765724\n",
      "step 9870 loss 0.021509550511837006\n",
      "step 9871 loss 0.02150852233171463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 9872 loss 0.021507473662495613\n",
      "step 9873 loss 0.021506451070308685\n",
      "step 9874 loss 0.021505389362573624\n",
      "step 9875 loss 0.0215043593198061\n",
      "step 9876 loss 0.021503331139683723\n",
      "step 9877 loss 0.021502288058400154\n",
      "step 9878 loss 0.021501250565052032\n",
      "step 9879 loss 0.02150021493434906\n",
      "step 9880 loss 0.02149917557835579\n",
      "step 9881 loss 0.02149813249707222\n",
      "step 9882 loss 0.021497121080756187\n",
      "step 9883 loss 0.021496068686246872\n",
      "step 9884 loss 0.0214950330555439\n",
      "step 9885 loss 0.021493997424840927\n",
      "step 9886 loss 0.021492965519428253\n",
      "step 9887 loss 0.021491922438144684\n",
      "step 9888 loss 0.02149089053273201\n",
      "step 9889 loss 0.02148984931409359\n",
      "step 9890 loss 0.021488819271326065\n",
      "step 9891 loss 0.021487779915332794\n",
      "step 9892 loss 0.021486740559339523\n",
      "step 9893 loss 0.021485701203346252\n",
      "step 9894 loss 0.02148466557264328\n",
      "step 9895 loss 0.021483631804585457\n",
      "step 9896 loss 0.02148258127272129\n",
      "step 9897 loss 0.021481551229953766\n",
      "step 9898 loss 0.021480511873960495\n",
      "step 9899 loss 0.02147948183119297\n",
      "step 9900 loss 0.021478433161973953\n",
      "step 9901 loss 0.02147740311920643\n",
      "step 9902 loss 0.02147636003792286\n",
      "step 9903 loss 0.021475329995155334\n",
      "step 9904 loss 0.021474285051226616\n",
      "step 9905 loss 0.02147325500845909\n",
      "step 9906 loss 0.02147221565246582\n",
      "step 9907 loss 0.02147117629647255\n",
      "step 9908 loss 0.02147013694047928\n",
      "step 9909 loss 0.021469097584486008\n",
      "step 9910 loss 0.021468061953783035\n",
      "step 9911 loss 0.02146703004837036\n",
      "step 9912 loss 0.021465981379151344\n",
      "step 9913 loss 0.021464945748448372\n",
      "step 9914 loss 0.021463904529809952\n",
      "step 9915 loss 0.02146286331117153\n",
      "step 9916 loss 0.021461835131049156\n",
      "step 9917 loss 0.02146078646183014\n",
      "step 9918 loss 0.021459750831127167\n",
      "step 9919 loss 0.021458717063069344\n",
      "step 9920 loss 0.021457696333527565\n",
      "step 9921 loss 0.021456636488437653\n",
      "step 9922 loss 0.02145557664334774\n",
      "step 9923 loss 0.021454554051160812\n",
      "step 9924 loss 0.02145351655781269\n",
      "step 9925 loss 0.021452460438013077\n",
      "step 9926 loss 0.021451419219374657\n",
      "step 9927 loss 0.02145037241280079\n",
      "step 9928 loss 0.02144932746887207\n",
      "step 9929 loss 0.021448273211717606\n",
      "step 9930 loss 0.021447230130434036\n",
      "step 9931 loss 0.02144618332386017\n",
      "step 9932 loss 0.0214451402425766\n",
      "step 9933 loss 0.02144409716129303\n",
      "step 9934 loss 0.021443037316203117\n",
      "step 9935 loss 0.021441994234919548\n",
      "step 9936 loss 0.02144094742834568\n",
      "step 9937 loss 0.021439895033836365\n",
      "step 9938 loss 0.021438851952552795\n",
      "step 9939 loss 0.02143779583275318\n",
      "step 9940 loss 0.021436739712953568\n",
      "step 9941 loss 0.021435702219605446\n",
      "step 9942 loss 0.021434636786580086\n",
      "step 9943 loss 0.021433591842651367\n",
      "step 9944 loss 0.021432561799883842\n",
      "step 9945 loss 0.02143150381743908\n",
      "step 9946 loss 0.02143046073615551\n",
      "step 9947 loss 0.021429408341646194\n",
      "step 9948 loss 0.021428346633911133\n",
      "step 9949 loss 0.021427292376756668\n",
      "step 9950 loss 0.02142626792192459\n",
      "step 9951 loss 0.021425195038318634\n",
      "step 9952 loss 0.02142414078116417\n",
      "step 9953 loss 0.021423110738396645\n",
      "step 9954 loss 0.021422071382403374\n",
      "step 9955 loss 0.021421005949378014\n",
      "step 9956 loss 0.0214199498295784\n",
      "step 9957 loss 0.02141890488564968\n",
      "step 9958 loss 0.02141786366701126\n",
      "step 9959 loss 0.021416807547211647\n",
      "step 9960 loss 0.021415753290057182\n",
      "step 9961 loss 0.021414706483483315\n",
      "step 9962 loss 0.021413646638393402\n",
      "step 9963 loss 0.021412605419754982\n",
      "step 9964 loss 0.021411553025245667\n",
      "step 9965 loss 0.021410508081316948\n",
      "step 9966 loss 0.021409451961517334\n",
      "step 9967 loss 0.021408401429653168\n",
      "step 9968 loss 0.021407347172498703\n",
      "step 9969 loss 0.021406294777989388\n",
      "step 9970 loss 0.021405251696705818\n",
      "step 9971 loss 0.021404199302196503\n",
      "step 9972 loss 0.021403135731816292\n",
      "step 9973 loss 0.02140209637582302\n",
      "step 9974 loss 0.02140102908015251\n",
      "step 9975 loss 0.02139999344944954\n",
      "step 9976 loss 0.02139892987906933\n",
      "step 9977 loss 0.021397890523076057\n",
      "step 9978 loss 0.021396823227405548\n",
      "step 9979 loss 0.021395787596702576\n",
      "step 9980 loss 0.021394729614257812\n",
      "step 9981 loss 0.021393675357103348\n",
      "step 9982 loss 0.021392621099948883\n",
      "step 9983 loss 0.021391572430729866\n",
      "step 9984 loss 0.021390508860349655\n",
      "step 9985 loss 0.021389462053775787\n",
      "step 9986 loss 0.02138841152191162\n",
      "step 9987 loss 0.021387357264757156\n",
      "step 9988 loss 0.02138630673289299\n",
      "step 9989 loss 0.021385258063673973\n",
      "step 9990 loss 0.021384190768003464\n",
      "step 9991 loss 0.021383143961429596\n",
      "step 9992 loss 0.021382104605436325\n",
      "step 9993 loss 0.021381044760346413\n",
      "step 9994 loss 0.021380005404353142\n",
      "step 9995 loss 0.02137894183397293\n",
      "step 9996 loss 0.021377887576818466\n",
      "step 9997 loss 0.021376833319664\n",
      "step 9998 loss 0.021375782787799835\n",
      "step 9999 loss 0.021374737843871117\n",
      "[[10.171006  10.492931  10.1939335 10.6424265]\n",
      " [10.770752  10.870486  10.502998  10.632202 ]\n",
      " [ 9.96619   10.333247  10.257809  10.730579 ]\n",
      " [10.748963  10.721471  10.410752  10.349009 ]\n",
      " [10.173744  10.412583  10.552483  10.792875 ]]\n",
      "[[10.22572155 10.42890072 10.16536958 10.68245183]\n",
      " [10.96134529 10.65759344 10.40082705 10.76043168]\n",
      " [10.00166003 10.29925064 10.23831878 10.75114171]\n",
      " [10.76980085 10.66423459 10.40454976 10.38818062]\n",
      " [10.21793505 10.37419436 10.52735131 10.81435375]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    #  first a necessary bit of Tensorflow boiler plate - initializing variables\n",
    "    #  we do this operation using the Session\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    #  we create a FileWriter object to write out summarys to a file for Tensorboard to read\n",
    "    writer = tf.summary.FileWriter('./logs', graph=sess.graph)\n",
    "    \n",
    "    #  now the training loop\n",
    "    #  we are not splitting our training data into batches\n",
    "    \n",
    "    for train_step in range(100):\n",
    "        #  now we run the tensorflow graph\n",
    "        #  the graph is run by calling the .run method on the session\n",
    "        #  the run method takes two inputs:\n",
    "        #   fetches = the tf operations to run\n",
    "        #   feed_dict = values for the placeholders\n",
    "\n",
    "        #  here we fetch two operations\n",
    "        #  train_op - the operation to train the network\n",
    "        #   summary - the summary operations for the graph\n",
    "        fetches = [loss, train_op, merged]\n",
    "\n",
    "        #  the feed_dict is a dictionary with\n",
    "        #   keys = the placeholders\n",
    "        #   values = the numpy arrays\n",
    "        #   note that we feed in multiple samples\n",
    "        feed_dict = {network_input: inputs,\n",
    "                     target: targets}\n",
    "        #  finally we run the session using the fetches and feed_dict\n",
    "        loss_value, _, summary = sess.run(fetches, feed_dict)\n",
    "        # the operation to add the summary to the tensorboard output file\n",
    "        writer.add_summary(summary, train_step)\n",
    "        print('step {} loss {}'.format(train_step, loss_value))        \n",
    "        \n",
    "    #  now training is done\n",
    "    #  generate a test set \n",
    "    test_in, test_out = generate_data(5)\n",
    "\n",
    "    # here we get predictions from our network - we don't train\n",
    "    pred = sess.run(output, {network_input: test_in})\n",
    "    print(pred)\n",
    "    print(test_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
